{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Section 4 - PyTorch Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzdA+iNMo4pd/CAQYoidlm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrajasek95/nlp-243-notebooks/blob/main/Section_4_PyTorch_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j-aoWjIvzKi"
      },
      "source": [
        "# PyTorch Basics\n",
        "\n",
        "This notebook will cover PyTorch tensors and associated concepts of linear algebra, and how to implement gradient descent using PyTorch. We will be covering more comprehensive use of PyTorch for actual classification and regression tasks in the next section when dealing with Multilayer Perceptrons.\n",
        "\n",
        "We will be making use of GPU training as a proof-of-concept, so please enable a GPU runtime by clicking on *Runtime > Change Runtime Type* and selecting GPU acceleration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT39UGmowjBN"
      },
      "source": [
        "## Using PyTorch\n",
        "\n",
        "To use the PyTorch library, we will need to import the `torch` module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWNm1hSTwiYY"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzCBIbg2wdRq"
      },
      "source": [
        "## PyTorch Tensors\n",
        "\n",
        "PyTorch Tensors are created using the `torch.Tensor` class. We will show different ways to declare tensors using subclasses and special convenience methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm8hTGjvvyWD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "e10f1742-718e-49ff-b574-e5455446666b"
      },
      "source": [
        "# Declare using the tensor class\n",
        "identity_3 = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "zero_3 = torch.FloatTensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
        "ones_3 = torch.LongTensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
        "\n",
        "# Convenience method to declare an identity matrix\n",
        "i3 = torch.eye(3)\n",
        "zero3 = torch.zeros((3, 3)) # 3 by 3 zero matrix\n",
        "ones3 = torch.ones((3, 3))\n",
        "\n",
        "# 4D tensor\n",
        "four_d_tensor = torch.stack((zero3, ones3))\n",
        "\n",
        "zero4 = torch.zeros((3, 3, 3))\n",
        "\n",
        "for tensor in [identity_3, zero_3, ones_3, i3, zero3, ones3, four_d_tensor, zero4]:\n",
        "    print(\"Tensor:\")\n",
        "    print(tensor)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor:\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "\n",
            "Tensor:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "\n",
            "Tensor:\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1],\n",
            "        [1, 1, 1]])\n",
            "\n",
            "Tensor:\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "\n",
            "Tensor:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "\n",
            "Tensor:\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "\n",
            "Tensor:\n",
            "tensor([[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[1., 1., 1.],\n",
            "         [1., 1., 1.],\n",
            "         [1., 1., 1.]]])\n",
            "\n",
            "Tensor:\n",
            "tensor([[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q15S7NNfyCBL"
      },
      "source": [
        "## Tensor Operations\n",
        "\n",
        "Tensor support common linear algebra operations:\n",
        "* Addition\n",
        "* Subtraction\n",
        "* Scalar multiplication\n",
        "* Dot product (useful for vectors)\n",
        "* Element wise product\n",
        "* Matrix product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsFdgXVux4OM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "1d1b8a6a-9ae7-4b16-e5ae-13b43f65126c"
      },
      "source": [
        "\n",
        "print(\"Sum of two ones matrices:\")\n",
        "print(ones3 + ones3)\n",
        "print()\n",
        "\n",
        "print(\"Difference between ones and identity matrices:\")\n",
        "print(ones3 - i3)\n",
        "\n",
        "print(\"Scaling a tensor:\")\n",
        "print(3 * i3)\n",
        "\n",
        "print(\"Element wise product:\")\n",
        "print((ones3 + ones3) * (ones3 + ones3))\n",
        "\n",
        "print(\"element wise division\")\n",
        "print((ones3 + ones3) / (ones3 + ones3))\n",
        "\n",
        "print(\"Matrix product:\")\n",
        "print((ones3 + ones3) @ (ones3 + ones3))\n",
        "\n",
        "vec1 = torch.FloatTensor([2, 0, 2])\n",
        "vec2 = torch.FloatTensor([0, 1, 2])\n",
        "\n",
        "print(\"Dot product:\")\n",
        "print(torch.dot(vec1, vec2))\n",
        "print(vec1 @ vec2) # Alternate\n",
        "\n",
        "print(\"Matrix product between tensor and vector:\")\n",
        "print(ones3 @ vec1) # Take dot product of vector with each row of the matrix. Vector must always be after the matrix for this to work"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum of two ones matrices:\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "\n",
            "Difference between ones and identity matrices:\n",
            "tensor([[0., 1., 1.],\n",
            "        [1., 0., 1.],\n",
            "        [1., 1., 0.]])\n",
            "Scaling a tensor:\n",
            "tensor([[3., 0., 0.],\n",
            "        [0., 3., 0.],\n",
            "        [0., 0., 3.]])\n",
            "Element wise product:\n",
            "tensor([[4., 4., 4.],\n",
            "        [4., 4., 4.],\n",
            "        [4., 4., 4.]])\n",
            "element wise division\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "Matrix product:\n",
            "tensor([[12., 12., 12.],\n",
            "        [12., 12., 12.],\n",
            "        [12., 12., 12.]])\n",
            "Dot product:\n",
            "tensor(4.)\n",
            "tensor(4.)\n",
            "Matrix product between tensor and vector:\n",
            "tensor([4., 4., 4.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGtkjApYuxgs"
      },
      "source": [
        "## Some Additional Linear Algebra Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iS9mNchuzAo"
      },
      "source": [
        "### Inverse of a Matrix\n",
        "\n",
        "Torch provides methods to calculate inverse of a square matrix. An inverse $A^{-1}$ is a matrix such that $AA^{-1} = A^{-1}A = I$ holds. The inverse is given by $A^{-1} = \\frac{1}{|A|} Cofactor(A)^T$ where $|A|$ is the determinant of $A$. This implies that when $|A| = 0$, the inverse is undefined as we will be performing a division by $0$. Such matrices are called *singular matrices*. \n",
        "\n",
        "Note that inverse is not defined for non-square matrices. There is an equivalent concept called _pseudoinverse_ for non-square matrices which behave like an inverse matrix for specific problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ4TferCu7SO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e3ead3c9-2d88-4fad-d1ab-2c6db2063a93"
      },
      "source": [
        "\n",
        "example_matrix = torch.Tensor([[1, 0, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(example_matrix)\n",
        "print(\"Inverse of 3x3 matrix\")\n",
        "print(torch.inverse(example_matrix))\n",
        "\n",
        "print(\"Product of matrix and inverse\")\n",
        "print(torch.mm(example_matrix, torch.inverse(example_matrix)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [7., 8., 9.]])\n",
            "Inverse of 3x3 matrix\n",
            "tensor([[ 0.2500, -2.0000,  1.2500],\n",
            "        [-0.5000,  1.0000, -0.5000],\n",
            "        [ 0.2500,  0.6667, -0.4167]])\n",
            "Product of matrix and inverse\n",
            "tensor([[ 1.0000e+00,  5.9605e-08, -5.9605e-08],\n",
            "        [ 2.0862e-07,  1.0000e+00,  1.1921e-07],\n",
            "        [ 3.1292e-07, -5.3644e-07,  1.0000e+00]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTseQ6-j0qfu"
      },
      "source": [
        "\n",
        "### Norm\n",
        "One important concept is that of the *Norm* of a vector or matrix. \n",
        "\n",
        "If you treat each element of a vector or tensor as some kind of distance from an origin along a particular axis, the norm tries to capture the total distance away from origin as a scalar value.\n",
        "\n",
        "For example, consider the vector $ \\langle 1, 0, 3 \\rangle $. This can represent the $ \\langle x, y, z \\rangle $ coordinates of a point from origin. If we try to calculate the Euclidean distance from origin, it will be $||D|| = \\sqrt{1^2 + 0^2 + 3^2} = \\sqrt{10}$. In general, for an n-dimensional vector $\\langle x_1, x_2, \\dots, x_n \\rangle$, the distance/norm is given by $||D|| = \\sqrt{\\sum_{i=1}^n x_i^2}$. This is also called the $L_2$ norm.\n",
        "\n",
        "There are other norms which use a different power to calculate this \"distance\". The general equation for a q-norm (also called Minkowski distance) is $||D||_q = (\\sum_{i=1}^n |x_i|^q)^\\frac{1}{q}$ where $|x|$ represents the absolute value. \n",
        "\n",
        "The $L_1$ norm or _Manhattan Distance_ is given by $||D||_1 = \\sum_{i=1}^n |x_i| $ which is the sum of absolute values of the components.\n",
        "\n",
        "One important property arises when each value of a vector/matrix is divided by the norm. The resultant norm of this new vector/matrix is always 1. This process of rescaling the vector/matrix to a unit norm is called normalization.\n",
        "\n",
        "*Exercise: Prove this property*\n",
        "\n",
        "For matrices, the norm is computed by computing the square root of the sum of squares of all the elements. This is called the Frobenius Norm.\n",
        "\n",
        "$$ ||A|| = \\sqrt{\\sum_i\\sum_j a_{ij}^2} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H65vmZA1yzzt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "8ae23482-d935-4e1d-eb99-71115d46d96a"
      },
      "source": [
        "vec3 = torch.FloatTensor([3, 0, 4])\n",
        "\n",
        "print(\"L2 Norm of (3, 0, 4) = ?\")\n",
        "print(torch.norm(vec3)) # By default, L2 norm is always calculated\n",
        "\n",
        "print(\"Dividing each element by the norm (a.k.a normalization)\")\n",
        "print(vec3 / torch.norm(vec3))\n",
        "\n",
        "print(\"L2 Norm of a normalized vector:\")\n",
        "print(torch.norm(vec3 / torch.norm(vec3)))\n",
        "\n",
        "vec4 = torch.FloatTensor([3, 0, -4])\n",
        "print(\"L1 Norm of (3, 0, -4) = ?\")\n",
        "print(torch.norm(vec4, p=1))\n",
        "\n",
        "print(\"Dividing each element by L1 norm\")\n",
        "print(vec4 / torch.norm(vec4, p=1))\n",
        "\n",
        "print(\"L1 Norm of a normalized vector:\")\n",
        "print(torch.norm(vec4 / torch.norm(vec4, p=1), p=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 Norm of (3, 0, 4) = ?\n",
            "tensor(5.)\n",
            "Dividing each element by the norm (a.k.a normalization)\n",
            "tensor([0.6000, 0.0000, 0.8000])\n",
            "L2 Norm of a normalized vector:\n",
            "tensor(1.)\n",
            "L1 Norm of (3, 0, -4) = ?\n",
            "tensor(7.)\n",
            "Dividing each element by L1 norm\n",
            "tensor([ 0.4286,  0.0000, -0.5714])\n",
            "L1 Norm of a normalized vector:\n",
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn7Pfr_24fnc"
      },
      "source": [
        "The notion of norm will become very important when we talk about regularization and normalizing data. \n",
        "\n",
        "Another useful concept is computing cosine similarity. In 2-D space, cosine similarity is calculated as the cosine angle between the two vectors\n",
        "\n",
        "$$ cos(( x_1,y_1 ), ( x_2, y_2 )) = \\frac{(x_1 * x2 + y_1 * y_2)}{\\sqrt{x_1^2 + y_1^2} \\sqrt{x_2^2 + y_2^2}}$$\n",
        "\n",
        "We can extend this to n-dimensional vectors by using the $L_2$ norm as follows\n",
        "\n",
        "$$ cos(U, V) = \\frac{ \\langle U, V \\rangle}{||U||_2 ||V||_2} $$\n",
        "\n",
        "Where $\\langle U, V \\rangle $ represents the inner/dot product of two vectors $U$ and $V$.\n",
        "\n",
        "Exercises:\n",
        "* Show that $cos(U, U) = 1$ for $U \\neq 0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n8a0rDi4A1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffb67cdd-c652-4aa0-a84f-0c27b1ff4d2e"
      },
      "source": [
        "def cosine_similarity(u, v):\n",
        "    # Find the cosine similarity of vectors U and V both of the same number of dimensions\n",
        "    return (u @ v) / (torch.norm(u) * torch.norm(v))\n",
        "\n",
        "\n",
        "print(cosine_similarity(vec3, vec4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-0.2800)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K764atl_yFLY"
      },
      "source": [
        "### Eigenvalues and Eigenvectors\n",
        "\n",
        "Eigenvectors of a square matrix $A$ are vectors $v$ which obey the property\n",
        "\n",
        "$$\n",
        "    Av = \\lambda v\n",
        "$$\n",
        "\n",
        "where $\\lambda$ is a scalar value called the Eigenvalue. This defines a notion that there are some vectors $v$ which simply get \"scaled\" by the matrix.\n",
        "\n",
        "Eigenvalues are calculated by solving the characteristic equation\n",
        "\n",
        "$$\n",
        "|A - \\lambda I| = 0\n",
        "$$\n",
        "\n",
        "Where $|A|$ is the determinant and $I$ is the identity matrix with the same dimensions as $A$\n",
        "\n",
        "If the matrix dimension is $n \\times n$, we will have an $n^{th}$ order equation to solve, which yields roots $\\lambda_1, \\dots, \\lambda_n$. These are called the eigenvalues of the matrix $A$. \n",
        "\n",
        "Once we have obtained the eigen values, we can substitute them in the equation\n",
        "$$\n",
        "(A - \\lambda_i)v_i = 0\n",
        "$$\n",
        "\n",
        "And solve for the terms in the vector.\n",
        "\n",
        "One important property is that these vectors can be thought of as the principal axes for a transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiHaKLes1IWa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a1f1d926-7874-45ce-e6ea-f5f65cb6f73b"
      },
      "source": [
        "print(torch.eig(ones3, eigenvectors=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.return_types.eig(\n",
            "eigenvalues=tensor([[ 3.0000e+00,  0.0000e+00],\n",
            "        [ 1.3765e-07,  0.0000e+00],\n",
            "        [-7.1054e-15,  0.0000e+00]]),\n",
            "eigenvectors=tensor([[-5.7735e-01, -8.1389e-01,  4.4561e-08],\n",
            "        [-5.7735e-01,  3.5054e-01, -7.0711e-01],\n",
            "        [-5.7735e-01,  4.6335e-01,  7.0711e-01]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pEdeHFe1Q_u"
      },
      "source": [
        "Eigenvalues and Eigenvectors are used for performing Principal Component Analysis which is a dimensionality reduction technique. We will not describe this in the interest of time, but it is useful to know about, even informally.\n",
        "\n",
        "Simple exercises for eigenvalues:\n",
        "* What are the eigenvalues and eigenvectors of a diagonal matrix $D$?\n",
        "* Read about and implement PCA using `torch.eig`\n",
        "\n",
        "Slightly more involved exercises:\n",
        "* Prove that the all the eigenvalues of a symmetric matrix are real\n",
        "* Show that for a symmetric matrix $A$, the eigenvectors form an orthonormal basis i.e. for any two eigenvectors $v_i$ and $v_j$ such that $v_i \\neq v_j$, we have $v_i^T v_j = 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkz6gclE4zNH"
      },
      "source": [
        "## Sampling Data from Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQZhAFMiGD1r"
      },
      "source": [
        "\n",
        "### Parameter Initialization\n",
        "For most parametric models , we will have to set the model weights to some  initial values. Typically, weights are initialized in a distribution centered at 0.\n",
        "\n",
        "Typical distributions used:\n",
        "* Uniform\n",
        "* Normal Distribution\n",
        "* Multinomial Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZj9JB4PViO3"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ynSuO3oGCdl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "04efefc6-658d-4f00-d05c-c8e1f18dda2d"
      },
      "source": [
        "# Drawing from a uniform distribution\n",
        "a = -1\n",
        "b = 1\n",
        "\n",
        "# torch.rand - samples from U(0, 1)\n",
        "# sample from [a, b] - a, \n",
        "w = torch.rand(size=(100000,))*(b - a) + a # Draw numbers randomly from a uniform distribution from [a, b)\n",
        "sns.displot(w)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c01f7ae6cebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m \u001b[0;31m# Draw numbers randomly from a uniform distribution from [a, b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASm56AiiVtGh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "840002d2-e71d-43eb-ec4c-e3c98fa31c45"
      },
      "source": [
        "# Drawing from a standard normal distribution\n",
        "# Alternatively, use torch.randn which by default draws from N(0, 1)\n",
        "w = torch.normal(3, 3, size=(10000,))\n",
        "sns.displot(w)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3d70db305b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFG_W8akWUNT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "c3e4967d-822c-4d6f-cc90-1d2cb0a70a78"
      },
      "source": [
        "# Draw from a multinomial distribution\n",
        "# [p_0, p_1, p_2]\n",
        "w = torch.multinomial(torch.Tensor([3, 3, 4]), num_samples=1000, replacement=True)\n",
        "\n",
        "sns.histplot(w)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1ed324718ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PUPndom8L4x"
      },
      "source": [
        "## Using PyTorch GPU\n",
        "\n",
        "PyTorch helps us use the GPU for performing linear algebra operations. In order to do so, the data must be copied onto the GPU directly. However, we must ensure that we are able to use the GPU via NVIDIA CUDA. For that, we must check if we can access it. If not, we must safely fall back to the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TEiyWth7JX4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "08b77224-b747-4e76-cd43-71e3f764e0aa"
      },
      "source": [
        "print(\"Do we have a GPU? \", torch.cuda.is_available())\n",
        "\n",
        "# Using CUDA\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device to use\", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Do we have a GPU?  True\n",
            "Device to use cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCD9F73j9Ws1"
      },
      "source": [
        "We now try to assign a tensor to a GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phZGksmy8u3L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2bb8f355-3036-44b5-cbfb-b4b67766e36e"
      },
      "source": [
        "cuda_ones = torch.ones((3, 3), device=device)\n",
        "\n",
        "print(cuda_ones) # Spot the difference in the output from earlier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9kNZhWv9AY8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fb09e39-a41b-4d27-c93f-a53630adee33"
      },
      "source": [
        "# Which is more efficient, CPU or GPU?\n",
        "import time\n",
        "def multiply_a_lot(U, V, out):\n",
        "    start = time.time()\n",
        "    for _ in range(100):\n",
        "        torch.matmul(U, V, out=out) \n",
        "        # We use out to avoid having to create a new tensor\n",
        "    end = time.time()\n",
        "\n",
        "    print(\"Time elapsed: \", end - start)\n",
        "\n",
        "    return end - start\n",
        "\n",
        "dim_sizes = [1, 10, 20, 50, 100, 200, 300, 500, 1000, 2000]\n",
        "cpu_times = []\n",
        "cuda_times = []\n",
        "\n",
        "for dim_size in dim_sizes:\n",
        "    torch.cuda.empty_cache() # Free memory\n",
        "\n",
        "    print(\"Dimension size: \", (dim_size, dim_size))\n",
        "    ones = torch.ones((dim_size, dim_size))\n",
        "    print(\"CPU\")\n",
        "    out = torch.empty_like(ones)\n",
        "\n",
        "    cpu_exec_time = multiply_a_lot(ones, ones, out)\n",
        "    cpu_times.append(cpu_exec_time)\n",
        "\n",
        "    print(\"CUDA\")\n",
        "    ones_cuda = torch.ones((dim_size, dim_size), device=device)\n",
        "    out_cuda = torch.empty_like(ones_cuda)\n",
        "    gpu_exec_time = multiply_a_lot(cuda_ones, cuda_ones, out_cuda)\n",
        "\n",
        "    cuda_times.append(gpu_exec_time)\n",
        "    print(\"Speedup: \", cpu_exec_time / gpu_exec_time)\n",
        "    print()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(dim_sizes, cpu_times, label='CPU')\n",
        "plt.plot(dim_sizes, cuda_times, label='GPU')\n",
        "plt.legend()\n",
        "plt.title('CPU vs GPU performance')\n",
        "plt.xlabel('Dimension: n x n')\n",
        "plt.ylabel('Execution time (s)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimension size:  (1, 1)\n",
            "CPU\n",
            "Time elapsed:  0.0004711151123046875\n",
            "CUDA\n",
            "Time elapsed:  0.0073659420013427734\n",
            "Speedup:  0.0639585693477909\n",
            "\n",
            "Dimension size:  (10, 10)\n",
            "CPU\n",
            "Time elapsed:  0.0014138221740722656\n",
            "CUDA\n",
            "Time elapsed:  0.001322031021118164\n",
            "Speedup:  1.0694319206492335\n",
            "\n",
            "Dimension size:  (20, 20)\n",
            "CPU\n",
            "Time elapsed:  0.0011703968048095703\n",
            "CUDA\n",
            "Time elapsed:  0.001039743423461914\n",
            "Speedup:  1.1256592524650308\n",
            "\n",
            "Dimension size:  (50, 50)\n",
            "CPU\n",
            "Time elapsed:  0.0016505718231201172\n",
            "CUDA\n",
            "Time elapsed:  0.0012559890747070312\n",
            "Speedup:  1.3141609719058467\n",
            "\n",
            "Dimension size:  (100, 100)\n",
            "CPU\n",
            "Time elapsed:  0.0038781166076660156\n",
            "CUDA\n",
            "Time elapsed:  0.0009424686431884766\n",
            "Speedup:  4.114849481406527\n",
            "\n",
            "Dimension size:  (200, 200)\n",
            "CPU\n",
            "Time elapsed:  0.035538673400878906\n",
            "CUDA\n",
            "Time elapsed:  0.0008263587951660156\n",
            "Speedup:  43.00634737449509\n",
            "\n",
            "Dimension size:  (300, 300)\n",
            "CPU\n",
            "Time elapsed:  0.05239534378051758\n",
            "CUDA\n",
            "Time elapsed:  0.0008790493011474609\n",
            "Speedup:  59.604556550040684\n",
            "\n",
            "Dimension size:  (500, 500)\n",
            "CPU\n",
            "Time elapsed:  0.22474002838134766\n",
            "CUDA\n",
            "Time elapsed:  0.0009174346923828125\n",
            "Speedup:  244.96569646569645\n",
            "\n",
            "Dimension size:  (1000, 1000)\n",
            "CPU\n",
            "Time elapsed:  1.7037785053253174\n",
            "CUDA\n",
            "Time elapsed:  0.0008444786071777344\n",
            "Speedup:  2017.550818746471\n",
            "\n",
            "Dimension size:  (2000, 2000)\n",
            "CPU\n",
            "Time elapsed:  13.327194213867188\n",
            "CUDA\n",
            "Time elapsed:  0.0009627342224121094\n",
            "Speedup:  13843.066864784547\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaguETdbCBTD"
      },
      "source": [
        "## Implementing a Linear Model\n",
        "\n",
        "Remembering the description for a linear model\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{w}^T \\mathbf{x} + \\mathbf{b} \\\\\n",
        "\\mathbf{x} = (x_1, \\dots, x_n) \\\\\n",
        "\\mathbf{w} = (w_1, \\dots, w_n)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Here, $\\mathbf{y}$ and $\\mathbf{b}$ are scalars and $\\mathbf{w}$ and $\\mathbf{x}$ are vectors of dimension $d$. We can use the add-a-dimension trick to convert the equation to \n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{w}^T \\mathbf{x} \\tag*{[$w_0 = b, x_0 = 1$, remaining terms remain the same]}\n",
        "$$\n",
        "\n",
        "If we have a dataset $\\mathcal{D} = (X, Y) = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dots, (x^{(n)}, y^{(n)})\\}$ and a weight vector $\\mathbf{w}$. We can compute the outputs for the whole dataset as\n",
        "\n",
        "$$\n",
        "\\hat{Y} = \\mathbf{w}^T \\mathbf{X}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuxDI-ozJ-kH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "f46ed67e-e98c-4773-f844-a6009ac490f2"
      },
      "source": [
        "true_w = torch.FloatTensor([3, 4])\n",
        "x1 = torch.linspace(0, 20, 100)\n",
        "x2 = torch.linspace(10, 30, 100)\n",
        "\n",
        "# y = 3*x1 + 4*x2\n",
        "\n",
        "X = torch.stack((x1, x2)).T\n",
        "\n",
        "Y = X @ true_w + torch.normal(0, 3, size=(100,))\n",
        "\n",
        "plt.scatter(X[:, 0], Y)\n",
        "plt.title('y vs. x1')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JRcfEpvKDnn"
      },
      "source": [
        "\n",
        "$$\\hat{Y} = \\begin{pmatrix}\\hat{y}^{(1)} \\\\ \\hat{y}^{(2)} \\\\ \\dots \\\\ \\hat{y}^{(n)}\\end{pmatrix}$$\n",
        "\n",
        "We can calculate the error of our model deviating from the true values using the mean squared error metric\n",
        "\n",
        "$$\n",
        "    \\mathcal{L}(Y, \\hat{Y}) = \\frac{1}{2N}\\sum_{i=1}^{n}(y^{(i)} - \\hat{y}^{(i)})^2 \\\\\n",
        "    \\mathcal{L}(Y, \\hat{Y}) = \\frac{1}{2N}(Y - \\hat{Y})^T . (Y - \\hat{Y}) \\\\\n",
        "    \\mathcal{L}(Y, \\hat{Y}) = \\frac{1}{2N}(Y - \\mathbf{w}^T \\mathbf{X} )^T . (Y - \\mathbf{w}^T \\mathbf{X})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OP43iDIJNmV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a650d61-02bb-4784-955b-4531b889e40c"
      },
      "source": [
        "# Let's assume that by some \"magic\" we got the true parameters\n",
        "estimated_w = true_w\n",
        "\n",
        "def mse_loss(Y, Y_hat):\n",
        "    # Calculate the mean squared error loss of our estimated outputs, \n",
        "    # with respect to our inputs\n",
        "\n",
        "    return 1 / (2 * Y.shape[0]) * torch.norm(Y - Y_hat)**2\n",
        "\n",
        "    # raise NotImplementedError()\n",
        "\n",
        "print(\"Loss: \", mse_loss(Y, X @ estimated_w))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(3.9061)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhu-G1O8JN6z"
      },
      "source": [
        "### Closed Form Solution\n",
        "We obtain the best fit by minimizing the loss with respect to $W$ which can be obtained by taking the gradient w.r.t. $W$ and setting it to 0.\n",
        "\n",
        "We can find a closed form solution $w$ as follows:\n",
        "\n",
        "$$\n",
        "w = (X^T X)^{-1}X^TY\n",
        "$$\n",
        "\n",
        "*Exercise: Can you derive this mathematically?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agiu8q8EHwaI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b906f49f-7820-4e12-ae3a-e43009eb0c64"
      },
      "source": [
        "estimated_w = torch.inverse((X.T @ X)) @ X.T @ Y\n",
        "\n",
        "print(estimated_w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3.1191, 3.9276])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXI2oNmpI-hd"
      },
      "source": [
        "### Linear Model Estimation using Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWAW7FyAj3Q1"
      },
      "source": [
        "#### Deriving the Gradient Descent update rule\n",
        "Alternatively, we can optimize using the gradient descent algorithm. This algorithm is based on the fact that the gradient of the loss function w.r.t. the parameters will be 0 at an optima and non-zero elsewhere. At a point close to an optima, the gradient will be a vector that will be pointing away from the minima.\n",
        "\n",
        "Using this information, we can push the parameters towards the minimum by using the following update rule\n",
        "\n",
        "$$\n",
        "w = w - \\eta \\nabla_w L(Y, \\hat{Y})\n",
        "$$\n",
        "\n",
        "In the above expression, $\\nabla_w L(Y, \\hat{Y})$ denotes the gradient of the loss function with respect to $w$ and $\\eta$ is a learning rate parameter that can be either constant or changing. \n",
        "\n",
        "This is the general form of the gradient descent algorithm that works for any differentiable function of the parameters.\n",
        "\n",
        "For the case of our linear model, we can show the gradient to be\n",
        "\n",
        "$$\n",
        " \\nabla_w L(Y, \\hat{Y}) = \\frac{1}{N}(\\hat{Y} - Y) X\n",
        "$$\n",
        "\n",
        "*Exercise: Derive this*\n",
        "\n",
        "This yields the update rule to be\n",
        "\n",
        "$$\n",
        "w = w - \\eta . \\frac{1}{N}(\\hat{Y} - Y) X\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0Zi0NWNIN_o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb0ea5c6-5aa5-4bc7-b59d-54623a6ff8ec"
      },
      "source": [
        "\n",
        "# w is the parameter to optimize\n",
        "w = torch.normal(mean=0, std=10, size=(X.shape[1],))\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "N_ITERATIONS = 10000\n",
        "\n",
        "\n",
        "prev_loss = torch.FloatTensor([float('inf')])\n",
        "\n",
        "for i in range(N_ITERATIONS):\n",
        "\n",
        "    Y_hat = X @ w\n",
        "\n",
        "    loss = mse_loss(Y, Y_hat)\n",
        "    print(f\"Step {i + 1} Loss: \", loss)\n",
        "    if torch.isclose(prev_loss, loss, atol=1e-4):\n",
        "        break\n",
        "    \n",
        "    w = w - LEARNING_RATE * ((Y_hat - Y) @ X) / Y.shape[0]\n",
        "    # Other ways to optimize: \n",
        "    # https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms\n",
        "    prev_loss = loss\n",
        "\n",
        "print(\"Estimated w:\", w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Loss:  tensor(16170.5469)\n",
            "Step 2 Loss:  tensor(3200.1890)\n",
            "Step 3 Loss:  tensor(710.3334)\n",
            "Step 4 Loss:  tensor(231.4626)\n",
            "Step 5 Loss:  tensor(138.4672)\n",
            "Step 6 Loss:  tensor(119.5261)\n",
            "Step 7 Loss:  tensor(114.8074)\n",
            "Step 8 Loss:  tensor(112.8309)\n",
            "Step 9 Loss:  tensor(111.3934)\n",
            "Step 10 Loss:  tensor(110.0721)\n",
            "Step 11 Loss:  tensor(108.7857)\n",
            "Step 12 Loss:  tensor(107.5186)\n",
            "Step 13 Loss:  tensor(106.2673)\n",
            "Step 14 Loss:  tensor(105.0313)\n",
            "Step 15 Loss:  tensor(103.8102)\n",
            "Step 16 Loss:  tensor(102.6039)\n",
            "Step 17 Loss:  tensor(101.4122)\n",
            "Step 18 Loss:  tensor(100.2348)\n",
            "Step 19 Loss:  tensor(99.0717)\n",
            "Step 20 Loss:  tensor(97.9225)\n",
            "Step 21 Loss:  tensor(96.7873)\n",
            "Step 22 Loss:  tensor(95.6657)\n",
            "Step 23 Loss:  tensor(94.5576)\n",
            "Step 24 Loss:  tensor(93.4630)\n",
            "Step 25 Loss:  tensor(92.3815)\n",
            "Step 26 Loss:  tensor(91.3131)\n",
            "Step 27 Loss:  tensor(90.2576)\n",
            "Step 28 Loss:  tensor(89.2148)\n",
            "Step 29 Loss:  tensor(88.1846)\n",
            "Step 30 Loss:  tensor(87.1669)\n",
            "Step 31 Loss:  tensor(86.1614)\n",
            "Step 32 Loss:  tensor(85.1680)\n",
            "Step 33 Loss:  tensor(84.1866)\n",
            "Step 34 Loss:  tensor(83.2171)\n",
            "Step 35 Loss:  tensor(82.2593)\n",
            "Step 36 Loss:  tensor(81.3130)\n",
            "Step 37 Loss:  tensor(80.3781)\n",
            "Step 38 Loss:  tensor(79.4546)\n",
            "Step 39 Loss:  tensor(78.5421)\n",
            "Step 40 Loss:  tensor(77.6407)\n",
            "Step 41 Loss:  tensor(76.7501)\n",
            "Step 42 Loss:  tensor(75.8704)\n",
            "Step 43 Loss:  tensor(75.0011)\n",
            "Step 44 Loss:  tensor(74.1425)\n",
            "Step 45 Loss:  tensor(73.2941)\n",
            "Step 46 Loss:  tensor(72.4560)\n",
            "Step 47 Loss:  tensor(71.6280)\n",
            "Step 48 Loss:  tensor(70.8101)\n",
            "Step 49 Loss:  tensor(70.0019)\n",
            "Step 50 Loss:  tensor(69.2036)\n",
            "Step 51 Loss:  tensor(68.4148)\n",
            "Step 52 Loss:  tensor(67.6356)\n",
            "Step 53 Loss:  tensor(66.8658)\n",
            "Step 54 Loss:  tensor(66.1052)\n",
            "Step 55 Loss:  tensor(65.3539)\n",
            "Step 56 Loss:  tensor(64.6116)\n",
            "Step 57 Loss:  tensor(63.8783)\n",
            "Step 58 Loss:  tensor(63.1538)\n",
            "Step 59 Loss:  tensor(62.4380)\n",
            "Step 60 Loss:  tensor(61.7309)\n",
            "Step 61 Loss:  tensor(61.0324)\n",
            "Step 62 Loss:  tensor(60.3422)\n",
            "Step 63 Loss:  tensor(59.6604)\n",
            "Step 64 Loss:  tensor(58.9868)\n",
            "Step 65 Loss:  tensor(58.3214)\n",
            "Step 66 Loss:  tensor(57.6639)\n",
            "Step 67 Loss:  tensor(57.0144)\n",
            "Step 68 Loss:  tensor(56.3728)\n",
            "Step 69 Loss:  tensor(55.7388)\n",
            "Step 70 Loss:  tensor(55.1126)\n",
            "Step 71 Loss:  tensor(54.4939)\n",
            "Step 72 Loss:  tensor(53.8826)\n",
            "Step 73 Loss:  tensor(53.2787)\n",
            "Step 74 Loss:  tensor(52.6821)\n",
            "Step 75 Loss:  tensor(52.0928)\n",
            "Step 76 Loss:  tensor(51.5105)\n",
            "Step 77 Loss:  tensor(50.9352)\n",
            "Step 78 Loss:  tensor(50.3669)\n",
            "Step 79 Loss:  tensor(49.8055)\n",
            "Step 80 Loss:  tensor(49.2508)\n",
            "Step 81 Loss:  tensor(48.7028)\n",
            "Step 82 Loss:  tensor(48.1614)\n",
            "Step 83 Loss:  tensor(47.6266)\n",
            "Step 84 Loss:  tensor(47.0982)\n",
            "Step 85 Loss:  tensor(46.5762)\n",
            "Step 86 Loss:  tensor(46.0605)\n",
            "Step 87 Loss:  tensor(45.5510)\n",
            "Step 88 Loss:  tensor(45.0477)\n",
            "Step 89 Loss:  tensor(44.5504)\n",
            "Step 90 Loss:  tensor(44.0591)\n",
            "Step 91 Loss:  tensor(43.5738)\n",
            "Step 92 Loss:  tensor(43.0943)\n",
            "Step 93 Loss:  tensor(42.6206)\n",
            "Step 94 Loss:  tensor(42.1526)\n",
            "Step 95 Loss:  tensor(41.6903)\n",
            "Step 96 Loss:  tensor(41.2335)\n",
            "Step 97 Loss:  tensor(40.7823)\n",
            "Step 98 Loss:  tensor(40.3365)\n",
            "Step 99 Loss:  tensor(39.8960)\n",
            "Step 100 Loss:  tensor(39.4609)\n",
            "Step 101 Loss:  tensor(39.0311)\n",
            "Step 102 Loss:  tensor(38.6064)\n",
            "Step 103 Loss:  tensor(38.1869)\n",
            "Step 104 Loss:  tensor(37.7724)\n",
            "Step 105 Loss:  tensor(37.3629)\n",
            "Step 106 Loss:  tensor(36.9583)\n",
            "Step 107 Loss:  tensor(36.5587)\n",
            "Step 108 Loss:  tensor(36.1638)\n",
            "Step 109 Loss:  tensor(35.7738)\n",
            "Step 110 Loss:  tensor(35.3884)\n",
            "Step 111 Loss:  tensor(35.0077)\n",
            "Step 112 Loss:  tensor(34.6316)\n",
            "Step 113 Loss:  tensor(34.2600)\n",
            "Step 114 Loss:  tensor(33.8929)\n",
            "Step 115 Loss:  tensor(33.5302)\n",
            "Step 116 Loss:  tensor(33.1719)\n",
            "Step 117 Loss:  tensor(32.8179)\n",
            "Step 118 Loss:  tensor(32.4682)\n",
            "Step 119 Loss:  tensor(32.1227)\n",
            "Step 120 Loss:  tensor(31.7814)\n",
            "Step 121 Loss:  tensor(31.4442)\n",
            "Step 122 Loss:  tensor(31.1111)\n",
            "Step 123 Loss:  tensor(30.7820)\n",
            "Step 124 Loss:  tensor(30.4569)\n",
            "Step 125 Loss:  tensor(30.1356)\n",
            "Step 126 Loss:  tensor(29.8183)\n",
            "Step 127 Loss:  tensor(29.5048)\n",
            "Step 128 Loss:  tensor(29.1951)\n",
            "Step 129 Loss:  tensor(28.8891)\n",
            "Step 130 Loss:  tensor(28.5868)\n",
            "Step 131 Loss:  tensor(28.2882)\n",
            "Step 132 Loss:  tensor(27.9931)\n",
            "Step 133 Loss:  tensor(27.7016)\n",
            "Step 134 Loss:  tensor(27.4136)\n",
            "Step 135 Loss:  tensor(27.1292)\n",
            "Step 136 Loss:  tensor(26.8481)\n",
            "Step 137 Loss:  tensor(26.5704)\n",
            "Step 138 Loss:  tensor(26.2961)\n",
            "Step 139 Loss:  tensor(26.0251)\n",
            "Step 140 Loss:  tensor(25.7574)\n",
            "Step 141 Loss:  tensor(25.4929)\n",
            "Step 142 Loss:  tensor(25.2315)\n",
            "Step 143 Loss:  tensor(24.9734)\n",
            "Step 144 Loss:  tensor(24.7183)\n",
            "Step 145 Loss:  tensor(24.4663)\n",
            "Step 146 Loss:  tensor(24.2174)\n",
            "Step 147 Loss:  tensor(23.9715)\n",
            "Step 148 Loss:  tensor(23.7285)\n",
            "Step 149 Loss:  tensor(23.4885)\n",
            "Step 150 Loss:  tensor(23.2514)\n",
            "Step 151 Loss:  tensor(23.0171)\n",
            "Step 152 Loss:  tensor(22.7857)\n",
            "Step 153 Loss:  tensor(22.5570)\n",
            "Step 154 Loss:  tensor(22.3311)\n",
            "Step 155 Loss:  tensor(22.1080)\n",
            "Step 156 Loss:  tensor(21.8875)\n",
            "Step 157 Loss:  tensor(21.6697)\n",
            "Step 158 Loss:  tensor(21.4545)\n",
            "Step 159 Loss:  tensor(21.2419)\n",
            "Step 160 Loss:  tensor(21.0319)\n",
            "Step 161 Loss:  tensor(20.8244)\n",
            "Step 162 Loss:  tensor(20.6194)\n",
            "Step 163 Loss:  tensor(20.4169)\n",
            "Step 164 Loss:  tensor(20.2168)\n",
            "Step 165 Loss:  tensor(20.0192)\n",
            "Step 166 Loss:  tensor(19.8239)\n",
            "Step 167 Loss:  tensor(19.6310)\n",
            "Step 168 Loss:  tensor(19.4404)\n",
            "Step 169 Loss:  tensor(19.2521)\n",
            "Step 170 Loss:  tensor(19.0661)\n",
            "Step 171 Loss:  tensor(18.8823)\n",
            "Step 172 Loss:  tensor(18.7008)\n",
            "Step 173 Loss:  tensor(18.5214)\n",
            "Step 174 Loss:  tensor(18.3442)\n",
            "Step 175 Loss:  tensor(18.1692)\n",
            "Step 176 Loss:  tensor(17.9962)\n",
            "Step 177 Loss:  tensor(17.8254)\n",
            "Step 178 Loss:  tensor(17.6566)\n",
            "Step 179 Loss:  tensor(17.4898)\n",
            "Step 180 Loss:  tensor(17.3251)\n",
            "Step 181 Loss:  tensor(17.1623)\n",
            "Step 182 Loss:  tensor(17.0015)\n",
            "Step 183 Loss:  tensor(16.8426)\n",
            "Step 184 Loss:  tensor(16.6857)\n",
            "Step 185 Loss:  tensor(16.5307)\n",
            "Step 186 Loss:  tensor(16.3775)\n",
            "Step 187 Loss:  tensor(16.2262)\n",
            "Step 188 Loss:  tensor(16.0767)\n",
            "Step 189 Loss:  tensor(15.9290)\n",
            "Step 190 Loss:  tensor(15.7830)\n",
            "Step 191 Loss:  tensor(15.6389)\n",
            "Step 192 Loss:  tensor(15.4965)\n",
            "Step 193 Loss:  tensor(15.3558)\n",
            "Step 194 Loss:  tensor(15.2168)\n",
            "Step 195 Loss:  tensor(15.0795)\n",
            "Step 196 Loss:  tensor(14.9438)\n",
            "Step 197 Loss:  tensor(14.8098)\n",
            "Step 198 Loss:  tensor(14.6773)\n",
            "Step 199 Loss:  tensor(14.5465)\n",
            "Step 200 Loss:  tensor(14.4173)\n",
            "Step 201 Loss:  tensor(14.2896)\n",
            "Step 202 Loss:  tensor(14.1635)\n",
            "Step 203 Loss:  tensor(14.0389)\n",
            "Step 204 Loss:  tensor(13.9158)\n",
            "Step 205 Loss:  tensor(13.7941)\n",
            "Step 206 Loss:  tensor(13.6740)\n",
            "Step 207 Loss:  tensor(13.5553)\n",
            "Step 208 Loss:  tensor(13.4380)\n",
            "Step 209 Loss:  tensor(13.3221)\n",
            "Step 210 Loss:  tensor(13.2077)\n",
            "Step 211 Loss:  tensor(13.0946)\n",
            "Step 212 Loss:  tensor(12.9829)\n",
            "Step 213 Loss:  tensor(12.8725)\n",
            "Step 214 Loss:  tensor(12.7635)\n",
            "Step 215 Loss:  tensor(12.6558)\n",
            "Step 216 Loss:  tensor(12.5493)\n",
            "Step 217 Loss:  tensor(12.4442)\n",
            "Step 218 Loss:  tensor(12.3403)\n",
            "Step 219 Loss:  tensor(12.2377)\n",
            "Step 220 Loss:  tensor(12.1363)\n",
            "Step 221 Loss:  tensor(12.0362)\n",
            "Step 222 Loss:  tensor(11.9373)\n",
            "Step 223 Loss:  tensor(11.8395)\n",
            "Step 224 Loss:  tensor(11.7429)\n",
            "Step 225 Loss:  tensor(11.6475)\n",
            "Step 226 Loss:  tensor(11.5533)\n",
            "Step 227 Loss:  tensor(11.4602)\n",
            "Step 228 Loss:  tensor(11.3682)\n",
            "Step 229 Loss:  tensor(11.2773)\n",
            "Step 230 Loss:  tensor(11.1875)\n",
            "Step 231 Loss:  tensor(11.0988)\n",
            "Step 232 Loss:  tensor(11.0112)\n",
            "Step 233 Loss:  tensor(10.9246)\n",
            "Step 234 Loss:  tensor(10.8390)\n",
            "Step 235 Loss:  tensor(10.7545)\n",
            "Step 236 Loss:  tensor(10.6711)\n",
            "Step 237 Loss:  tensor(10.5886)\n",
            "Step 238 Loss:  tensor(10.5071)\n",
            "Step 239 Loss:  tensor(10.4266)\n",
            "Step 240 Loss:  tensor(10.3471)\n",
            "Step 241 Loss:  tensor(10.2685)\n",
            "Step 242 Loss:  tensor(10.1909)\n",
            "Step 243 Loss:  tensor(10.1142)\n",
            "Step 244 Loss:  tensor(10.0385)\n",
            "Step 245 Loss:  tensor(9.9636)\n",
            "Step 246 Loss:  tensor(9.8897)\n",
            "Step 247 Loss:  tensor(9.8167)\n",
            "Step 248 Loss:  tensor(9.7445)\n",
            "Step 249 Loss:  tensor(9.6732)\n",
            "Step 250 Loss:  tensor(9.6028)\n",
            "Step 251 Loss:  tensor(9.5332)\n",
            "Step 252 Loss:  tensor(9.4644)\n",
            "Step 253 Loss:  tensor(9.3965)\n",
            "Step 254 Loss:  tensor(9.3294)\n",
            "Step 255 Loss:  tensor(9.2632)\n",
            "Step 256 Loss:  tensor(9.1977)\n",
            "Step 257 Loss:  tensor(9.1330)\n",
            "Step 258 Loss:  tensor(9.0691)\n",
            "Step 259 Loss:  tensor(9.0059)\n",
            "Step 260 Loss:  tensor(8.9435)\n",
            "Step 261 Loss:  tensor(8.8819)\n",
            "Step 262 Loss:  tensor(8.8210)\n",
            "Step 263 Loss:  tensor(8.7609)\n",
            "Step 264 Loss:  tensor(8.7015)\n",
            "Step 265 Loss:  tensor(8.6427)\n",
            "Step 266 Loss:  tensor(8.5847)\n",
            "Step 267 Loss:  tensor(8.5274)\n",
            "Step 268 Loss:  tensor(8.4708)\n",
            "Step 269 Loss:  tensor(8.4149)\n",
            "Step 270 Loss:  tensor(8.3597)\n",
            "Step 271 Loss:  tensor(8.3051)\n",
            "Step 272 Loss:  tensor(8.2512)\n",
            "Step 273 Loss:  tensor(8.1979)\n",
            "Step 274 Loss:  tensor(8.1453)\n",
            "Step 275 Loss:  tensor(8.0933)\n",
            "Step 276 Loss:  tensor(8.0419)\n",
            "Step 277 Loss:  tensor(7.9911)\n",
            "Step 278 Loss:  tensor(7.9410)\n",
            "Step 279 Loss:  tensor(7.8915)\n",
            "Step 280 Loss:  tensor(7.8425)\n",
            "Step 281 Loss:  tensor(7.7942)\n",
            "Step 282 Loss:  tensor(7.7464)\n",
            "Step 283 Loss:  tensor(7.6993)\n",
            "Step 284 Loss:  tensor(7.6526)\n",
            "Step 285 Loss:  tensor(7.6066)\n",
            "Step 286 Loss:  tensor(7.5611)\n",
            "Step 287 Loss:  tensor(7.5162)\n",
            "Step 288 Loss:  tensor(7.4717)\n",
            "Step 289 Loss:  tensor(7.4279)\n",
            "Step 290 Loss:  tensor(7.3845)\n",
            "Step 291 Loss:  tensor(7.3417)\n",
            "Step 292 Loss:  tensor(7.2994)\n",
            "Step 293 Loss:  tensor(7.2576)\n",
            "Step 294 Loss:  tensor(7.2163)\n",
            "Step 295 Loss:  tensor(7.1756)\n",
            "Step 296 Loss:  tensor(7.1353)\n",
            "Step 297 Loss:  tensor(7.0955)\n",
            "Step 298 Loss:  tensor(7.0561)\n",
            "Step 299 Loss:  tensor(7.0173)\n",
            "Step 300 Loss:  tensor(6.9789)\n",
            "Step 301 Loss:  tensor(6.9410)\n",
            "Step 302 Loss:  tensor(6.9035)\n",
            "Step 303 Loss:  tensor(6.8665)\n",
            "Step 304 Loss:  tensor(6.8299)\n",
            "Step 305 Loss:  tensor(6.7938)\n",
            "Step 306 Loss:  tensor(6.7581)\n",
            "Step 307 Loss:  tensor(6.7228)\n",
            "Step 308 Loss:  tensor(6.6880)\n",
            "Step 309 Loss:  tensor(6.6536)\n",
            "Step 310 Loss:  tensor(6.6196)\n",
            "Step 311 Loss:  tensor(6.5860)\n",
            "Step 312 Loss:  tensor(6.5528)\n",
            "Step 313 Loss:  tensor(6.5201)\n",
            "Step 314 Loss:  tensor(6.4877)\n",
            "Step 315 Loss:  tensor(6.4557)\n",
            "Step 316 Loss:  tensor(6.4241)\n",
            "Step 317 Loss:  tensor(6.3928)\n",
            "Step 318 Loss:  tensor(6.3620)\n",
            "Step 319 Loss:  tensor(6.3315)\n",
            "Step 320 Loss:  tensor(6.3014)\n",
            "Step 321 Loss:  tensor(6.2717)\n",
            "Step 322 Loss:  tensor(6.2423)\n",
            "Step 323 Loss:  tensor(6.2132)\n",
            "Step 324 Loss:  tensor(6.1846)\n",
            "Step 325 Loss:  tensor(6.1562)\n",
            "Step 326 Loss:  tensor(6.1282)\n",
            "Step 327 Loss:  tensor(6.1006)\n",
            "Step 328 Loss:  tensor(6.0732)\n",
            "Step 329 Loss:  tensor(6.0462)\n",
            "Step 330 Loss:  tensor(6.0196)\n",
            "Step 331 Loss:  tensor(5.9932)\n",
            "Step 332 Loss:  tensor(5.9672)\n",
            "Step 333 Loss:  tensor(5.9415)\n",
            "Step 334 Loss:  tensor(5.9161)\n",
            "Step 335 Loss:  tensor(5.8910)\n",
            "Step 336 Loss:  tensor(5.8662)\n",
            "Step 337 Loss:  tensor(5.8417)\n",
            "Step 338 Loss:  tensor(5.8175)\n",
            "Step 339 Loss:  tensor(5.7936)\n",
            "Step 340 Loss:  tensor(5.7700)\n",
            "Step 341 Loss:  tensor(5.7466)\n",
            "Step 342 Loss:  tensor(5.7236)\n",
            "Step 343 Loss:  tensor(5.7008)\n",
            "Step 344 Loss:  tensor(5.6783)\n",
            "Step 345 Loss:  tensor(5.6561)\n",
            "Step 346 Loss:  tensor(5.6341)\n",
            "Step 347 Loss:  tensor(5.6124)\n",
            "Step 348 Loss:  tensor(5.5910)\n",
            "Step 349 Loss:  tensor(5.5698)\n",
            "Step 350 Loss:  tensor(5.5489)\n",
            "Step 351 Loss:  tensor(5.5282)\n",
            "Step 352 Loss:  tensor(5.5078)\n",
            "Step 353 Loss:  tensor(5.4876)\n",
            "Step 354 Loss:  tensor(5.4677)\n",
            "Step 355 Loss:  tensor(5.4480)\n",
            "Step 356 Loss:  tensor(5.4286)\n",
            "Step 357 Loss:  tensor(5.4093)\n",
            "Step 358 Loss:  tensor(5.3904)\n",
            "Step 359 Loss:  tensor(5.3716)\n",
            "Step 360 Loss:  tensor(5.3531)\n",
            "Step 361 Loss:  tensor(5.3348)\n",
            "Step 362 Loss:  tensor(5.3167)\n",
            "Step 363 Loss:  tensor(5.2988)\n",
            "Step 364 Loss:  tensor(5.2812)\n",
            "Step 365 Loss:  tensor(5.2637)\n",
            "Step 366 Loss:  tensor(5.2465)\n",
            "Step 367 Loss:  tensor(5.2295)\n",
            "Step 368 Loss:  tensor(5.2127)\n",
            "Step 369 Loss:  tensor(5.1961)\n",
            "Step 370 Loss:  tensor(5.1797)\n",
            "Step 371 Loss:  tensor(5.1634)\n",
            "Step 372 Loss:  tensor(5.1474)\n",
            "Step 373 Loss:  tensor(5.1316)\n",
            "Step 374 Loss:  tensor(5.1160)\n",
            "Step 375 Loss:  tensor(5.1005)\n",
            "Step 376 Loss:  tensor(5.0853)\n",
            "Step 377 Loss:  tensor(5.0702)\n",
            "Step 378 Loss:  tensor(5.0553)\n",
            "Step 379 Loss:  tensor(5.0406)\n",
            "Step 380 Loss:  tensor(5.0261)\n",
            "Step 381 Loss:  tensor(5.0117)\n",
            "Step 382 Loss:  tensor(4.9975)\n",
            "Step 383 Loss:  tensor(4.9835)\n",
            "Step 384 Loss:  tensor(4.9697)\n",
            "Step 385 Loss:  tensor(4.9560)\n",
            "Step 386 Loss:  tensor(4.9425)\n",
            "Step 387 Loss:  tensor(4.9291)\n",
            "Step 388 Loss:  tensor(4.9159)\n",
            "Step 389 Loss:  tensor(4.9029)\n",
            "Step 390 Loss:  tensor(4.8900)\n",
            "Step 391 Loss:  tensor(4.8773)\n",
            "Step 392 Loss:  tensor(4.8648)\n",
            "Step 393 Loss:  tensor(4.8523)\n",
            "Step 394 Loss:  tensor(4.8401)\n",
            "Step 395 Loss:  tensor(4.8280)\n",
            "Step 396 Loss:  tensor(4.8160)\n",
            "Step 397 Loss:  tensor(4.8042)\n",
            "Step 398 Loss:  tensor(4.7925)\n",
            "Step 399 Loss:  tensor(4.7809)\n",
            "Step 400 Loss:  tensor(4.7695)\n",
            "Step 401 Loss:  tensor(4.7583)\n",
            "Step 402 Loss:  tensor(4.7472)\n",
            "Step 403 Loss:  tensor(4.7362)\n",
            "Step 404 Loss:  tensor(4.7253)\n",
            "Step 405 Loss:  tensor(4.7146)\n",
            "Step 406 Loss:  tensor(4.7040)\n",
            "Step 407 Loss:  tensor(4.6935)\n",
            "Step 408 Loss:  tensor(4.6832)\n",
            "Step 409 Loss:  tensor(4.6729)\n",
            "Step 410 Loss:  tensor(4.6628)\n",
            "Step 411 Loss:  tensor(4.6529)\n",
            "Step 412 Loss:  tensor(4.6430)\n",
            "Step 413 Loss:  tensor(4.6333)\n",
            "Step 414 Loss:  tensor(4.6236)\n",
            "Step 415 Loss:  tensor(4.6141)\n",
            "Step 416 Loss:  tensor(4.6048)\n",
            "Step 417 Loss:  tensor(4.5955)\n",
            "Step 418 Loss:  tensor(4.5863)\n",
            "Step 419 Loss:  tensor(4.5773)\n",
            "Step 420 Loss:  tensor(4.5683)\n",
            "Step 421 Loss:  tensor(4.5595)\n",
            "Step 422 Loss:  tensor(4.5508)\n",
            "Step 423 Loss:  tensor(4.5421)\n",
            "Step 424 Loss:  tensor(4.5336)\n",
            "Step 425 Loss:  tensor(4.5252)\n",
            "Step 426 Loss:  tensor(4.5169)\n",
            "Step 427 Loss:  tensor(4.5087)\n",
            "Step 428 Loss:  tensor(4.5006)\n",
            "Step 429 Loss:  tensor(4.4925)\n",
            "Step 430 Loss:  tensor(4.4846)\n",
            "Step 431 Loss:  tensor(4.4768)\n",
            "Step 432 Loss:  tensor(4.4691)\n",
            "Step 433 Loss:  tensor(4.4614)\n",
            "Step 434 Loss:  tensor(4.4539)\n",
            "Step 435 Loss:  tensor(4.4464)\n",
            "Step 436 Loss:  tensor(4.4391)\n",
            "Step 437 Loss:  tensor(4.4318)\n",
            "Step 438 Loss:  tensor(4.4246)\n",
            "Step 439 Loss:  tensor(4.4175)\n",
            "Step 440 Loss:  tensor(4.4105)\n",
            "Step 441 Loss:  tensor(4.4035)\n",
            "Step 442 Loss:  tensor(4.3967)\n",
            "Step 443 Loss:  tensor(4.3899)\n",
            "Step 444 Loss:  tensor(4.3832)\n",
            "Step 445 Loss:  tensor(4.3766)\n",
            "Step 446 Loss:  tensor(4.3701)\n",
            "Step 447 Loss:  tensor(4.3637)\n",
            "Step 448 Loss:  tensor(4.3573)\n",
            "Step 449 Loss:  tensor(4.3510)\n",
            "Step 450 Loss:  tensor(4.3448)\n",
            "Step 451 Loss:  tensor(4.3387)\n",
            "Step 452 Loss:  tensor(4.3326)\n",
            "Step 453 Loss:  tensor(4.3266)\n",
            "Step 454 Loss:  tensor(4.3207)\n",
            "Step 455 Loss:  tensor(4.3149)\n",
            "Step 456 Loss:  tensor(4.3091)\n",
            "Step 457 Loss:  tensor(4.3034)\n",
            "Step 458 Loss:  tensor(4.2977)\n",
            "Step 459 Loss:  tensor(4.2922)\n",
            "Step 460 Loss:  tensor(4.2867)\n",
            "Step 461 Loss:  tensor(4.2812)\n",
            "Step 462 Loss:  tensor(4.2758)\n",
            "Step 463 Loss:  tensor(4.2705)\n",
            "Step 464 Loss:  tensor(4.2653)\n",
            "Step 465 Loss:  tensor(4.2601)\n",
            "Step 466 Loss:  tensor(4.2550)\n",
            "Step 467 Loss:  tensor(4.2499)\n",
            "Step 468 Loss:  tensor(4.2450)\n",
            "Step 469 Loss:  tensor(4.2400)\n",
            "Step 470 Loss:  tensor(4.2351)\n",
            "Step 471 Loss:  tensor(4.2303)\n",
            "Step 472 Loss:  tensor(4.2256)\n",
            "Step 473 Loss:  tensor(4.2209)\n",
            "Step 474 Loss:  tensor(4.2162)\n",
            "Step 475 Loss:  tensor(4.2116)\n",
            "Step 476 Loss:  tensor(4.2071)\n",
            "Step 477 Loss:  tensor(4.2026)\n",
            "Step 478 Loss:  tensor(4.1982)\n",
            "Step 479 Loss:  tensor(4.1938)\n",
            "Step 480 Loss:  tensor(4.1895)\n",
            "Step 481 Loss:  tensor(4.1853)\n",
            "Step 482 Loss:  tensor(4.1810)\n",
            "Step 483 Loss:  tensor(4.1769)\n",
            "Step 484 Loss:  tensor(4.1728)\n",
            "Step 485 Loss:  tensor(4.1687)\n",
            "Step 486 Loss:  tensor(4.1647)\n",
            "Step 487 Loss:  tensor(4.1607)\n",
            "Step 488 Loss:  tensor(4.1568)\n",
            "Step 489 Loss:  tensor(4.1529)\n",
            "Step 490 Loss:  tensor(4.1491)\n",
            "Step 491 Loss:  tensor(4.1453)\n",
            "Step 492 Loss:  tensor(4.1416)\n",
            "Step 493 Loss:  tensor(4.1379)\n",
            "Step 494 Loss:  tensor(4.1343)\n",
            "Step 495 Loss:  tensor(4.1307)\n",
            "Step 496 Loss:  tensor(4.1271)\n",
            "Step 497 Loss:  tensor(4.1236)\n",
            "Step 498 Loss:  tensor(4.1201)\n",
            "Step 499 Loss:  tensor(4.1167)\n",
            "Step 500 Loss:  tensor(4.1133)\n",
            "Step 501 Loss:  tensor(4.1100)\n",
            "Step 502 Loss:  tensor(4.1067)\n",
            "Step 503 Loss:  tensor(4.1034)\n",
            "Step 504 Loss:  tensor(4.1002)\n",
            "Step 505 Loss:  tensor(4.0970)\n",
            "Step 506 Loss:  tensor(4.0939)\n",
            "Step 507 Loss:  tensor(4.0907)\n",
            "Step 508 Loss:  tensor(4.0877)\n",
            "Step 509 Loss:  tensor(4.0846)\n",
            "Step 510 Loss:  tensor(4.0816)\n",
            "Step 511 Loss:  tensor(4.0787)\n",
            "Step 512 Loss:  tensor(4.0757)\n",
            "Step 513 Loss:  tensor(4.0729)\n",
            "Step 514 Loss:  tensor(4.0700)\n",
            "Step 515 Loss:  tensor(4.0672)\n",
            "Step 516 Loss:  tensor(4.0644)\n",
            "Step 517 Loss:  tensor(4.0616)\n",
            "Step 518 Loss:  tensor(4.0589)\n",
            "Step 519 Loss:  tensor(4.0562)\n",
            "Step 520 Loss:  tensor(4.0536)\n",
            "Step 521 Loss:  tensor(4.0509)\n",
            "Step 522 Loss:  tensor(4.0483)\n",
            "Step 523 Loss:  tensor(4.0458)\n",
            "Step 524 Loss:  tensor(4.0433)\n",
            "Step 525 Loss:  tensor(4.0408)\n",
            "Step 526 Loss:  tensor(4.0383)\n",
            "Step 527 Loss:  tensor(4.0358)\n",
            "Step 528 Loss:  tensor(4.0334)\n",
            "Step 529 Loss:  tensor(4.0311)\n",
            "Step 530 Loss:  tensor(4.0287)\n",
            "Step 531 Loss:  tensor(4.0264)\n",
            "Step 532 Loss:  tensor(4.0241)\n",
            "Step 533 Loss:  tensor(4.0218)\n",
            "Step 534 Loss:  tensor(4.0196)\n",
            "Step 535 Loss:  tensor(4.0174)\n",
            "Step 536 Loss:  tensor(4.0152)\n",
            "Step 537 Loss:  tensor(4.0130)\n",
            "Step 538 Loss:  tensor(4.0109)\n",
            "Step 539 Loss:  tensor(4.0088)\n",
            "Step 540 Loss:  tensor(4.0067)\n",
            "Step 541 Loss:  tensor(4.0046)\n",
            "Step 542 Loss:  tensor(4.0026)\n",
            "Step 543 Loss:  tensor(4.0006)\n",
            "Step 544 Loss:  tensor(3.9986)\n",
            "Step 545 Loss:  tensor(3.9966)\n",
            "Step 546 Loss:  tensor(3.9947)\n",
            "Step 547 Loss:  tensor(3.9928)\n",
            "Step 548 Loss:  tensor(3.9909)\n",
            "Step 549 Loss:  tensor(3.9890)\n",
            "Step 550 Loss:  tensor(3.9872)\n",
            "Step 551 Loss:  tensor(3.9854)\n",
            "Step 552 Loss:  tensor(3.9836)\n",
            "Step 553 Loss:  tensor(3.9818)\n",
            "Step 554 Loss:  tensor(3.9800)\n",
            "Step 555 Loss:  tensor(3.9783)\n",
            "Step 556 Loss:  tensor(3.9766)\n",
            "Step 557 Loss:  tensor(3.9749)\n",
            "Step 558 Loss:  tensor(3.9732)\n",
            "Step 559 Loss:  tensor(3.9715)\n",
            "Step 560 Loss:  tensor(3.9699)\n",
            "Step 561 Loss:  tensor(3.9683)\n",
            "Step 562 Loss:  tensor(3.9667)\n",
            "Step 563 Loss:  tensor(3.9651)\n",
            "Step 564 Loss:  tensor(3.9636)\n",
            "Step 565 Loss:  tensor(3.9620)\n",
            "Step 566 Loss:  tensor(3.9605)\n",
            "Step 567 Loss:  tensor(3.9590)\n",
            "Step 568 Loss:  tensor(3.9575)\n",
            "Step 569 Loss:  tensor(3.9561)\n",
            "Step 570 Loss:  tensor(3.9546)\n",
            "Step 571 Loss:  tensor(3.9532)\n",
            "Step 572 Loss:  tensor(3.9518)\n",
            "Step 573 Loss:  tensor(3.9504)\n",
            "Step 574 Loss:  tensor(3.9490)\n",
            "Step 575 Loss:  tensor(3.9476)\n",
            "Step 576 Loss:  tensor(3.9463)\n",
            "Step 577 Loss:  tensor(3.9449)\n",
            "Step 578 Loss:  tensor(3.9436)\n",
            "Step 579 Loss:  tensor(3.9423)\n",
            "Step 580 Loss:  tensor(3.9411)\n",
            "Step 581 Loss:  tensor(3.9398)\n",
            "Step 582 Loss:  tensor(3.9385)\n",
            "Step 583 Loss:  tensor(3.9373)\n",
            "Step 584 Loss:  tensor(3.9361)\n",
            "Step 585 Loss:  tensor(3.9349)\n",
            "Step 586 Loss:  tensor(3.9337)\n",
            "Step 587 Loss:  tensor(3.9325)\n",
            "Step 588 Loss:  tensor(3.9313)\n",
            "Step 589 Loss:  tensor(3.9302)\n",
            "Step 590 Loss:  tensor(3.9291)\n",
            "Step 591 Loss:  tensor(3.9279)\n",
            "Step 592 Loss:  tensor(3.9268)\n",
            "Step 593 Loss:  tensor(3.9257)\n",
            "Step 594 Loss:  tensor(3.9246)\n",
            "Step 595 Loss:  tensor(3.9236)\n",
            "Step 596 Loss:  tensor(3.9225)\n",
            "Step 597 Loss:  tensor(3.9215)\n",
            "Step 598 Loss:  tensor(3.9205)\n",
            "Step 599 Loss:  tensor(3.9194)\n",
            "Step 600 Loss:  tensor(3.9184)\n",
            "Step 601 Loss:  tensor(3.9174)\n",
            "Step 602 Loss:  tensor(3.9165)\n",
            "Step 603 Loss:  tensor(3.9155)\n",
            "Step 604 Loss:  tensor(3.9145)\n",
            "Step 605 Loss:  tensor(3.9136)\n",
            "Step 606 Loss:  tensor(3.9126)\n",
            "Step 607 Loss:  tensor(3.9117)\n",
            "Step 608 Loss:  tensor(3.9108)\n",
            "Step 609 Loss:  tensor(3.9099)\n",
            "Step 610 Loss:  tensor(3.9090)\n",
            "Step 611 Loss:  tensor(3.9081)\n",
            "Step 612 Loss:  tensor(3.9073)\n",
            "Step 613 Loss:  tensor(3.9064)\n",
            "Step 614 Loss:  tensor(3.9056)\n",
            "Step 615 Loss:  tensor(3.9047)\n",
            "Step 616 Loss:  tensor(3.9039)\n",
            "Step 617 Loss:  tensor(3.9031)\n",
            "Step 618 Loss:  tensor(3.9023)\n",
            "Step 619 Loss:  tensor(3.9015)\n",
            "Step 620 Loss:  tensor(3.9007)\n",
            "Step 621 Loss:  tensor(3.8999)\n",
            "Step 622 Loss:  tensor(3.8991)\n",
            "Step 623 Loss:  tensor(3.8984)\n",
            "Step 624 Loss:  tensor(3.8976)\n",
            "Step 625 Loss:  tensor(3.8969)\n",
            "Step 626 Loss:  tensor(3.8961)\n",
            "Step 627 Loss:  tensor(3.8954)\n",
            "Step 628 Loss:  tensor(3.8947)\n",
            "Step 629 Loss:  tensor(3.8940)\n",
            "Step 630 Loss:  tensor(3.8933)\n",
            "Step 631 Loss:  tensor(3.8926)\n",
            "Step 632 Loss:  tensor(3.8919)\n",
            "Step 633 Loss:  tensor(3.8912)\n",
            "Step 634 Loss:  tensor(3.8906)\n",
            "Step 635 Loss:  tensor(3.8899)\n",
            "Step 636 Loss:  tensor(3.8893)\n",
            "Step 637 Loss:  tensor(3.8886)\n",
            "Step 638 Loss:  tensor(3.8880)\n",
            "Step 639 Loss:  tensor(3.8874)\n",
            "Step 640 Loss:  tensor(3.8868)\n",
            "Step 641 Loss:  tensor(3.8861)\n",
            "Step 642 Loss:  tensor(3.8855)\n",
            "Step 643 Loss:  tensor(3.8849)\n",
            "Step 644 Loss:  tensor(3.8843)\n",
            "Step 645 Loss:  tensor(3.8838)\n",
            "Step 646 Loss:  tensor(3.8832)\n",
            "Step 647 Loss:  tensor(3.8826)\n",
            "Step 648 Loss:  tensor(3.8821)\n",
            "Step 649 Loss:  tensor(3.8815)\n",
            "Step 650 Loss:  tensor(3.8810)\n",
            "Step 651 Loss:  tensor(3.8804)\n",
            "Step 652 Loss:  tensor(3.8799)\n",
            "Step 653 Loss:  tensor(3.8794)\n",
            "Step 654 Loss:  tensor(3.8788)\n",
            "Step 655 Loss:  tensor(3.8783)\n",
            "Step 656 Loss:  tensor(3.8778)\n",
            "Step 657 Loss:  tensor(3.8773)\n",
            "Step 658 Loss:  tensor(3.8768)\n",
            "Step 659 Loss:  tensor(3.8763)\n",
            "Step 660 Loss:  tensor(3.8758)\n",
            "Step 661 Loss:  tensor(3.8753)\n",
            "Step 662 Loss:  tensor(3.8749)\n",
            "Step 663 Loss:  tensor(3.8744)\n",
            "Step 664 Loss:  tensor(3.8739)\n",
            "Step 665 Loss:  tensor(3.8735)\n",
            "Step 666 Loss:  tensor(3.8730)\n",
            "Step 667 Loss:  tensor(3.8726)\n",
            "Step 668 Loss:  tensor(3.8721)\n",
            "Step 669 Loss:  tensor(3.8717)\n",
            "Step 670 Loss:  tensor(3.8713)\n",
            "Step 671 Loss:  tensor(3.8709)\n",
            "Step 672 Loss:  tensor(3.8704)\n",
            "Step 673 Loss:  tensor(3.8700)\n",
            "Step 674 Loss:  tensor(3.8696)\n",
            "Step 675 Loss:  tensor(3.8692)\n",
            "Step 676 Loss:  tensor(3.8688)\n",
            "Step 677 Loss:  tensor(3.8684)\n",
            "Step 678 Loss:  tensor(3.8680)\n",
            "Step 679 Loss:  tensor(3.8676)\n",
            "Step 680 Loss:  tensor(3.8673)\n",
            "Step 681 Loss:  tensor(3.8669)\n",
            "Step 682 Loss:  tensor(3.8665)\n",
            "Step 683 Loss:  tensor(3.8661)\n",
            "Step 684 Loss:  tensor(3.8658)\n",
            "Step 685 Loss:  tensor(3.8654)\n",
            "Step 686 Loss:  tensor(3.8651)\n",
            "Step 687 Loss:  tensor(3.8647)\n",
            "Step 688 Loss:  tensor(3.8644)\n",
            "Step 689 Loss:  tensor(3.8640)\n",
            "Step 690 Loss:  tensor(3.8637)\n",
            "Step 691 Loss:  tensor(3.8634)\n",
            "Step 692 Loss:  tensor(3.8630)\n",
            "Step 693 Loss:  tensor(3.8627)\n",
            "Step 694 Loss:  tensor(3.8624)\n",
            "Step 695 Loss:  tensor(3.8621)\n",
            "Step 696 Loss:  tensor(3.8618)\n",
            "Step 697 Loss:  tensor(3.8614)\n",
            "Step 698 Loss:  tensor(3.8611)\n",
            "Step 699 Loss:  tensor(3.8608)\n",
            "Step 700 Loss:  tensor(3.8605)\n",
            "Step 701 Loss:  tensor(3.8602)\n",
            "Step 702 Loss:  tensor(3.8599)\n",
            "Step 703 Loss:  tensor(3.8597)\n",
            "Step 704 Loss:  tensor(3.8594)\n",
            "Step 705 Loss:  tensor(3.8591)\n",
            "Step 706 Loss:  tensor(3.8588)\n",
            "Step 707 Loss:  tensor(3.8585)\n",
            "Step 708 Loss:  tensor(3.8583)\n",
            "Step 709 Loss:  tensor(3.8580)\n",
            "Step 710 Loss:  tensor(3.8577)\n",
            "Step 711 Loss:  tensor(3.8575)\n",
            "Step 712 Loss:  tensor(3.8572)\n",
            "Step 713 Loss:  tensor(3.8570)\n",
            "Step 714 Loss:  tensor(3.8567)\n",
            "Step 715 Loss:  tensor(3.8565)\n",
            "Step 716 Loss:  tensor(3.8562)\n",
            "Step 717 Loss:  tensor(3.8560)\n",
            "Step 718 Loss:  tensor(3.8557)\n",
            "Step 719 Loss:  tensor(3.8555)\n",
            "Step 720 Loss:  tensor(3.8553)\n",
            "Step 721 Loss:  tensor(3.8550)\n",
            "Step 722 Loss:  tensor(3.8548)\n",
            "Step 723 Loss:  tensor(3.8546)\n",
            "Step 724 Loss:  tensor(3.8544)\n",
            "Step 725 Loss:  tensor(3.8541)\n",
            "Step 726 Loss:  tensor(3.8539)\n",
            "Step 727 Loss:  tensor(3.8537)\n",
            "Step 728 Loss:  tensor(3.8535)\n",
            "Step 729 Loss:  tensor(3.8533)\n",
            "Step 730 Loss:  tensor(3.8531)\n",
            "Step 731 Loss:  tensor(3.8529)\n",
            "Step 732 Loss:  tensor(3.8527)\n",
            "Step 733 Loss:  tensor(3.8525)\n",
            "Step 734 Loss:  tensor(3.8523)\n",
            "Step 735 Loss:  tensor(3.8521)\n",
            "Step 736 Loss:  tensor(3.8519)\n",
            "Step 737 Loss:  tensor(3.8517)\n",
            "Step 738 Loss:  tensor(3.8515)\n",
            "Step 739 Loss:  tensor(3.8513)\n",
            "Step 740 Loss:  tensor(3.8511)\n",
            "Step 741 Loss:  tensor(3.8509)\n",
            "Step 742 Loss:  tensor(3.8508)\n",
            "Step 743 Loss:  tensor(3.8506)\n",
            "Step 744 Loss:  tensor(3.8504)\n",
            "Step 745 Loss:  tensor(3.8502)\n",
            "Step 746 Loss:  tensor(3.8501)\n",
            "Step 747 Loss:  tensor(3.8499)\n",
            "Step 748 Loss:  tensor(3.8497)\n",
            "Step 749 Loss:  tensor(3.8496)\n",
            "Step 750 Loss:  tensor(3.8494)\n",
            "Step 751 Loss:  tensor(3.8492)\n",
            "Step 752 Loss:  tensor(3.8491)\n",
            "Step 753 Loss:  tensor(3.8489)\n",
            "Step 754 Loss:  tensor(3.8488)\n",
            "Step 755 Loss:  tensor(3.8486)\n",
            "Step 756 Loss:  tensor(3.8485)\n",
            "Step 757 Loss:  tensor(3.8483)\n",
            "Step 758 Loss:  tensor(3.8482)\n",
            "Step 759 Loss:  tensor(3.8480)\n",
            "Step 760 Loss:  tensor(3.8479)\n",
            "Step 761 Loss:  tensor(3.8477)\n",
            "Step 762 Loss:  tensor(3.8476)\n",
            "Step 763 Loss:  tensor(3.8475)\n",
            "Estimated w: tensor([3.0652, 3.9571])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BP__FPZS27Y"
      },
      "source": [
        "### Gradient Descent using Autograd\n",
        "\n",
        "The most powerful feature of PyTorch, which is going to form the bedrock of any model that we develop, is the automatic differentiation mechanism. \n",
        "\n",
        "In our earlier case, we had to explicitly find the gradient of the loss function with respect to $w$ and formulate our update rule.\n",
        "\n",
        "If we had a way to automatically compute the gradient for any differentiable function (like our mean squared), we could use that mechanism to compute the gradient w.r.t. our desired variable $w$. Then, the update rule becomes greatly simplified since we only need to get the automatically computed gradient value!\n",
        "\n",
        "PyTorch uses the autograd mechanism to mark variables for which automatic differentiation needs to be done. We will need to specify that the gradient w.r.t. $w$ needs to be computed. Any variable that uses $w$ in any way, will also have its gradient computed. So, if our loss term is a function $L = f(w)$, then PyTorch will automatically compute $\\nabla_W L$ when required. \n",
        "\n",
        "We will need to call `loss.backward()` to compute the gradient of our loss term. The autograd mechanism will automatically populate our `w` with its gradient in a property called `w.grad`. This can be used to make our gradient descent algorithm work in a generic fashion.\n",
        "\n",
        "Andrej Karpathy's CSE 231N lectures on Backpropagation is a good video to understand what's happening in this mechanism: <a>https://www.youtube.com/watch?v=i94OvYb6noo<a/>.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToVVnMGGOBJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfebd4d0-8b4e-4d2e-d8de-841a7e0c60bd"
      },
      "source": [
        "# w is the parameter to optimize\n",
        "w = torch.normal(mean=0, std=10, size=(X.shape[1],))\n",
        "w.requires_grad = True\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "N_ITERATIONS = 1000\n",
        "\n",
        "\n",
        "prev_loss = torch.FloatTensor([float('inf')])\n",
        "\n",
        "for i in range(N_ITERATIONS):\n",
        "\n",
        "    Y_hat = X @ w\n",
        "\n",
        "    loss = mse_loss(Y, Y_hat)\n",
        "    print(f\"Step {i + 1} Loss: \", loss)\n",
        "    if torch.isclose(prev_loss, loss, atol=1e-4):\n",
        "        break\n",
        "    \n",
        "    \n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= LEARNING_RATE * w.grad\n",
        "        w.grad.zero_()\n",
        "    prev_loss = loss\n",
        "\n",
        "print(\"Estimated w\", w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Loss:  tensor(1729.6105, grad_fn=<MulBackward0>)\n",
            "Step 2 Loss:  tensor(380.1035, grad_fn=<MulBackward0>)\n",
            "Step 3 Loss:  tensor(120.6192, grad_fn=<MulBackward0>)\n",
            "Step 4 Loss:  tensor(70.2921, grad_fn=<MulBackward0>)\n",
            "Step 5 Loss:  tensor(60.1040, grad_fn=<MulBackward0>)\n",
            "Step 6 Loss:  tensor(57.6241, grad_fn=<MulBackward0>)\n",
            "Step 7 Loss:  tensor(56.6295, grad_fn=<MulBackward0>)\n",
            "Step 8 Loss:  tensor(55.9262, grad_fn=<MulBackward0>)\n",
            "Step 9 Loss:  tensor(55.2849, grad_fn=<MulBackward0>)\n",
            "Step 10 Loss:  tensor(54.6617, grad_fn=<MulBackward0>)\n",
            "Step 11 Loss:  tensor(54.0479, grad_fn=<MulBackward0>)\n",
            "Step 12 Loss:  tensor(53.4420, grad_fn=<MulBackward0>)\n",
            "Step 13 Loss:  tensor(52.8434, grad_fn=<MulBackward0>)\n",
            "Step 14 Loss:  tensor(52.2520, grad_fn=<MulBackward0>)\n",
            "Step 15 Loss:  tensor(51.6679, grad_fn=<MulBackward0>)\n",
            "Step 16 Loss:  tensor(51.0907, grad_fn=<MulBackward0>)\n",
            "Step 17 Loss:  tensor(50.5205, grad_fn=<MulBackward0>)\n",
            "Step 18 Loss:  tensor(49.9572, grad_fn=<MulBackward0>)\n",
            "Step 19 Loss:  tensor(49.4007, grad_fn=<MulBackward0>)\n",
            "Step 20 Loss:  tensor(48.8509, grad_fn=<MulBackward0>)\n",
            "Step 21 Loss:  tensor(48.3078, grad_fn=<MulBackward0>)\n",
            "Step 22 Loss:  tensor(47.7712, grad_fn=<MulBackward0>)\n",
            "Step 23 Loss:  tensor(47.2410, grad_fn=<MulBackward0>)\n",
            "Step 24 Loss:  tensor(46.7173, grad_fn=<MulBackward0>)\n",
            "Step 25 Loss:  tensor(46.1999, grad_fn=<MulBackward0>)\n",
            "Step 26 Loss:  tensor(45.6887, grad_fn=<MulBackward0>)\n",
            "Step 27 Loss:  tensor(45.1837, grad_fn=<MulBackward0>)\n",
            "Step 28 Loss:  tensor(44.6848, grad_fn=<MulBackward0>)\n",
            "Step 29 Loss:  tensor(44.1919, grad_fn=<MulBackward0>)\n",
            "Step 30 Loss:  tensor(43.7050, grad_fn=<MulBackward0>)\n",
            "Step 31 Loss:  tensor(43.2239, grad_fn=<MulBackward0>)\n",
            "Step 32 Loss:  tensor(42.7486, grad_fn=<MulBackward0>)\n",
            "Step 33 Loss:  tensor(42.2791, grad_fn=<MulBackward0>)\n",
            "Step 34 Loss:  tensor(41.8153, grad_fn=<MulBackward0>)\n",
            "Step 35 Loss:  tensor(41.3570, grad_fn=<MulBackward0>)\n",
            "Step 36 Loss:  tensor(40.9042, grad_fn=<MulBackward0>)\n",
            "Step 37 Loss:  tensor(40.4569, grad_fn=<MulBackward0>)\n",
            "Step 38 Loss:  tensor(40.0151, grad_fn=<MulBackward0>)\n",
            "Step 39 Loss:  tensor(39.5785, grad_fn=<MulBackward0>)\n",
            "Step 40 Loss:  tensor(39.1473, grad_fn=<MulBackward0>)\n",
            "Step 41 Loss:  tensor(38.7212, grad_fn=<MulBackward0>)\n",
            "Step 42 Loss:  tensor(38.3002, grad_fn=<MulBackward0>)\n",
            "Step 43 Loss:  tensor(37.8844, grad_fn=<MulBackward0>)\n",
            "Step 44 Loss:  tensor(37.4735, grad_fn=<MulBackward0>)\n",
            "Step 45 Loss:  tensor(37.0677, grad_fn=<MulBackward0>)\n",
            "Step 46 Loss:  tensor(36.6667, grad_fn=<MulBackward0>)\n",
            "Step 47 Loss:  tensor(36.2706, grad_fn=<MulBackward0>)\n",
            "Step 48 Loss:  tensor(35.8792, grad_fn=<MulBackward0>)\n",
            "Step 49 Loss:  tensor(35.4926, grad_fn=<MulBackward0>)\n",
            "Step 50 Loss:  tensor(35.1106, grad_fn=<MulBackward0>)\n",
            "Step 51 Loss:  tensor(34.7332, grad_fn=<MulBackward0>)\n",
            "Step 52 Loss:  tensor(34.3604, grad_fn=<MulBackward0>)\n",
            "Step 53 Loss:  tensor(33.9921, grad_fn=<MulBackward0>)\n",
            "Step 54 Loss:  tensor(33.6282, grad_fn=<MulBackward0>)\n",
            "Step 55 Loss:  tensor(33.2687, grad_fn=<MulBackward0>)\n",
            "Step 56 Loss:  tensor(32.9136, grad_fn=<MulBackward0>)\n",
            "Step 57 Loss:  tensor(32.5627, grad_fn=<MulBackward0>)\n",
            "Step 58 Loss:  tensor(32.2161, grad_fn=<MulBackward0>)\n",
            "Step 59 Loss:  tensor(31.8737, grad_fn=<MulBackward0>)\n",
            "Step 60 Loss:  tensor(31.5354, grad_fn=<MulBackward0>)\n",
            "Step 61 Loss:  tensor(31.2012, grad_fn=<MulBackward0>)\n",
            "Step 62 Loss:  tensor(30.8710, grad_fn=<MulBackward0>)\n",
            "Step 63 Loss:  tensor(30.5447, grad_fn=<MulBackward0>)\n",
            "Step 64 Loss:  tensor(30.2225, grad_fn=<MulBackward0>)\n",
            "Step 65 Loss:  tensor(29.9041, grad_fn=<MulBackward0>)\n",
            "Step 66 Loss:  tensor(29.5896, grad_fn=<MulBackward0>)\n",
            "Step 67 Loss:  tensor(29.2788, grad_fn=<MulBackward0>)\n",
            "Step 68 Loss:  tensor(28.9718, grad_fn=<MulBackward0>)\n",
            "Step 69 Loss:  tensor(28.6685, grad_fn=<MulBackward0>)\n",
            "Step 70 Loss:  tensor(28.3689, grad_fn=<MulBackward0>)\n",
            "Step 71 Loss:  tensor(28.0729, grad_fn=<MulBackward0>)\n",
            "Step 72 Loss:  tensor(27.7804, grad_fn=<MulBackward0>)\n",
            "Step 73 Loss:  tensor(27.4915, grad_fn=<MulBackward0>)\n",
            "Step 74 Loss:  tensor(27.2061, grad_fn=<MulBackward0>)\n",
            "Step 75 Loss:  tensor(26.9241, grad_fn=<MulBackward0>)\n",
            "Step 76 Loss:  tensor(26.6455, grad_fn=<MulBackward0>)\n",
            "Step 77 Loss:  tensor(26.3703, grad_fn=<MulBackward0>)\n",
            "Step 78 Loss:  tensor(26.0984, grad_fn=<MulBackward0>)\n",
            "Step 79 Loss:  tensor(25.8298, grad_fn=<MulBackward0>)\n",
            "Step 80 Loss:  tensor(25.5644, grad_fn=<MulBackward0>)\n",
            "Step 81 Loss:  tensor(25.3022, grad_fn=<MulBackward0>)\n",
            "Step 82 Loss:  tensor(25.0432, grad_fn=<MulBackward0>)\n",
            "Step 83 Loss:  tensor(24.7873, grad_fn=<MulBackward0>)\n",
            "Step 84 Loss:  tensor(24.5345, grad_fn=<MulBackward0>)\n",
            "Step 85 Loss:  tensor(24.2847, grad_fn=<MulBackward0>)\n",
            "Step 86 Loss:  tensor(24.0380, grad_fn=<MulBackward0>)\n",
            "Step 87 Loss:  tensor(23.7942, grad_fn=<MulBackward0>)\n",
            "Step 88 Loss:  tensor(23.5534, grad_fn=<MulBackward0>)\n",
            "Step 89 Loss:  tensor(23.3155, grad_fn=<MulBackward0>)\n",
            "Step 90 Loss:  tensor(23.0805, grad_fn=<MulBackward0>)\n",
            "Step 91 Loss:  tensor(22.8482, grad_fn=<MulBackward0>)\n",
            "Step 92 Loss:  tensor(22.6188, grad_fn=<MulBackward0>)\n",
            "Step 93 Loss:  tensor(22.3922, grad_fn=<MulBackward0>)\n",
            "Step 94 Loss:  tensor(22.1683, grad_fn=<MulBackward0>)\n",
            "Step 95 Loss:  tensor(21.9471, grad_fn=<MulBackward0>)\n",
            "Step 96 Loss:  tensor(21.7286, grad_fn=<MulBackward0>)\n",
            "Step 97 Loss:  tensor(21.5127, grad_fn=<MulBackward0>)\n",
            "Step 98 Loss:  tensor(21.2994, grad_fn=<MulBackward0>)\n",
            "Step 99 Loss:  tensor(21.0887, grad_fn=<MulBackward0>)\n",
            "Step 100 Loss:  tensor(20.8805, grad_fn=<MulBackward0>)\n",
            "Step 101 Loss:  tensor(20.6748, grad_fn=<MulBackward0>)\n",
            "Step 102 Loss:  tensor(20.4716, grad_fn=<MulBackward0>)\n",
            "Step 103 Loss:  tensor(20.2709, grad_fn=<MulBackward0>)\n",
            "Step 104 Loss:  tensor(20.0726, grad_fn=<MulBackward0>)\n",
            "Step 105 Loss:  tensor(19.8767, grad_fn=<MulBackward0>)\n",
            "Step 106 Loss:  tensor(19.6831, grad_fn=<MulBackward0>)\n",
            "Step 107 Loss:  tensor(19.4919, grad_fn=<MulBackward0>)\n",
            "Step 108 Loss:  tensor(19.3030, grad_fn=<MulBackward0>)\n",
            "Step 109 Loss:  tensor(19.1164, grad_fn=<MulBackward0>)\n",
            "Step 110 Loss:  tensor(18.9320, grad_fn=<MulBackward0>)\n",
            "Step 111 Loss:  tensor(18.7499, grad_fn=<MulBackward0>)\n",
            "Step 112 Loss:  tensor(18.5699, grad_fn=<MulBackward0>)\n",
            "Step 113 Loss:  tensor(18.3921, grad_fn=<MulBackward0>)\n",
            "Step 114 Loss:  tensor(18.2165, grad_fn=<MulBackward0>)\n",
            "Step 115 Loss:  tensor(18.0430, grad_fn=<MulBackward0>)\n",
            "Step 116 Loss:  tensor(17.8715, grad_fn=<MulBackward0>)\n",
            "Step 117 Loss:  tensor(17.7022, grad_fn=<MulBackward0>)\n",
            "Step 118 Loss:  tensor(17.5349, grad_fn=<MulBackward0>)\n",
            "Step 119 Loss:  tensor(17.3696, grad_fn=<MulBackward0>)\n",
            "Step 120 Loss:  tensor(17.2063, grad_fn=<MulBackward0>)\n",
            "Step 121 Loss:  tensor(17.0449, grad_fn=<MulBackward0>)\n",
            "Step 122 Loss:  tensor(16.8856, grad_fn=<MulBackward0>)\n",
            "Step 123 Loss:  tensor(16.7281, grad_fn=<MulBackward0>)\n",
            "Step 124 Loss:  tensor(16.5726, grad_fn=<MulBackward0>)\n",
            "Step 125 Loss:  tensor(16.4189, grad_fn=<MulBackward0>)\n",
            "Step 126 Loss:  tensor(16.2670, grad_fn=<MulBackward0>)\n",
            "Step 127 Loss:  tensor(16.1170, grad_fn=<MulBackward0>)\n",
            "Step 128 Loss:  tensor(15.9689, grad_fn=<MulBackward0>)\n",
            "Step 129 Loss:  tensor(15.8225, grad_fn=<MulBackward0>)\n",
            "Step 130 Loss:  tensor(15.6778, grad_fn=<MulBackward0>)\n",
            "Step 131 Loss:  tensor(15.5349, grad_fn=<MulBackward0>)\n",
            "Step 132 Loss:  tensor(15.3938, grad_fn=<MulBackward0>)\n",
            "Step 133 Loss:  tensor(15.2543, grad_fn=<MulBackward0>)\n",
            "Step 134 Loss:  tensor(15.1166, grad_fn=<MulBackward0>)\n",
            "Step 135 Loss:  tensor(14.9804, grad_fn=<MulBackward0>)\n",
            "Step 136 Loss:  tensor(14.8460, grad_fn=<MulBackward0>)\n",
            "Step 137 Loss:  tensor(14.7131, grad_fn=<MulBackward0>)\n",
            "Step 138 Loss:  tensor(14.5819, grad_fn=<MulBackward0>)\n",
            "Step 139 Loss:  tensor(14.4522, grad_fn=<MulBackward0>)\n",
            "Step 140 Loss:  tensor(14.3241, grad_fn=<MulBackward0>)\n",
            "Step 141 Loss:  tensor(14.1976, grad_fn=<MulBackward0>)\n",
            "Step 142 Loss:  tensor(14.0725, grad_fn=<MulBackward0>)\n",
            "Step 143 Loss:  tensor(13.9490, grad_fn=<MulBackward0>)\n",
            "Step 144 Loss:  tensor(13.8270, grad_fn=<MulBackward0>)\n",
            "Step 145 Loss:  tensor(13.7064, grad_fn=<MulBackward0>)\n",
            "Step 146 Loss:  tensor(13.5874, grad_fn=<MulBackward0>)\n",
            "Step 147 Loss:  tensor(13.4697, grad_fn=<MulBackward0>)\n",
            "Step 148 Loss:  tensor(13.3534, grad_fn=<MulBackward0>)\n",
            "Step 149 Loss:  tensor(13.2386, grad_fn=<MulBackward0>)\n",
            "Step 150 Loss:  tensor(13.1252, grad_fn=<MulBackward0>)\n",
            "Step 151 Loss:  tensor(13.0131, grad_fn=<MulBackward0>)\n",
            "Step 152 Loss:  tensor(12.9023, grad_fn=<MulBackward0>)\n",
            "Step 153 Loss:  tensor(12.7929, grad_fn=<MulBackward0>)\n",
            "Step 154 Loss:  tensor(12.6849, grad_fn=<MulBackward0>)\n",
            "Step 155 Loss:  tensor(12.5781, grad_fn=<MulBackward0>)\n",
            "Step 156 Loss:  tensor(12.4726, grad_fn=<MulBackward0>)\n",
            "Step 157 Loss:  tensor(12.3684, grad_fn=<MulBackward0>)\n",
            "Step 158 Loss:  tensor(12.2655, grad_fn=<MulBackward0>)\n",
            "Step 159 Loss:  tensor(12.1637, grad_fn=<MulBackward0>)\n",
            "Step 160 Loss:  tensor(12.0633, grad_fn=<MulBackward0>)\n",
            "Step 161 Loss:  tensor(11.9640, grad_fn=<MulBackward0>)\n",
            "Step 162 Loss:  tensor(11.8659, grad_fn=<MulBackward0>)\n",
            "Step 163 Loss:  tensor(11.7690, grad_fn=<MulBackward0>)\n",
            "Step 164 Loss:  tensor(11.6733, grad_fn=<MulBackward0>)\n",
            "Step 165 Loss:  tensor(11.5787, grad_fn=<MulBackward0>)\n",
            "Step 166 Loss:  tensor(11.4853, grad_fn=<MulBackward0>)\n",
            "Step 167 Loss:  tensor(11.3930, grad_fn=<MulBackward0>)\n",
            "Step 168 Loss:  tensor(11.3018, grad_fn=<MulBackward0>)\n",
            "Step 169 Loss:  tensor(11.2117, grad_fn=<MulBackward0>)\n",
            "Step 170 Loss:  tensor(11.1227, grad_fn=<MulBackward0>)\n",
            "Step 171 Loss:  tensor(11.0348, grad_fn=<MulBackward0>)\n",
            "Step 172 Loss:  tensor(10.9480, grad_fn=<MulBackward0>)\n",
            "Step 173 Loss:  tensor(10.8621, grad_fn=<MulBackward0>)\n",
            "Step 174 Loss:  tensor(10.7774, grad_fn=<MulBackward0>)\n",
            "Step 175 Loss:  tensor(10.6936, grad_fn=<MulBackward0>)\n",
            "Step 176 Loss:  tensor(10.6109, grad_fn=<MulBackward0>)\n",
            "Step 177 Loss:  tensor(10.5291, grad_fn=<MulBackward0>)\n",
            "Step 178 Loss:  tensor(10.4484, grad_fn=<MulBackward0>)\n",
            "Step 179 Loss:  tensor(10.3686, grad_fn=<MulBackward0>)\n",
            "Step 180 Loss:  tensor(10.2898, grad_fn=<MulBackward0>)\n",
            "Step 181 Loss:  tensor(10.2119, grad_fn=<MulBackward0>)\n",
            "Step 182 Loss:  tensor(10.1349, grad_fn=<MulBackward0>)\n",
            "Step 183 Loss:  tensor(10.0589, grad_fn=<MulBackward0>)\n",
            "Step 184 Loss:  tensor(9.9838, grad_fn=<MulBackward0>)\n",
            "Step 185 Loss:  tensor(9.9097, grad_fn=<MulBackward0>)\n",
            "Step 186 Loss:  tensor(9.8364, grad_fn=<MulBackward0>)\n",
            "Step 187 Loss:  tensor(9.7640, grad_fn=<MulBackward0>)\n",
            "Step 188 Loss:  tensor(9.6925, grad_fn=<MulBackward0>)\n",
            "Step 189 Loss:  tensor(9.6218, grad_fn=<MulBackward0>)\n",
            "Step 190 Loss:  tensor(9.5520, grad_fn=<MulBackward0>)\n",
            "Step 191 Loss:  tensor(9.4830, grad_fn=<MulBackward0>)\n",
            "Step 192 Loss:  tensor(9.4149, grad_fn=<MulBackward0>)\n",
            "Step 193 Loss:  tensor(9.3476, grad_fn=<MulBackward0>)\n",
            "Step 194 Loss:  tensor(9.2811, grad_fn=<MulBackward0>)\n",
            "Step 195 Loss:  tensor(9.2154, grad_fn=<MulBackward0>)\n",
            "Step 196 Loss:  tensor(9.1504, grad_fn=<MulBackward0>)\n",
            "Step 197 Loss:  tensor(9.0863, grad_fn=<MulBackward0>)\n",
            "Step 198 Loss:  tensor(9.0230, grad_fn=<MulBackward0>)\n",
            "Step 199 Loss:  tensor(8.9604, grad_fn=<MulBackward0>)\n",
            "Step 200 Loss:  tensor(8.8986, grad_fn=<MulBackward0>)\n",
            "Step 201 Loss:  tensor(8.8375, grad_fn=<MulBackward0>)\n",
            "Step 202 Loss:  tensor(8.7771, grad_fn=<MulBackward0>)\n",
            "Step 203 Loss:  tensor(8.7175, grad_fn=<MulBackward0>)\n",
            "Step 204 Loss:  tensor(8.6586, grad_fn=<MulBackward0>)\n",
            "Step 205 Loss:  tensor(8.6004, grad_fn=<MulBackward0>)\n",
            "Step 206 Loss:  tensor(8.5429, grad_fn=<MulBackward0>)\n",
            "Step 207 Loss:  tensor(8.4861, grad_fn=<MulBackward0>)\n",
            "Step 208 Loss:  tensor(8.4300, grad_fn=<MulBackward0>)\n",
            "Step 209 Loss:  tensor(8.3746, grad_fn=<MulBackward0>)\n",
            "Step 210 Loss:  tensor(8.3198, grad_fn=<MulBackward0>)\n",
            "Step 211 Loss:  tensor(8.2657, grad_fn=<MulBackward0>)\n",
            "Step 212 Loss:  tensor(8.2123, grad_fn=<MulBackward0>)\n",
            "Step 213 Loss:  tensor(8.1595, grad_fn=<MulBackward0>)\n",
            "Step 214 Loss:  tensor(8.1073, grad_fn=<MulBackward0>)\n",
            "Step 215 Loss:  tensor(8.0558, grad_fn=<MulBackward0>)\n",
            "Step 216 Loss:  tensor(8.0048, grad_fn=<MulBackward0>)\n",
            "Step 217 Loss:  tensor(7.9545, grad_fn=<MulBackward0>)\n",
            "Step 218 Loss:  tensor(7.9049, grad_fn=<MulBackward0>)\n",
            "Step 219 Loss:  tensor(7.8558, grad_fn=<MulBackward0>)\n",
            "Step 220 Loss:  tensor(7.8073, grad_fn=<MulBackward0>)\n",
            "Step 221 Loss:  tensor(7.7593, grad_fn=<MulBackward0>)\n",
            "Step 222 Loss:  tensor(7.7120, grad_fn=<MulBackward0>)\n",
            "Step 223 Loss:  tensor(7.6652, grad_fn=<MulBackward0>)\n",
            "Step 224 Loss:  tensor(7.6190, grad_fn=<MulBackward0>)\n",
            "Step 225 Loss:  tensor(7.5734, grad_fn=<MulBackward0>)\n",
            "Step 226 Loss:  tensor(7.5283, grad_fn=<MulBackward0>)\n",
            "Step 227 Loss:  tensor(7.4837, grad_fn=<MulBackward0>)\n",
            "Step 228 Loss:  tensor(7.4397, grad_fn=<MulBackward0>)\n",
            "Step 229 Loss:  tensor(7.3962, grad_fn=<MulBackward0>)\n",
            "Step 230 Loss:  tensor(7.3533, grad_fn=<MulBackward0>)\n",
            "Step 231 Loss:  tensor(7.3108, grad_fn=<MulBackward0>)\n",
            "Step 232 Loss:  tensor(7.2689, grad_fn=<MulBackward0>)\n",
            "Step 233 Loss:  tensor(7.2275, grad_fn=<MulBackward0>)\n",
            "Step 234 Loss:  tensor(7.1866, grad_fn=<MulBackward0>)\n",
            "Step 235 Loss:  tensor(7.1461, grad_fn=<MulBackward0>)\n",
            "Step 236 Loss:  tensor(7.1062, grad_fn=<MulBackward0>)\n",
            "Step 237 Loss:  tensor(7.0667, grad_fn=<MulBackward0>)\n",
            "Step 238 Loss:  tensor(7.0278, grad_fn=<MulBackward0>)\n",
            "Step 239 Loss:  tensor(6.9893, grad_fn=<MulBackward0>)\n",
            "Step 240 Loss:  tensor(6.9512, grad_fn=<MulBackward0>)\n",
            "Step 241 Loss:  tensor(6.9136, grad_fn=<MulBackward0>)\n",
            "Step 242 Loss:  tensor(6.8765, grad_fn=<MulBackward0>)\n",
            "Step 243 Loss:  tensor(6.8398, grad_fn=<MulBackward0>)\n",
            "Step 244 Loss:  tensor(6.8036, grad_fn=<MulBackward0>)\n",
            "Step 245 Loss:  tensor(6.7677, grad_fn=<MulBackward0>)\n",
            "Step 246 Loss:  tensor(6.7324, grad_fn=<MulBackward0>)\n",
            "Step 247 Loss:  tensor(6.6974, grad_fn=<MulBackward0>)\n",
            "Step 248 Loss:  tensor(6.6629, grad_fn=<MulBackward0>)\n",
            "Step 249 Loss:  tensor(6.6288, grad_fn=<MulBackward0>)\n",
            "Step 250 Loss:  tensor(6.5951, grad_fn=<MulBackward0>)\n",
            "Step 251 Loss:  tensor(6.5618, grad_fn=<MulBackward0>)\n",
            "Step 252 Loss:  tensor(6.5289, grad_fn=<MulBackward0>)\n",
            "Step 253 Loss:  tensor(6.4964, grad_fn=<MulBackward0>)\n",
            "Step 254 Loss:  tensor(6.4643, grad_fn=<MulBackward0>)\n",
            "Step 255 Loss:  tensor(6.4326, grad_fn=<MulBackward0>)\n",
            "Step 256 Loss:  tensor(6.4013, grad_fn=<MulBackward0>)\n",
            "Step 257 Loss:  tensor(6.3703, grad_fn=<MulBackward0>)\n",
            "Step 258 Loss:  tensor(6.3398, grad_fn=<MulBackward0>)\n",
            "Step 259 Loss:  tensor(6.3095, grad_fn=<MulBackward0>)\n",
            "Step 260 Loss:  tensor(6.2797, grad_fn=<MulBackward0>)\n",
            "Step 261 Loss:  tensor(6.2502, grad_fn=<MulBackward0>)\n",
            "Step 262 Loss:  tensor(6.2211, grad_fn=<MulBackward0>)\n",
            "Step 263 Loss:  tensor(6.1923, grad_fn=<MulBackward0>)\n",
            "Step 264 Loss:  tensor(6.1639, grad_fn=<MulBackward0>)\n",
            "Step 265 Loss:  tensor(6.1358, grad_fn=<MulBackward0>)\n",
            "Step 266 Loss:  tensor(6.1080, grad_fn=<MulBackward0>)\n",
            "Step 267 Loss:  tensor(6.0806, grad_fn=<MulBackward0>)\n",
            "Step 268 Loss:  tensor(6.0535, grad_fn=<MulBackward0>)\n",
            "Step 269 Loss:  tensor(6.0268, grad_fn=<MulBackward0>)\n",
            "Step 270 Loss:  tensor(6.0003, grad_fn=<MulBackward0>)\n",
            "Step 271 Loss:  tensor(5.9742, grad_fn=<MulBackward0>)\n",
            "Step 272 Loss:  tensor(5.9484, grad_fn=<MulBackward0>)\n",
            "Step 273 Loss:  tensor(5.9229, grad_fn=<MulBackward0>)\n",
            "Step 274 Loss:  tensor(5.8978, grad_fn=<MulBackward0>)\n",
            "Step 275 Loss:  tensor(5.8729, grad_fn=<MulBackward0>)\n",
            "Step 276 Loss:  tensor(5.8483, grad_fn=<MulBackward0>)\n",
            "Step 277 Loss:  tensor(5.8240, grad_fn=<MulBackward0>)\n",
            "Step 278 Loss:  tensor(5.8000, grad_fn=<MulBackward0>)\n",
            "Step 279 Loss:  tensor(5.7763, grad_fn=<MulBackward0>)\n",
            "Step 280 Loss:  tensor(5.7529, grad_fn=<MulBackward0>)\n",
            "Step 281 Loss:  tensor(5.7298, grad_fn=<MulBackward0>)\n",
            "Step 282 Loss:  tensor(5.7070, grad_fn=<MulBackward0>)\n",
            "Step 283 Loss:  tensor(5.6844, grad_fn=<MulBackward0>)\n",
            "Step 284 Loss:  tensor(5.6621, grad_fn=<MulBackward0>)\n",
            "Step 285 Loss:  tensor(5.6400, grad_fn=<MulBackward0>)\n",
            "Step 286 Loss:  tensor(5.6183, grad_fn=<MulBackward0>)\n",
            "Step 287 Loss:  tensor(5.5968, grad_fn=<MulBackward0>)\n",
            "Step 288 Loss:  tensor(5.5755, grad_fn=<MulBackward0>)\n",
            "Step 289 Loss:  tensor(5.5545, grad_fn=<MulBackward0>)\n",
            "Step 290 Loss:  tensor(5.5338, grad_fn=<MulBackward0>)\n",
            "Step 291 Loss:  tensor(5.5133, grad_fn=<MulBackward0>)\n",
            "Step 292 Loss:  tensor(5.4931, grad_fn=<MulBackward0>)\n",
            "Step 293 Loss:  tensor(5.4731, grad_fn=<MulBackward0>)\n",
            "Step 294 Loss:  tensor(5.4533, grad_fn=<MulBackward0>)\n",
            "Step 295 Loss:  tensor(5.4338, grad_fn=<MulBackward0>)\n",
            "Step 296 Loss:  tensor(5.4145, grad_fn=<MulBackward0>)\n",
            "Step 297 Loss:  tensor(5.3955, grad_fn=<MulBackward0>)\n",
            "Step 298 Loss:  tensor(5.3767, grad_fn=<MulBackward0>)\n",
            "Step 299 Loss:  tensor(5.3581, grad_fn=<MulBackward0>)\n",
            "Step 300 Loss:  tensor(5.3397, grad_fn=<MulBackward0>)\n",
            "Step 301 Loss:  tensor(5.3216, grad_fn=<MulBackward0>)\n",
            "Step 302 Loss:  tensor(5.3037, grad_fn=<MulBackward0>)\n",
            "Step 303 Loss:  tensor(5.2859, grad_fn=<MulBackward0>)\n",
            "Step 304 Loss:  tensor(5.2685, grad_fn=<MulBackward0>)\n",
            "Step 305 Loss:  tensor(5.2512, grad_fn=<MulBackward0>)\n",
            "Step 306 Loss:  tensor(5.2341, grad_fn=<MulBackward0>)\n",
            "Step 307 Loss:  tensor(5.2172, grad_fn=<MulBackward0>)\n",
            "Step 308 Loss:  tensor(5.2006, grad_fn=<MulBackward0>)\n",
            "Step 309 Loss:  tensor(5.1841, grad_fn=<MulBackward0>)\n",
            "Step 310 Loss:  tensor(5.1678, grad_fn=<MulBackward0>)\n",
            "Step 311 Loss:  tensor(5.1518, grad_fn=<MulBackward0>)\n",
            "Step 312 Loss:  tensor(5.1359, grad_fn=<MulBackward0>)\n",
            "Step 313 Loss:  tensor(5.1202, grad_fn=<MulBackward0>)\n",
            "Step 314 Loss:  tensor(5.1047, grad_fn=<MulBackward0>)\n",
            "Step 315 Loss:  tensor(5.0894, grad_fn=<MulBackward0>)\n",
            "Step 316 Loss:  tensor(5.0743, grad_fn=<MulBackward0>)\n",
            "Step 317 Loss:  tensor(5.0593, grad_fn=<MulBackward0>)\n",
            "Step 318 Loss:  tensor(5.0446, grad_fn=<MulBackward0>)\n",
            "Step 319 Loss:  tensor(5.0300, grad_fn=<MulBackward0>)\n",
            "Step 320 Loss:  tensor(5.0156, grad_fn=<MulBackward0>)\n",
            "Step 321 Loss:  tensor(5.0014, grad_fn=<MulBackward0>)\n",
            "Step 322 Loss:  tensor(4.9873, grad_fn=<MulBackward0>)\n",
            "Step 323 Loss:  tensor(4.9734, grad_fn=<MulBackward0>)\n",
            "Step 324 Loss:  tensor(4.9597, grad_fn=<MulBackward0>)\n",
            "Step 325 Loss:  tensor(4.9461, grad_fn=<MulBackward0>)\n",
            "Step 326 Loss:  tensor(4.9327, grad_fn=<MulBackward0>)\n",
            "Step 327 Loss:  tensor(4.9195, grad_fn=<MulBackward0>)\n",
            "Step 328 Loss:  tensor(4.9064, grad_fn=<MulBackward0>)\n",
            "Step 329 Loss:  tensor(4.8935, grad_fn=<MulBackward0>)\n",
            "Step 330 Loss:  tensor(4.8807, grad_fn=<MulBackward0>)\n",
            "Step 331 Loss:  tensor(4.8681, grad_fn=<MulBackward0>)\n",
            "Step 332 Loss:  tensor(4.8557, grad_fn=<MulBackward0>)\n",
            "Step 333 Loss:  tensor(4.8434, grad_fn=<MulBackward0>)\n",
            "Step 334 Loss:  tensor(4.8312, grad_fn=<MulBackward0>)\n",
            "Step 335 Loss:  tensor(4.8192, grad_fn=<MulBackward0>)\n",
            "Step 336 Loss:  tensor(4.8074, grad_fn=<MulBackward0>)\n",
            "Step 337 Loss:  tensor(4.7956, grad_fn=<MulBackward0>)\n",
            "Step 338 Loss:  tensor(4.7841, grad_fn=<MulBackward0>)\n",
            "Step 339 Loss:  tensor(4.7726, grad_fn=<MulBackward0>)\n",
            "Step 340 Loss:  tensor(4.7613, grad_fn=<MulBackward0>)\n",
            "Step 341 Loss:  tensor(4.7502, grad_fn=<MulBackward0>)\n",
            "Step 342 Loss:  tensor(4.7391, grad_fn=<MulBackward0>)\n",
            "Step 343 Loss:  tensor(4.7282, grad_fn=<MulBackward0>)\n",
            "Step 344 Loss:  tensor(4.7175, grad_fn=<MulBackward0>)\n",
            "Step 345 Loss:  tensor(4.7068, grad_fn=<MulBackward0>)\n",
            "Step 346 Loss:  tensor(4.6963, grad_fn=<MulBackward0>)\n",
            "Step 347 Loss:  tensor(4.6860, grad_fn=<MulBackward0>)\n",
            "Step 348 Loss:  tensor(4.6757, grad_fn=<MulBackward0>)\n",
            "Step 349 Loss:  tensor(4.6656, grad_fn=<MulBackward0>)\n",
            "Step 350 Loss:  tensor(4.6556, grad_fn=<MulBackward0>)\n",
            "Step 351 Loss:  tensor(4.6457, grad_fn=<MulBackward0>)\n",
            "Step 352 Loss:  tensor(4.6359, grad_fn=<MulBackward0>)\n",
            "Step 353 Loss:  tensor(4.6262, grad_fn=<MulBackward0>)\n",
            "Step 354 Loss:  tensor(4.6167, grad_fn=<MulBackward0>)\n",
            "Step 355 Loss:  tensor(4.6073, grad_fn=<MulBackward0>)\n",
            "Step 356 Loss:  tensor(4.5980, grad_fn=<MulBackward0>)\n",
            "Step 357 Loss:  tensor(4.5888, grad_fn=<MulBackward0>)\n",
            "Step 358 Loss:  tensor(4.5797, grad_fn=<MulBackward0>)\n",
            "Step 359 Loss:  tensor(4.5707, grad_fn=<MulBackward0>)\n",
            "Step 360 Loss:  tensor(4.5619, grad_fn=<MulBackward0>)\n",
            "Step 361 Loss:  tensor(4.5531, grad_fn=<MulBackward0>)\n",
            "Step 362 Loss:  tensor(4.5445, grad_fn=<MulBackward0>)\n",
            "Step 363 Loss:  tensor(4.5359, grad_fn=<MulBackward0>)\n",
            "Step 364 Loss:  tensor(4.5275, grad_fn=<MulBackward0>)\n",
            "Step 365 Loss:  tensor(4.5191, grad_fn=<MulBackward0>)\n",
            "Step 366 Loss:  tensor(4.5109, grad_fn=<MulBackward0>)\n",
            "Step 367 Loss:  tensor(4.5027, grad_fn=<MulBackward0>)\n",
            "Step 368 Loss:  tensor(4.4947, grad_fn=<MulBackward0>)\n",
            "Step 369 Loss:  tensor(4.4868, grad_fn=<MulBackward0>)\n",
            "Step 370 Loss:  tensor(4.4789, grad_fn=<MulBackward0>)\n",
            "Step 371 Loss:  tensor(4.4711, grad_fn=<MulBackward0>)\n",
            "Step 372 Loss:  tensor(4.4635, grad_fn=<MulBackward0>)\n",
            "Step 373 Loss:  tensor(4.4559, grad_fn=<MulBackward0>)\n",
            "Step 374 Loss:  tensor(4.4484, grad_fn=<MulBackward0>)\n",
            "Step 375 Loss:  tensor(4.4410, grad_fn=<MulBackward0>)\n",
            "Step 376 Loss:  tensor(4.4337, grad_fn=<MulBackward0>)\n",
            "Step 377 Loss:  tensor(4.4265, grad_fn=<MulBackward0>)\n",
            "Step 378 Loss:  tensor(4.4194, grad_fn=<MulBackward0>)\n",
            "Step 379 Loss:  tensor(4.4124, grad_fn=<MulBackward0>)\n",
            "Step 380 Loss:  tensor(4.4054, grad_fn=<MulBackward0>)\n",
            "Step 381 Loss:  tensor(4.3985, grad_fn=<MulBackward0>)\n",
            "Step 382 Loss:  tensor(4.3918, grad_fn=<MulBackward0>)\n",
            "Step 383 Loss:  tensor(4.3851, grad_fn=<MulBackward0>)\n",
            "Step 384 Loss:  tensor(4.3784, grad_fn=<MulBackward0>)\n",
            "Step 385 Loss:  tensor(4.3719, grad_fn=<MulBackward0>)\n",
            "Step 386 Loss:  tensor(4.3654, grad_fn=<MulBackward0>)\n",
            "Step 387 Loss:  tensor(4.3590, grad_fn=<MulBackward0>)\n",
            "Step 388 Loss:  tensor(4.3527, grad_fn=<MulBackward0>)\n",
            "Step 389 Loss:  tensor(4.3465, grad_fn=<MulBackward0>)\n",
            "Step 390 Loss:  tensor(4.3403, grad_fn=<MulBackward0>)\n",
            "Step 391 Loss:  tensor(4.3342, grad_fn=<MulBackward0>)\n",
            "Step 392 Loss:  tensor(4.3282, grad_fn=<MulBackward0>)\n",
            "Step 393 Loss:  tensor(4.3223, grad_fn=<MulBackward0>)\n",
            "Step 394 Loss:  tensor(4.3164, grad_fn=<MulBackward0>)\n",
            "Step 395 Loss:  tensor(4.3106, grad_fn=<MulBackward0>)\n",
            "Step 396 Loss:  tensor(4.3049, grad_fn=<MulBackward0>)\n",
            "Step 397 Loss:  tensor(4.2993, grad_fn=<MulBackward0>)\n",
            "Step 398 Loss:  tensor(4.2937, grad_fn=<MulBackward0>)\n",
            "Step 399 Loss:  tensor(4.2881, grad_fn=<MulBackward0>)\n",
            "Step 400 Loss:  tensor(4.2827, grad_fn=<MulBackward0>)\n",
            "Step 401 Loss:  tensor(4.2773, grad_fn=<MulBackward0>)\n",
            "Step 402 Loss:  tensor(4.2720, grad_fn=<MulBackward0>)\n",
            "Step 403 Loss:  tensor(4.2667, grad_fn=<MulBackward0>)\n",
            "Step 404 Loss:  tensor(4.2615, grad_fn=<MulBackward0>)\n",
            "Step 405 Loss:  tensor(4.2564, grad_fn=<MulBackward0>)\n",
            "Step 406 Loss:  tensor(4.2513, grad_fn=<MulBackward0>)\n",
            "Step 407 Loss:  tensor(4.2463, grad_fn=<MulBackward0>)\n",
            "Step 408 Loss:  tensor(4.2414, grad_fn=<MulBackward0>)\n",
            "Step 409 Loss:  tensor(4.2365, grad_fn=<MulBackward0>)\n",
            "Step 410 Loss:  tensor(4.2316, grad_fn=<MulBackward0>)\n",
            "Step 411 Loss:  tensor(4.2269, grad_fn=<MulBackward0>)\n",
            "Step 412 Loss:  tensor(4.2221, grad_fn=<MulBackward0>)\n",
            "Step 413 Loss:  tensor(4.2175, grad_fn=<MulBackward0>)\n",
            "Step 414 Loss:  tensor(4.2129, grad_fn=<MulBackward0>)\n",
            "Step 415 Loss:  tensor(4.2083, grad_fn=<MulBackward0>)\n",
            "Step 416 Loss:  tensor(4.2038, grad_fn=<MulBackward0>)\n",
            "Step 417 Loss:  tensor(4.1994, grad_fn=<MulBackward0>)\n",
            "Step 418 Loss:  tensor(4.1950, grad_fn=<MulBackward0>)\n",
            "Step 419 Loss:  tensor(4.1907, grad_fn=<MulBackward0>)\n",
            "Step 420 Loss:  tensor(4.1864, grad_fn=<MulBackward0>)\n",
            "Step 421 Loss:  tensor(4.1822, grad_fn=<MulBackward0>)\n",
            "Step 422 Loss:  tensor(4.1780, grad_fn=<MulBackward0>)\n",
            "Step 423 Loss:  tensor(4.1739, grad_fn=<MulBackward0>)\n",
            "Step 424 Loss:  tensor(4.1698, grad_fn=<MulBackward0>)\n",
            "Step 425 Loss:  tensor(4.1658, grad_fn=<MulBackward0>)\n",
            "Step 426 Loss:  tensor(4.1618, grad_fn=<MulBackward0>)\n",
            "Step 427 Loss:  tensor(4.1579, grad_fn=<MulBackward0>)\n",
            "Step 428 Loss:  tensor(4.1540, grad_fn=<MulBackward0>)\n",
            "Step 429 Loss:  tensor(4.1502, grad_fn=<MulBackward0>)\n",
            "Step 430 Loss:  tensor(4.1464, grad_fn=<MulBackward0>)\n",
            "Step 431 Loss:  tensor(4.1426, grad_fn=<MulBackward0>)\n",
            "Step 432 Loss:  tensor(4.1389, grad_fn=<MulBackward0>)\n",
            "Step 433 Loss:  tensor(4.1353, grad_fn=<MulBackward0>)\n",
            "Step 434 Loss:  tensor(4.1317, grad_fn=<MulBackward0>)\n",
            "Step 435 Loss:  tensor(4.1281, grad_fn=<MulBackward0>)\n",
            "Step 436 Loss:  tensor(4.1246, grad_fn=<MulBackward0>)\n",
            "Step 437 Loss:  tensor(4.1211, grad_fn=<MulBackward0>)\n",
            "Step 438 Loss:  tensor(4.1176, grad_fn=<MulBackward0>)\n",
            "Step 439 Loss:  tensor(4.1142, grad_fn=<MulBackward0>)\n",
            "Step 440 Loss:  tensor(4.1109, grad_fn=<MulBackward0>)\n",
            "Step 441 Loss:  tensor(4.1076, grad_fn=<MulBackward0>)\n",
            "Step 442 Loss:  tensor(4.1043, grad_fn=<MulBackward0>)\n",
            "Step 443 Loss:  tensor(4.1011, grad_fn=<MulBackward0>)\n",
            "Step 444 Loss:  tensor(4.0979, grad_fn=<MulBackward0>)\n",
            "Step 445 Loss:  tensor(4.0947, grad_fn=<MulBackward0>)\n",
            "Step 446 Loss:  tensor(4.0916, grad_fn=<MulBackward0>)\n",
            "Step 447 Loss:  tensor(4.0885, grad_fn=<MulBackward0>)\n",
            "Step 448 Loss:  tensor(4.0855, grad_fn=<MulBackward0>)\n",
            "Step 449 Loss:  tensor(4.0824, grad_fn=<MulBackward0>)\n",
            "Step 450 Loss:  tensor(4.0795, grad_fn=<MulBackward0>)\n",
            "Step 451 Loss:  tensor(4.0765, grad_fn=<MulBackward0>)\n",
            "Step 452 Loss:  tensor(4.0736, grad_fn=<MulBackward0>)\n",
            "Step 453 Loss:  tensor(4.0708, grad_fn=<MulBackward0>)\n",
            "Step 454 Loss:  tensor(4.0679, grad_fn=<MulBackward0>)\n",
            "Step 455 Loss:  tensor(4.0651, grad_fn=<MulBackward0>)\n",
            "Step 456 Loss:  tensor(4.0624, grad_fn=<MulBackward0>)\n",
            "Step 457 Loss:  tensor(4.0596, grad_fn=<MulBackward0>)\n",
            "Step 458 Loss:  tensor(4.0569, grad_fn=<MulBackward0>)\n",
            "Step 459 Loss:  tensor(4.0543, grad_fn=<MulBackward0>)\n",
            "Step 460 Loss:  tensor(4.0517, grad_fn=<MulBackward0>)\n",
            "Step 461 Loss:  tensor(4.0491, grad_fn=<MulBackward0>)\n",
            "Step 462 Loss:  tensor(4.0465, grad_fn=<MulBackward0>)\n",
            "Step 463 Loss:  tensor(4.0439, grad_fn=<MulBackward0>)\n",
            "Step 464 Loss:  tensor(4.0414, grad_fn=<MulBackward0>)\n",
            "Step 465 Loss:  tensor(4.0390, grad_fn=<MulBackward0>)\n",
            "Step 466 Loss:  tensor(4.0365, grad_fn=<MulBackward0>)\n",
            "Step 467 Loss:  tensor(4.0341, grad_fn=<MulBackward0>)\n",
            "Step 468 Loss:  tensor(4.0317, grad_fn=<MulBackward0>)\n",
            "Step 469 Loss:  tensor(4.0293, grad_fn=<MulBackward0>)\n",
            "Step 470 Loss:  tensor(4.0270, grad_fn=<MulBackward0>)\n",
            "Step 471 Loss:  tensor(4.0247, grad_fn=<MulBackward0>)\n",
            "Step 472 Loss:  tensor(4.0224, grad_fn=<MulBackward0>)\n",
            "Step 473 Loss:  tensor(4.0202, grad_fn=<MulBackward0>)\n",
            "Step 474 Loss:  tensor(4.0180, grad_fn=<MulBackward0>)\n",
            "Step 475 Loss:  tensor(4.0158, grad_fn=<MulBackward0>)\n",
            "Step 476 Loss:  tensor(4.0136, grad_fn=<MulBackward0>)\n",
            "Step 477 Loss:  tensor(4.0115, grad_fn=<MulBackward0>)\n",
            "Step 478 Loss:  tensor(4.0093, grad_fn=<MulBackward0>)\n",
            "Step 479 Loss:  tensor(4.0072, grad_fn=<MulBackward0>)\n",
            "Step 480 Loss:  tensor(4.0052, grad_fn=<MulBackward0>)\n",
            "Step 481 Loss:  tensor(4.0031, grad_fn=<MulBackward0>)\n",
            "Step 482 Loss:  tensor(4.0011, grad_fn=<MulBackward0>)\n",
            "Step 483 Loss:  tensor(3.9991, grad_fn=<MulBackward0>)\n",
            "Step 484 Loss:  tensor(3.9972, grad_fn=<MulBackward0>)\n",
            "Step 485 Loss:  tensor(3.9952, grad_fn=<MulBackward0>)\n",
            "Step 486 Loss:  tensor(3.9933, grad_fn=<MulBackward0>)\n",
            "Step 487 Loss:  tensor(3.9914, grad_fn=<MulBackward0>)\n",
            "Step 488 Loss:  tensor(3.9895, grad_fn=<MulBackward0>)\n",
            "Step 489 Loss:  tensor(3.9877, grad_fn=<MulBackward0>)\n",
            "Step 490 Loss:  tensor(3.9858, grad_fn=<MulBackward0>)\n",
            "Step 491 Loss:  tensor(3.9840, grad_fn=<MulBackward0>)\n",
            "Step 492 Loss:  tensor(3.9823, grad_fn=<MulBackward0>)\n",
            "Step 493 Loss:  tensor(3.9805, grad_fn=<MulBackward0>)\n",
            "Step 494 Loss:  tensor(3.9787, grad_fn=<MulBackward0>)\n",
            "Step 495 Loss:  tensor(3.9770, grad_fn=<MulBackward0>)\n",
            "Step 496 Loss:  tensor(3.9753, grad_fn=<MulBackward0>)\n",
            "Step 497 Loss:  tensor(3.9736, grad_fn=<MulBackward0>)\n",
            "Step 498 Loss:  tensor(3.9720, grad_fn=<MulBackward0>)\n",
            "Step 499 Loss:  tensor(3.9703, grad_fn=<MulBackward0>)\n",
            "Step 500 Loss:  tensor(3.9687, grad_fn=<MulBackward0>)\n",
            "Step 501 Loss:  tensor(3.9671, grad_fn=<MulBackward0>)\n",
            "Step 502 Loss:  tensor(3.9655, grad_fn=<MulBackward0>)\n",
            "Step 503 Loss:  tensor(3.9640, grad_fn=<MulBackward0>)\n",
            "Step 504 Loss:  tensor(3.9624, grad_fn=<MulBackward0>)\n",
            "Step 505 Loss:  tensor(3.9609, grad_fn=<MulBackward0>)\n",
            "Step 506 Loss:  tensor(3.9594, grad_fn=<MulBackward0>)\n",
            "Step 507 Loss:  tensor(3.9579, grad_fn=<MulBackward0>)\n",
            "Step 508 Loss:  tensor(3.9565, grad_fn=<MulBackward0>)\n",
            "Step 509 Loss:  tensor(3.9550, grad_fn=<MulBackward0>)\n",
            "Step 510 Loss:  tensor(3.9536, grad_fn=<MulBackward0>)\n",
            "Step 511 Loss:  tensor(3.9521, grad_fn=<MulBackward0>)\n",
            "Step 512 Loss:  tensor(3.9507, grad_fn=<MulBackward0>)\n",
            "Step 513 Loss:  tensor(3.9494, grad_fn=<MulBackward0>)\n",
            "Step 514 Loss:  tensor(3.9480, grad_fn=<MulBackward0>)\n",
            "Step 515 Loss:  tensor(3.9466, grad_fn=<MulBackward0>)\n",
            "Step 516 Loss:  tensor(3.9453, grad_fn=<MulBackward0>)\n",
            "Step 517 Loss:  tensor(3.9440, grad_fn=<MulBackward0>)\n",
            "Step 518 Loss:  tensor(3.9427, grad_fn=<MulBackward0>)\n",
            "Step 519 Loss:  tensor(3.9414, grad_fn=<MulBackward0>)\n",
            "Step 520 Loss:  tensor(3.9401, grad_fn=<MulBackward0>)\n",
            "Step 521 Loss:  tensor(3.9389, grad_fn=<MulBackward0>)\n",
            "Step 522 Loss:  tensor(3.9376, grad_fn=<MulBackward0>)\n",
            "Step 523 Loss:  tensor(3.9364, grad_fn=<MulBackward0>)\n",
            "Step 524 Loss:  tensor(3.9352, grad_fn=<MulBackward0>)\n",
            "Step 525 Loss:  tensor(3.9340, grad_fn=<MulBackward0>)\n",
            "Step 526 Loss:  tensor(3.9328, grad_fn=<MulBackward0>)\n",
            "Step 527 Loss:  tensor(3.9317, grad_fn=<MulBackward0>)\n",
            "Step 528 Loss:  tensor(3.9305, grad_fn=<MulBackward0>)\n",
            "Step 529 Loss:  tensor(3.9294, grad_fn=<MulBackward0>)\n",
            "Step 530 Loss:  tensor(3.9282, grad_fn=<MulBackward0>)\n",
            "Step 531 Loss:  tensor(3.9271, grad_fn=<MulBackward0>)\n",
            "Step 532 Loss:  tensor(3.9260, grad_fn=<MulBackward0>)\n",
            "Step 533 Loss:  tensor(3.9249, grad_fn=<MulBackward0>)\n",
            "Step 534 Loss:  tensor(3.9239, grad_fn=<MulBackward0>)\n",
            "Step 535 Loss:  tensor(3.9228, grad_fn=<MulBackward0>)\n",
            "Step 536 Loss:  tensor(3.9218, grad_fn=<MulBackward0>)\n",
            "Step 537 Loss:  tensor(3.9207, grad_fn=<MulBackward0>)\n",
            "Step 538 Loss:  tensor(3.9197, grad_fn=<MulBackward0>)\n",
            "Step 539 Loss:  tensor(3.9187, grad_fn=<MulBackward0>)\n",
            "Step 540 Loss:  tensor(3.9177, grad_fn=<MulBackward0>)\n",
            "Step 541 Loss:  tensor(3.9167, grad_fn=<MulBackward0>)\n",
            "Step 542 Loss:  tensor(3.9157, grad_fn=<MulBackward0>)\n",
            "Step 543 Loss:  tensor(3.9148, grad_fn=<MulBackward0>)\n",
            "Step 544 Loss:  tensor(3.9138, grad_fn=<MulBackward0>)\n",
            "Step 545 Loss:  tensor(3.9129, grad_fn=<MulBackward0>)\n",
            "Step 546 Loss:  tensor(3.9120, grad_fn=<MulBackward0>)\n",
            "Step 547 Loss:  tensor(3.9111, grad_fn=<MulBackward0>)\n",
            "Step 548 Loss:  tensor(3.9101, grad_fn=<MulBackward0>)\n",
            "Step 549 Loss:  tensor(3.9093, grad_fn=<MulBackward0>)\n",
            "Step 550 Loss:  tensor(3.9084, grad_fn=<MulBackward0>)\n",
            "Step 551 Loss:  tensor(3.9075, grad_fn=<MulBackward0>)\n",
            "Step 552 Loss:  tensor(3.9066, grad_fn=<MulBackward0>)\n",
            "Step 553 Loss:  tensor(3.9058, grad_fn=<MulBackward0>)\n",
            "Step 554 Loss:  tensor(3.9049, grad_fn=<MulBackward0>)\n",
            "Step 555 Loss:  tensor(3.9041, grad_fn=<MulBackward0>)\n",
            "Step 556 Loss:  tensor(3.9033, grad_fn=<MulBackward0>)\n",
            "Step 557 Loss:  tensor(3.9025, grad_fn=<MulBackward0>)\n",
            "Step 558 Loss:  tensor(3.9017, grad_fn=<MulBackward0>)\n",
            "Step 559 Loss:  tensor(3.9009, grad_fn=<MulBackward0>)\n",
            "Step 560 Loss:  tensor(3.9001, grad_fn=<MulBackward0>)\n",
            "Step 561 Loss:  tensor(3.8993, grad_fn=<MulBackward0>)\n",
            "Step 562 Loss:  tensor(3.8986, grad_fn=<MulBackward0>)\n",
            "Step 563 Loss:  tensor(3.8978, grad_fn=<MulBackward0>)\n",
            "Step 564 Loss:  tensor(3.8971, grad_fn=<MulBackward0>)\n",
            "Step 565 Loss:  tensor(3.8963, grad_fn=<MulBackward0>)\n",
            "Step 566 Loss:  tensor(3.8956, grad_fn=<MulBackward0>)\n",
            "Step 567 Loss:  tensor(3.8949, grad_fn=<MulBackward0>)\n",
            "Step 568 Loss:  tensor(3.8942, grad_fn=<MulBackward0>)\n",
            "Step 569 Loss:  tensor(3.8935, grad_fn=<MulBackward0>)\n",
            "Step 570 Loss:  tensor(3.8928, grad_fn=<MulBackward0>)\n",
            "Step 571 Loss:  tensor(3.8921, grad_fn=<MulBackward0>)\n",
            "Step 572 Loss:  tensor(3.8914, grad_fn=<MulBackward0>)\n",
            "Step 573 Loss:  tensor(3.8908, grad_fn=<MulBackward0>)\n",
            "Step 574 Loss:  tensor(3.8901, grad_fn=<MulBackward0>)\n",
            "Step 575 Loss:  tensor(3.8894, grad_fn=<MulBackward0>)\n",
            "Step 576 Loss:  tensor(3.8888, grad_fn=<MulBackward0>)\n",
            "Step 577 Loss:  tensor(3.8882, grad_fn=<MulBackward0>)\n",
            "Step 578 Loss:  tensor(3.8875, grad_fn=<MulBackward0>)\n",
            "Step 579 Loss:  tensor(3.8869, grad_fn=<MulBackward0>)\n",
            "Step 580 Loss:  tensor(3.8863, grad_fn=<MulBackward0>)\n",
            "Step 581 Loss:  tensor(3.8857, grad_fn=<MulBackward0>)\n",
            "Step 582 Loss:  tensor(3.8851, grad_fn=<MulBackward0>)\n",
            "Step 583 Loss:  tensor(3.8845, grad_fn=<MulBackward0>)\n",
            "Step 584 Loss:  tensor(3.8839, grad_fn=<MulBackward0>)\n",
            "Step 585 Loss:  tensor(3.8833, grad_fn=<MulBackward0>)\n",
            "Step 586 Loss:  tensor(3.8828, grad_fn=<MulBackward0>)\n",
            "Step 587 Loss:  tensor(3.8822, grad_fn=<MulBackward0>)\n",
            "Step 588 Loss:  tensor(3.8817, grad_fn=<MulBackward0>)\n",
            "Step 589 Loss:  tensor(3.8811, grad_fn=<MulBackward0>)\n",
            "Step 590 Loss:  tensor(3.8806, grad_fn=<MulBackward0>)\n",
            "Step 591 Loss:  tensor(3.8800, grad_fn=<MulBackward0>)\n",
            "Step 592 Loss:  tensor(3.8795, grad_fn=<MulBackward0>)\n",
            "Step 593 Loss:  tensor(3.8790, grad_fn=<MulBackward0>)\n",
            "Step 594 Loss:  tensor(3.8785, grad_fn=<MulBackward0>)\n",
            "Step 595 Loss:  tensor(3.8779, grad_fn=<MulBackward0>)\n",
            "Step 596 Loss:  tensor(3.8774, grad_fn=<MulBackward0>)\n",
            "Step 597 Loss:  tensor(3.8769, grad_fn=<MulBackward0>)\n",
            "Step 598 Loss:  tensor(3.8764, grad_fn=<MulBackward0>)\n",
            "Step 599 Loss:  tensor(3.8760, grad_fn=<MulBackward0>)\n",
            "Step 600 Loss:  tensor(3.8755, grad_fn=<MulBackward0>)\n",
            "Step 601 Loss:  tensor(3.8750, grad_fn=<MulBackward0>)\n",
            "Step 602 Loss:  tensor(3.8745, grad_fn=<MulBackward0>)\n",
            "Step 603 Loss:  tensor(3.8741, grad_fn=<MulBackward0>)\n",
            "Step 604 Loss:  tensor(3.8736, grad_fn=<MulBackward0>)\n",
            "Step 605 Loss:  tensor(3.8732, grad_fn=<MulBackward0>)\n",
            "Step 606 Loss:  tensor(3.8727, grad_fn=<MulBackward0>)\n",
            "Step 607 Loss:  tensor(3.8723, grad_fn=<MulBackward0>)\n",
            "Step 608 Loss:  tensor(3.8718, grad_fn=<MulBackward0>)\n",
            "Step 609 Loss:  tensor(3.8714, grad_fn=<MulBackward0>)\n",
            "Step 610 Loss:  tensor(3.8710, grad_fn=<MulBackward0>)\n",
            "Step 611 Loss:  tensor(3.8706, grad_fn=<MulBackward0>)\n",
            "Step 612 Loss:  tensor(3.8701, grad_fn=<MulBackward0>)\n",
            "Step 613 Loss:  tensor(3.8697, grad_fn=<MulBackward0>)\n",
            "Step 614 Loss:  tensor(3.8693, grad_fn=<MulBackward0>)\n",
            "Step 615 Loss:  tensor(3.8689, grad_fn=<MulBackward0>)\n",
            "Step 616 Loss:  tensor(3.8685, grad_fn=<MulBackward0>)\n",
            "Step 617 Loss:  tensor(3.8681, grad_fn=<MulBackward0>)\n",
            "Step 618 Loss:  tensor(3.8677, grad_fn=<MulBackward0>)\n",
            "Step 619 Loss:  tensor(3.8674, grad_fn=<MulBackward0>)\n",
            "Step 620 Loss:  tensor(3.8670, grad_fn=<MulBackward0>)\n",
            "Step 621 Loss:  tensor(3.8666, grad_fn=<MulBackward0>)\n",
            "Step 622 Loss:  tensor(3.8662, grad_fn=<MulBackward0>)\n",
            "Step 623 Loss:  tensor(3.8659, grad_fn=<MulBackward0>)\n",
            "Step 624 Loss:  tensor(3.8655, grad_fn=<MulBackward0>)\n",
            "Step 625 Loss:  tensor(3.8652, grad_fn=<MulBackward0>)\n",
            "Step 626 Loss:  tensor(3.8648, grad_fn=<MulBackward0>)\n",
            "Step 627 Loss:  tensor(3.8645, grad_fn=<MulBackward0>)\n",
            "Step 628 Loss:  tensor(3.8641, grad_fn=<MulBackward0>)\n",
            "Step 629 Loss:  tensor(3.8638, grad_fn=<MulBackward0>)\n",
            "Step 630 Loss:  tensor(3.8635, grad_fn=<MulBackward0>)\n",
            "Step 631 Loss:  tensor(3.8631, grad_fn=<MulBackward0>)\n",
            "Step 632 Loss:  tensor(3.8628, grad_fn=<MulBackward0>)\n",
            "Step 633 Loss:  tensor(3.8625, grad_fn=<MulBackward0>)\n",
            "Step 634 Loss:  tensor(3.8622, grad_fn=<MulBackward0>)\n",
            "Step 635 Loss:  tensor(3.8618, grad_fn=<MulBackward0>)\n",
            "Step 636 Loss:  tensor(3.8615, grad_fn=<MulBackward0>)\n",
            "Step 637 Loss:  tensor(3.8612, grad_fn=<MulBackward0>)\n",
            "Step 638 Loss:  tensor(3.8609, grad_fn=<MulBackward0>)\n",
            "Step 639 Loss:  tensor(3.8606, grad_fn=<MulBackward0>)\n",
            "Step 640 Loss:  tensor(3.8603, grad_fn=<MulBackward0>)\n",
            "Step 641 Loss:  tensor(3.8600, grad_fn=<MulBackward0>)\n",
            "Step 642 Loss:  tensor(3.8597, grad_fn=<MulBackward0>)\n",
            "Step 643 Loss:  tensor(3.8595, grad_fn=<MulBackward0>)\n",
            "Step 644 Loss:  tensor(3.8592, grad_fn=<MulBackward0>)\n",
            "Step 645 Loss:  tensor(3.8589, grad_fn=<MulBackward0>)\n",
            "Step 646 Loss:  tensor(3.8586, grad_fn=<MulBackward0>)\n",
            "Step 647 Loss:  tensor(3.8583, grad_fn=<MulBackward0>)\n",
            "Step 648 Loss:  tensor(3.8581, grad_fn=<MulBackward0>)\n",
            "Step 649 Loss:  tensor(3.8578, grad_fn=<MulBackward0>)\n",
            "Step 650 Loss:  tensor(3.8576, grad_fn=<MulBackward0>)\n",
            "Step 651 Loss:  tensor(3.8573, grad_fn=<MulBackward0>)\n",
            "Step 652 Loss:  tensor(3.8570, grad_fn=<MulBackward0>)\n",
            "Step 653 Loss:  tensor(3.8568, grad_fn=<MulBackward0>)\n",
            "Step 654 Loss:  tensor(3.8565, grad_fn=<MulBackward0>)\n",
            "Step 655 Loss:  tensor(3.8563, grad_fn=<MulBackward0>)\n",
            "Step 656 Loss:  tensor(3.8560, grad_fn=<MulBackward0>)\n",
            "Step 657 Loss:  tensor(3.8558, grad_fn=<MulBackward0>)\n",
            "Step 658 Loss:  tensor(3.8556, grad_fn=<MulBackward0>)\n",
            "Step 659 Loss:  tensor(3.8553, grad_fn=<MulBackward0>)\n",
            "Step 660 Loss:  tensor(3.8551, grad_fn=<MulBackward0>)\n",
            "Step 661 Loss:  tensor(3.8549, grad_fn=<MulBackward0>)\n",
            "Step 662 Loss:  tensor(3.8546, grad_fn=<MulBackward0>)\n",
            "Step 663 Loss:  tensor(3.8544, grad_fn=<MulBackward0>)\n",
            "Step 664 Loss:  tensor(3.8542, grad_fn=<MulBackward0>)\n",
            "Step 665 Loss:  tensor(3.8540, grad_fn=<MulBackward0>)\n",
            "Step 666 Loss:  tensor(3.8538, grad_fn=<MulBackward0>)\n",
            "Step 667 Loss:  tensor(3.8535, grad_fn=<MulBackward0>)\n",
            "Step 668 Loss:  tensor(3.8533, grad_fn=<MulBackward0>)\n",
            "Step 669 Loss:  tensor(3.8531, grad_fn=<MulBackward0>)\n",
            "Step 670 Loss:  tensor(3.8529, grad_fn=<MulBackward0>)\n",
            "Step 671 Loss:  tensor(3.8527, grad_fn=<MulBackward0>)\n",
            "Step 672 Loss:  tensor(3.8525, grad_fn=<MulBackward0>)\n",
            "Step 673 Loss:  tensor(3.8523, grad_fn=<MulBackward0>)\n",
            "Step 674 Loss:  tensor(3.8521, grad_fn=<MulBackward0>)\n",
            "Step 675 Loss:  tensor(3.8519, grad_fn=<MulBackward0>)\n",
            "Step 676 Loss:  tensor(3.8517, grad_fn=<MulBackward0>)\n",
            "Step 677 Loss:  tensor(3.8515, grad_fn=<MulBackward0>)\n",
            "Step 678 Loss:  tensor(3.8514, grad_fn=<MulBackward0>)\n",
            "Step 679 Loss:  tensor(3.8512, grad_fn=<MulBackward0>)\n",
            "Step 680 Loss:  tensor(3.8510, grad_fn=<MulBackward0>)\n",
            "Step 681 Loss:  tensor(3.8508, grad_fn=<MulBackward0>)\n",
            "Step 682 Loss:  tensor(3.8506, grad_fn=<MulBackward0>)\n",
            "Step 683 Loss:  tensor(3.8505, grad_fn=<MulBackward0>)\n",
            "Step 684 Loss:  tensor(3.8503, grad_fn=<MulBackward0>)\n",
            "Step 685 Loss:  tensor(3.8501, grad_fn=<MulBackward0>)\n",
            "Step 686 Loss:  tensor(3.8499, grad_fn=<MulBackward0>)\n",
            "Step 687 Loss:  tensor(3.8498, grad_fn=<MulBackward0>)\n",
            "Step 688 Loss:  tensor(3.8496, grad_fn=<MulBackward0>)\n",
            "Step 689 Loss:  tensor(3.8495, grad_fn=<MulBackward0>)\n",
            "Step 690 Loss:  tensor(3.8493, grad_fn=<MulBackward0>)\n",
            "Step 691 Loss:  tensor(3.8491, grad_fn=<MulBackward0>)\n",
            "Step 692 Loss:  tensor(3.8490, grad_fn=<MulBackward0>)\n",
            "Step 693 Loss:  tensor(3.8488, grad_fn=<MulBackward0>)\n",
            "Step 694 Loss:  tensor(3.8487, grad_fn=<MulBackward0>)\n",
            "Step 695 Loss:  tensor(3.8485, grad_fn=<MulBackward0>)\n",
            "Step 696 Loss:  tensor(3.8484, grad_fn=<MulBackward0>)\n",
            "Step 697 Loss:  tensor(3.8482, grad_fn=<MulBackward0>)\n",
            "Step 698 Loss:  tensor(3.8481, grad_fn=<MulBackward0>)\n",
            "Step 699 Loss:  tensor(3.8479, grad_fn=<MulBackward0>)\n",
            "Step 700 Loss:  tensor(3.8478, grad_fn=<MulBackward0>)\n",
            "Step 701 Loss:  tensor(3.8476, grad_fn=<MulBackward0>)\n",
            "Step 702 Loss:  tensor(3.8475, grad_fn=<MulBackward0>)\n",
            "Step 703 Loss:  tensor(3.8474, grad_fn=<MulBackward0>)\n",
            "Step 704 Loss:  tensor(3.8472, grad_fn=<MulBackward0>)\n",
            "Estimated w tensor([3.1723, 3.8985], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDwjlO6Cn7zI"
      },
      "source": [
        "## Classification using Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-f97IDOoBKO"
      },
      "source": [
        "The logistic regression model is another linear model that is utilized for the task of classification. It is written as \n",
        "\n",
        "$$\n",
        "Y = \\sigma (w^T X)\n",
        "$$\n",
        "\n",
        "Where $\\sigma(x)$ is the logistic/sigmoid function\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "For a vector $X$, the sigmoid is applied elementwise.\n",
        "\n",
        "Important properties of the sigmoid function is that all values lie in $[0, 1]$ and it is monotonically increasing. The range of the sigmoid function makes it useful to describe probabilities. If we assume a binary classification problem of predicting the probability of a variable $y=1$, we can express our model as\n",
        "\n",
        "$$\n",
        "P(y=1 | w, x) = \\sigma(w^T.x)\n",
        "$$\n",
        "\n",
        "The other possibility is $y=0$. By using the sum rule of probability we get\n",
        "\n",
        "$$\n",
        "P(y=0 | w, x) = 1 - \\sigma(w^T, x)\n",
        "$$\n",
        "\n",
        "An ideal decision rule for classification is to predict the class that has the highest probability for a given $w$ and $x$.\n",
        "\n",
        "The loss function for logistic regression is called Cross-Entropy loss and it is given by\n",
        "\n",
        "$$\n",
        "L(Y, \\hat{Y}) = - \\frac{1}{N}\\sum_{i=1}^{N} \\left[ y^{(i)} log \\hat{y}^{(i)} + (1 - y^{(i)}) log (1 - \\hat{y}^{(i)}) \\right]\n",
        "$$\n",
        "\n",
        "The above expression is also called negative-log-likelihood loss (NLL). There is no closed form expression for the optimal solution for the Logistic Regression. Therefore, we optimize it using Gradient Descent or some other iterative optimization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiKrMgSquC2w"
      },
      "source": [
        "#### Deriving the Gradient Descent update rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Z196xiufpT"
      },
      "source": [
        "We again calculate $\\nabla_W L(Y, \\hat{Y}) $. \n",
        "\n",
        "We get our gradient expression to be\n",
        "\n",
        "$$\n",
        " \\nabla_w L(Y, \\hat{Y}) = \\frac{1}{N}(\\hat{Y} - Y) X\n",
        "$$\n",
        "\n",
        "Interestingly, this is identical to the gradient update rule for our linear regression. The difference lies in the fact that for linear regression $\\hat{Y} = w^T X$ while for logistic regression, we have $\\hat{Y} = \\sigma(w^T X)$. \n",
        "\n",
        "\n",
        "This yields the update rule to be\n",
        "\n",
        "$$\n",
        "w = w - \\eta . \\frac{1}{N}(\\hat{Y} - Y) X\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLxN5OjGw2kp"
      },
      "source": [
        "#### Defining a toy classification problem\n",
        "\n",
        "Let us solve a toy classification problem of distinguishing between points drawn from two Gaussian distributions centered at $(0, 0)$ and $(5, 5)$ respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAkRYDiTw2Dv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "8434b444-42bb-4d76-b545-a34ee0fda038"
      },
      "source": [
        "X_1 = torch.normal(0,1, size=(100, 2))\n",
        "Y_1 = torch.zeros((100, ))\n",
        "X_2 = torch.normal(5,1, size=(100, 2))\n",
        "Y_2 = torch.ones((100,))\n",
        "plt.scatter(X_1[:,0], X_1[:, 1])\n",
        "plt.scatter(X_2[:,0], X_2[:, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r5Pqa8jn_T2"
      },
      "source": [
        "X_orig = torch.cat((X_1, X_2))\n",
        "Y_orig = torch.cat((Y_1, Y_2))\n",
        "\n",
        "indices = list(range(X.shape[0]))\n",
        "\n",
        "import random\n",
        "random.shuffle(indices)\n",
        "\n",
        "X = X_orig[indices]\n",
        "Y = Y_orig[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUFl05hWywI6"
      },
      "source": [
        "EPSILON = 1e-8\n",
        "def cross_entropy_loss(Y, Y_hat):\n",
        "    return - 1 / ( Y.shape[0]) * torch.sum(Y * torch.log(Y_hat + EPSILON) + (1 - Y) * torch.log(1 - Y_hat + EPSILON))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNz7Lkb54T-G"
      },
      "source": [
        "### Application of gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3GrG0KmzPzD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21c94f70-3a97-4d5f-b0e4-7c65d056bf86"
      },
      "source": [
        "# w is the parameter to optimize\n",
        "w = torch.normal(mean=0, std=10, size=(X.shape[1],))\n",
        "\n",
        "LEARNING_RATE = 1e-1\n",
        "\n",
        "N_ITERATIONS = 10000\n",
        "\n",
        "prev_loss = torch.FloatTensor([float('inf')])\n",
        "\n",
        "for i in range(N_ITERATIONS):\n",
        "\n",
        "    Y_hat = torch.sigmoid(X @ w)\n",
        "\n",
        "    loss = cross_entropy_loss(Y, Y_hat)\n",
        "    print(f\"Step {i + 1} Loss: \", loss)\n",
        "    if torch.isclose(prev_loss, loss, atol=1e-5):\n",
        "        break\n",
        "    \n",
        "    w = w - LEARNING_RATE * ((Y_hat - Y) @ X) / Y.shape[0]\n",
        "    prev_loss = loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Loss:  tensor(6.4149)\n",
            "Step 2 Loss:  tensor(6.4064)\n",
            "Step 3 Loss:  tensor(6.3952)\n",
            "Step 4 Loss:  tensor(6.3866)\n",
            "Step 5 Loss:  tensor(6.3739)\n",
            "Step 6 Loss:  tensor(6.3655)\n",
            "Step 7 Loss:  tensor(6.3547)\n",
            "Step 8 Loss:  tensor(6.3209)\n",
            "Step 9 Loss:  tensor(6.3122)\n",
            "Step 10 Loss:  tensor(6.3036)\n",
            "Step 11 Loss:  tensor(6.2933)\n",
            "Step 12 Loss:  tensor(6.2820)\n",
            "Step 13 Loss:  tensor(6.2733)\n",
            "Step 14 Loss:  tensor(6.2633)\n",
            "Step 15 Loss:  tensor(6.2547)\n",
            "Step 16 Loss:  tensor(6.2462)\n",
            "Step 17 Loss:  tensor(6.2211)\n",
            "Step 18 Loss:  tensor(6.2126)\n",
            "Step 19 Loss:  tensor(6.2030)\n",
            "Step 20 Loss:  tensor(6.1944)\n",
            "Step 21 Loss:  tensor(6.1574)\n",
            "Step 22 Loss:  tensor(6.1489)\n",
            "Step 23 Loss:  tensor(6.1395)\n",
            "Step 24 Loss:  tensor(6.1285)\n",
            "Step 25 Loss:  tensor(6.1201)\n",
            "Step 26 Loss:  tensor(6.1069)\n",
            "Step 27 Loss:  tensor(6.0923)\n",
            "Step 28 Loss:  tensor(6.0838)\n",
            "Step 29 Loss:  tensor(6.0479)\n",
            "Step 30 Loss:  tensor(6.0066)\n",
            "Step 31 Loss:  tensor(5.9976)\n",
            "Step 32 Loss:  tensor(5.9846)\n",
            "Step 33 Loss:  tensor(5.9729)\n",
            "Step 34 Loss:  tensor(5.9629)\n",
            "Step 35 Loss:  tensor(5.9531)\n",
            "Step 36 Loss:  tensor(5.9415)\n",
            "Step 37 Loss:  tensor(5.9318)\n",
            "Step 38 Loss:  tensor(5.9142)\n",
            "Step 39 Loss:  tensor(5.9024)\n",
            "Step 40 Loss:  tensor(5.8890)\n",
            "Step 41 Loss:  tensor(5.8775)\n",
            "Step 42 Loss:  tensor(5.8671)\n",
            "Step 43 Loss:  tensor(5.8575)\n",
            "Step 44 Loss:  tensor(5.8479)\n",
            "Step 45 Loss:  tensor(5.8355)\n",
            "Step 46 Loss:  tensor(5.7872)\n",
            "Step 47 Loss:  tensor(5.7497)\n",
            "Step 48 Loss:  tensor(5.7336)\n",
            "Step 49 Loss:  tensor(5.7232)\n",
            "Step 50 Loss:  tensor(5.7112)\n",
            "Step 51 Loss:  tensor(5.6996)\n",
            "Step 52 Loss:  tensor(5.6857)\n",
            "Step 53 Loss:  tensor(5.6751)\n",
            "Step 54 Loss:  tensor(5.6610)\n",
            "Step 55 Loss:  tensor(5.6480)\n",
            "Step 56 Loss:  tensor(5.6359)\n",
            "Step 57 Loss:  tensor(5.6210)\n",
            "Step 58 Loss:  tensor(5.6086)\n",
            "Step 59 Loss:  tensor(5.5940)\n",
            "Step 60 Loss:  tensor(5.5834)\n",
            "Step 61 Loss:  tensor(5.5715)\n",
            "Step 62 Loss:  tensor(5.5339)\n",
            "Step 63 Loss:  tensor(5.5170)\n",
            "Step 64 Loss:  tensor(5.4920)\n",
            "Step 65 Loss:  tensor(5.4808)\n",
            "Step 66 Loss:  tensor(5.4415)\n",
            "Step 67 Loss:  tensor(5.4278)\n",
            "Step 68 Loss:  tensor(5.4165)\n",
            "Step 69 Loss:  tensor(5.4028)\n",
            "Step 70 Loss:  tensor(5.3902)\n",
            "Step 71 Loss:  tensor(5.3729)\n",
            "Step 72 Loss:  tensor(5.3613)\n",
            "Step 73 Loss:  tensor(5.3440)\n",
            "Step 74 Loss:  tensor(5.3309)\n",
            "Step 75 Loss:  tensor(5.3194)\n",
            "Step 76 Loss:  tensor(5.2959)\n",
            "Step 77 Loss:  tensor(5.2834)\n",
            "Step 78 Loss:  tensor(5.2678)\n",
            "Step 79 Loss:  tensor(5.2551)\n",
            "Step 80 Loss:  tensor(5.2332)\n",
            "Step 81 Loss:  tensor(5.2211)\n",
            "Step 82 Loss:  tensor(5.2078)\n",
            "Step 83 Loss:  tensor(5.1872)\n",
            "Step 84 Loss:  tensor(5.1744)\n",
            "Step 85 Loss:  tensor(5.1600)\n",
            "Step 86 Loss:  tensor(5.1196)\n",
            "Step 87 Loss:  tensor(5.1004)\n",
            "Step 88 Loss:  tensor(5.0859)\n",
            "Step 89 Loss:  tensor(5.0714)\n",
            "Step 90 Loss:  tensor(5.0561)\n",
            "Step 91 Loss:  tensor(5.0410)\n",
            "Step 92 Loss:  tensor(4.9993)\n",
            "Step 93 Loss:  tensor(4.9830)\n",
            "Step 94 Loss:  tensor(4.9441)\n",
            "Step 95 Loss:  tensor(4.9257)\n",
            "Step 96 Loss:  tensor(4.9121)\n",
            "Step 97 Loss:  tensor(4.8965)\n",
            "Step 98 Loss:  tensor(4.8804)\n",
            "Step 99 Loss:  tensor(4.8660)\n",
            "Step 100 Loss:  tensor(4.8493)\n",
            "Step 101 Loss:  tensor(4.8280)\n",
            "Step 102 Loss:  tensor(4.8127)\n",
            "Step 103 Loss:  tensor(4.7976)\n",
            "Step 104 Loss:  tensor(4.7821)\n",
            "Step 105 Loss:  tensor(4.7659)\n",
            "Step 106 Loss:  tensor(4.7509)\n",
            "Step 107 Loss:  tensor(4.7224)\n",
            "Step 108 Loss:  tensor(4.7033)\n",
            "Step 109 Loss:  tensor(4.6873)\n",
            "Step 110 Loss:  tensor(4.6467)\n",
            "Step 111 Loss:  tensor(4.6057)\n",
            "Step 112 Loss:  tensor(4.5648)\n",
            "Step 113 Loss:  tensor(4.5388)\n",
            "Step 114 Loss:  tensor(4.5230)\n",
            "Step 115 Loss:  tensor(4.5083)\n",
            "Step 116 Loss:  tensor(4.4618)\n",
            "Step 117 Loss:  tensor(4.4442)\n",
            "Step 118 Loss:  tensor(4.4287)\n",
            "Step 119 Loss:  tensor(4.4095)\n",
            "Step 120 Loss:  tensor(4.3942)\n",
            "Step 121 Loss:  tensor(4.3752)\n",
            "Step 122 Loss:  tensor(4.3323)\n",
            "Step 123 Loss:  tensor(4.3009)\n",
            "Step 124 Loss:  tensor(4.2777)\n",
            "Step 125 Loss:  tensor(4.2582)\n",
            "Step 126 Loss:  tensor(4.2422)\n",
            "Step 127 Loss:  tensor(4.2169)\n",
            "Step 128 Loss:  tensor(4.1966)\n",
            "Step 129 Loss:  tensor(4.1744)\n",
            "Step 130 Loss:  tensor(4.1526)\n",
            "Step 131 Loss:  tensor(4.1349)\n",
            "Step 132 Loss:  tensor(4.1155)\n",
            "Step 133 Loss:  tensor(4.0850)\n",
            "Step 134 Loss:  tensor(4.0643)\n",
            "Step 135 Loss:  tensor(4.0447)\n",
            "Step 136 Loss:  tensor(4.0245)\n",
            "Step 137 Loss:  tensor(4.0004)\n",
            "Step 138 Loss:  tensor(3.9797)\n",
            "Step 139 Loss:  tensor(3.9540)\n",
            "Step 140 Loss:  tensor(3.9344)\n",
            "Step 141 Loss:  tensor(3.9118)\n",
            "Step 142 Loss:  tensor(3.8931)\n",
            "Step 143 Loss:  tensor(3.8682)\n",
            "Step 144 Loss:  tensor(3.8484)\n",
            "Step 145 Loss:  tensor(3.8255)\n",
            "Step 146 Loss:  tensor(3.7780)\n",
            "Step 147 Loss:  tensor(3.7553)\n",
            "Step 148 Loss:  tensor(3.7347)\n",
            "Step 149 Loss:  tensor(3.7124)\n",
            "Step 150 Loss:  tensor(3.6916)\n",
            "Step 151 Loss:  tensor(3.6689)\n",
            "Step 152 Loss:  tensor(3.6478)\n",
            "Step 153 Loss:  tensor(3.6251)\n",
            "Step 154 Loss:  tensor(3.6037)\n",
            "Step 155 Loss:  tensor(3.5823)\n",
            "Step 156 Loss:  tensor(3.5352)\n",
            "Step 157 Loss:  tensor(3.5068)\n",
            "Step 158 Loss:  tensor(3.4856)\n",
            "Step 159 Loss:  tensor(3.4636)\n",
            "Step 160 Loss:  tensor(3.4425)\n",
            "Step 161 Loss:  tensor(3.4206)\n",
            "Step 162 Loss:  tensor(3.3950)\n",
            "Step 163 Loss:  tensor(3.3738)\n",
            "Step 164 Loss:  tensor(3.3520)\n",
            "Step 165 Loss:  tensor(3.3240)\n",
            "Step 166 Loss:  tensor(3.2998)\n",
            "Step 167 Loss:  tensor(3.2781)\n",
            "Step 168 Loss:  tensor(3.2548)\n",
            "Step 169 Loss:  tensor(3.2333)\n",
            "Step 170 Loss:  tensor(3.2063)\n",
            "Step 171 Loss:  tensor(3.1847)\n",
            "Step 172 Loss:  tensor(3.1620)\n",
            "Step 173 Loss:  tensor(3.1365)\n",
            "Step 174 Loss:  tensor(3.1152)\n",
            "Step 175 Loss:  tensor(3.0906)\n",
            "Step 176 Loss:  tensor(3.0682)\n",
            "Step 177 Loss:  tensor(3.0443)\n",
            "Step 178 Loss:  tensor(3.0223)\n",
            "Step 179 Loss:  tensor(2.9981)\n",
            "Step 180 Loss:  tensor(2.9749)\n",
            "Step 181 Loss:  tensor(2.9514)\n",
            "Step 182 Loss:  tensor(2.9282)\n",
            "Step 183 Loss:  tensor(2.9057)\n",
            "Step 184 Loss:  tensor(2.8816)\n",
            "Step 185 Loss:  tensor(2.8591)\n",
            "Step 186 Loss:  tensor(2.8357)\n",
            "Step 187 Loss:  tensor(2.8129)\n",
            "Step 188 Loss:  tensor(2.7897)\n",
            "Step 189 Loss:  tensor(2.7670)\n",
            "Step 190 Loss:  tensor(2.7439)\n",
            "Step 191 Loss:  tensor(2.7211)\n",
            "Step 192 Loss:  tensor(2.6982)\n",
            "Step 193 Loss:  tensor(2.6757)\n",
            "Step 194 Loss:  tensor(2.6528)\n",
            "Step 195 Loss:  tensor(2.6301)\n",
            "Step 196 Loss:  tensor(2.6073)\n",
            "Step 197 Loss:  tensor(2.5846)\n",
            "Step 198 Loss:  tensor(2.5620)\n",
            "Step 199 Loss:  tensor(2.5394)\n",
            "Step 200 Loss:  tensor(2.5168)\n",
            "Step 201 Loss:  tensor(2.4945)\n",
            "Step 202 Loss:  tensor(2.4720)\n",
            "Step 203 Loss:  tensor(2.4496)\n",
            "Step 204 Loss:  tensor(2.4273)\n",
            "Step 205 Loss:  tensor(2.4049)\n",
            "Step 206 Loss:  tensor(2.3826)\n",
            "Step 207 Loss:  tensor(2.3605)\n",
            "Step 208 Loss:  tensor(2.3383)\n",
            "Step 209 Loss:  tensor(2.3162)\n",
            "Step 210 Loss:  tensor(2.2942)\n",
            "Step 211 Loss:  tensor(2.2722)\n",
            "Step 212 Loss:  tensor(2.2502)\n",
            "Step 213 Loss:  tensor(2.2283)\n",
            "Step 214 Loss:  tensor(2.2065)\n",
            "Step 215 Loss:  tensor(2.1847)\n",
            "Step 216 Loss:  tensor(2.1630)\n",
            "Step 217 Loss:  tensor(2.1414)\n",
            "Step 218 Loss:  tensor(2.1198)\n",
            "Step 219 Loss:  tensor(2.0983)\n",
            "Step 220 Loss:  tensor(2.0768)\n",
            "Step 221 Loss:  tensor(2.0554)\n",
            "Step 222 Loss:  tensor(2.0341)\n",
            "Step 223 Loss:  tensor(2.0129)\n",
            "Step 224 Loss:  tensor(1.9917)\n",
            "Step 225 Loss:  tensor(1.9706)\n",
            "Step 226 Loss:  tensor(1.9496)\n",
            "Step 227 Loss:  tensor(1.9287)\n",
            "Step 228 Loss:  tensor(1.9078)\n",
            "Step 229 Loss:  tensor(1.8870)\n",
            "Step 230 Loss:  tensor(1.8663)\n",
            "Step 231 Loss:  tensor(1.8457)\n",
            "Step 232 Loss:  tensor(1.8252)\n",
            "Step 233 Loss:  tensor(1.8048)\n",
            "Step 234 Loss:  tensor(1.7844)\n",
            "Step 235 Loss:  tensor(1.7642)\n",
            "Step 236 Loss:  tensor(1.7441)\n",
            "Step 237 Loss:  tensor(1.7240)\n",
            "Step 238 Loss:  tensor(1.7041)\n",
            "Step 239 Loss:  tensor(1.6842)\n",
            "Step 240 Loss:  tensor(1.6645)\n",
            "Step 241 Loss:  tensor(1.6449)\n",
            "Step 242 Loss:  tensor(1.6254)\n",
            "Step 243 Loss:  tensor(1.6060)\n",
            "Step 244 Loss:  tensor(1.5868)\n",
            "Step 245 Loss:  tensor(1.5676)\n",
            "Step 246 Loss:  tensor(1.5486)\n",
            "Step 247 Loss:  tensor(1.5297)\n",
            "Step 248 Loss:  tensor(1.5110)\n",
            "Step 249 Loss:  tensor(1.4923)\n",
            "Step 250 Loss:  tensor(1.4739)\n",
            "Step 251 Loss:  tensor(1.4555)\n",
            "Step 252 Loss:  tensor(1.4373)\n",
            "Step 253 Loss:  tensor(1.4193)\n",
            "Step 254 Loss:  tensor(1.4014)\n",
            "Step 255 Loss:  tensor(1.3837)\n",
            "Step 256 Loss:  tensor(1.3661)\n",
            "Step 257 Loss:  tensor(1.3487)\n",
            "Step 258 Loss:  tensor(1.3315)\n",
            "Step 259 Loss:  tensor(1.3144)\n",
            "Step 260 Loss:  tensor(1.2975)\n",
            "Step 261 Loss:  tensor(1.2808)\n",
            "Step 262 Loss:  tensor(1.2643)\n",
            "Step 263 Loss:  tensor(1.2480)\n",
            "Step 264 Loss:  tensor(1.2319)\n",
            "Step 265 Loss:  tensor(1.2159)\n",
            "Step 266 Loss:  tensor(1.2002)\n",
            "Step 267 Loss:  tensor(1.1847)\n",
            "Step 268 Loss:  tensor(1.1694)\n",
            "Step 269 Loss:  tensor(1.1544)\n",
            "Step 270 Loss:  tensor(1.1395)\n",
            "Step 271 Loss:  tensor(1.1249)\n",
            "Step 272 Loss:  tensor(1.1105)\n",
            "Step 273 Loss:  tensor(1.0963)\n",
            "Step 274 Loss:  tensor(1.0824)\n",
            "Step 275 Loss:  tensor(1.0687)\n",
            "Step 276 Loss:  tensor(1.0553)\n",
            "Step 277 Loss:  tensor(1.0421)\n",
            "Step 278 Loss:  tensor(1.0292)\n",
            "Step 279 Loss:  tensor(1.0166)\n",
            "Step 280 Loss:  tensor(1.0042)\n",
            "Step 281 Loss:  tensor(0.9921)\n",
            "Step 282 Loss:  tensor(0.9802)\n",
            "Step 283 Loss:  tensor(0.9686)\n",
            "Step 284 Loss:  tensor(0.9573)\n",
            "Step 285 Loss:  tensor(0.9463)\n",
            "Step 286 Loss:  tensor(0.9355)\n",
            "Step 287 Loss:  tensor(0.9250)\n",
            "Step 288 Loss:  tensor(0.9148)\n",
            "Step 289 Loss:  tensor(0.9049)\n",
            "Step 290 Loss:  tensor(0.8953)\n",
            "Step 291 Loss:  tensor(0.8859)\n",
            "Step 292 Loss:  tensor(0.8769)\n",
            "Step 293 Loss:  tensor(0.8681)\n",
            "Step 294 Loss:  tensor(0.8595)\n",
            "Step 295 Loss:  tensor(0.8513)\n",
            "Step 296 Loss:  tensor(0.8433)\n",
            "Step 297 Loss:  tensor(0.8356)\n",
            "Step 298 Loss:  tensor(0.8282)\n",
            "Step 299 Loss:  tensor(0.8210)\n",
            "Step 300 Loss:  tensor(0.8141)\n",
            "Step 301 Loss:  tensor(0.8075)\n",
            "Step 302 Loss:  tensor(0.8011)\n",
            "Step 303 Loss:  tensor(0.7949)\n",
            "Step 304 Loss:  tensor(0.7890)\n",
            "Step 305 Loss:  tensor(0.7833)\n",
            "Step 306 Loss:  tensor(0.7779)\n",
            "Step 307 Loss:  tensor(0.7727)\n",
            "Step 308 Loss:  tensor(0.7677)\n",
            "Step 309 Loss:  tensor(0.7629)\n",
            "Step 310 Loss:  tensor(0.7583)\n",
            "Step 311 Loss:  tensor(0.7539)\n",
            "Step 312 Loss:  tensor(0.7498)\n",
            "Step 313 Loss:  tensor(0.7458)\n",
            "Step 314 Loss:  tensor(0.7420)\n",
            "Step 315 Loss:  tensor(0.7384)\n",
            "Step 316 Loss:  tensor(0.7349)\n",
            "Step 317 Loss:  tensor(0.7316)\n",
            "Step 318 Loss:  tensor(0.7285)\n",
            "Step 319 Loss:  tensor(0.7255)\n",
            "Step 320 Loss:  tensor(0.7226)\n",
            "Step 321 Loss:  tensor(0.7199)\n",
            "Step 322 Loss:  tensor(0.7174)\n",
            "Step 323 Loss:  tensor(0.7149)\n",
            "Step 324 Loss:  tensor(0.7126)\n",
            "Step 325 Loss:  tensor(0.7104)\n",
            "Step 326 Loss:  tensor(0.7083)\n",
            "Step 327 Loss:  tensor(0.7063)\n",
            "Step 328 Loss:  tensor(0.7045)\n",
            "Step 329 Loss:  tensor(0.7027)\n",
            "Step 330 Loss:  tensor(0.7010)\n",
            "Step 331 Loss:  tensor(0.6994)\n",
            "Step 332 Loss:  tensor(0.6979)\n",
            "Step 333 Loss:  tensor(0.6964)\n",
            "Step 334 Loss:  tensor(0.6951)\n",
            "Step 335 Loss:  tensor(0.6938)\n",
            "Step 336 Loss:  tensor(0.6925)\n",
            "Step 337 Loss:  tensor(0.6914)\n",
            "Step 338 Loss:  tensor(0.6903)\n",
            "Step 339 Loss:  tensor(0.6892)\n",
            "Step 340 Loss:  tensor(0.6882)\n",
            "Step 341 Loss:  tensor(0.6873)\n",
            "Step 342 Loss:  tensor(0.6864)\n",
            "Step 343 Loss:  tensor(0.6856)\n",
            "Step 344 Loss:  tensor(0.6848)\n",
            "Step 345 Loss:  tensor(0.6840)\n",
            "Step 346 Loss:  tensor(0.6833)\n",
            "Step 347 Loss:  tensor(0.6826)\n",
            "Step 348 Loss:  tensor(0.6820)\n",
            "Step 349 Loss:  tensor(0.6814)\n",
            "Step 350 Loss:  tensor(0.6808)\n",
            "Step 351 Loss:  tensor(0.6803)\n",
            "Step 352 Loss:  tensor(0.6797)\n",
            "Step 353 Loss:  tensor(0.6792)\n",
            "Step 354 Loss:  tensor(0.6788)\n",
            "Step 355 Loss:  tensor(0.6783)\n",
            "Step 356 Loss:  tensor(0.6779)\n",
            "Step 357 Loss:  tensor(0.6775)\n",
            "Step 358 Loss:  tensor(0.6772)\n",
            "Step 359 Loss:  tensor(0.6768)\n",
            "Step 360 Loss:  tensor(0.6765)\n",
            "Step 361 Loss:  tensor(0.6761)\n",
            "Step 362 Loss:  tensor(0.6758)\n",
            "Step 363 Loss:  tensor(0.6756)\n",
            "Step 364 Loss:  tensor(0.6753)\n",
            "Step 365 Loss:  tensor(0.6750)\n",
            "Step 366 Loss:  tensor(0.6748)\n",
            "Step 367 Loss:  tensor(0.6746)\n",
            "Step 368 Loss:  tensor(0.6743)\n",
            "Step 369 Loss:  tensor(0.6741)\n",
            "Step 370 Loss:  tensor(0.6739)\n",
            "Step 371 Loss:  tensor(0.6737)\n",
            "Step 372 Loss:  tensor(0.6736)\n",
            "Step 373 Loss:  tensor(0.6734)\n",
            "Step 374 Loss:  tensor(0.6732)\n",
            "Step 375 Loss:  tensor(0.6731)\n",
            "Step 376 Loss:  tensor(0.6729)\n",
            "Step 377 Loss:  tensor(0.6728)\n",
            "Step 378 Loss:  tensor(0.6727)\n",
            "Step 379 Loss:  tensor(0.6725)\n",
            "Step 380 Loss:  tensor(0.6724)\n",
            "Step 381 Loss:  tensor(0.6723)\n",
            "Step 382 Loss:  tensor(0.6722)\n",
            "Step 383 Loss:  tensor(0.6721)\n",
            "Step 384 Loss:  tensor(0.6720)\n",
            "Step 385 Loss:  tensor(0.6719)\n",
            "Step 386 Loss:  tensor(0.6718)\n",
            "Step 387 Loss:  tensor(0.6718)\n",
            "Step 388 Loss:  tensor(0.6717)\n",
            "Step 389 Loss:  tensor(0.6716)\n",
            "Step 390 Loss:  tensor(0.6715)\n",
            "Step 391 Loss:  tensor(0.6715)\n",
            "Step 392 Loss:  tensor(0.6714)\n",
            "Step 393 Loss:  tensor(0.6713)\n",
            "Step 394 Loss:  tensor(0.6713)\n",
            "Step 395 Loss:  tensor(0.6712)\n",
            "Step 396 Loss:  tensor(0.6712)\n",
            "Step 397 Loss:  tensor(0.6711)\n",
            "Step 398 Loss:  tensor(0.6711)\n",
            "Step 399 Loss:  tensor(0.6710)\n",
            "Step 400 Loss:  tensor(0.6710)\n",
            "Step 401 Loss:  tensor(0.6709)\n",
            "Step 402 Loss:  tensor(0.6709)\n",
            "Step 403 Loss:  tensor(0.6709)\n",
            "Step 404 Loss:  tensor(0.6708)\n",
            "Step 405 Loss:  tensor(0.6708)\n",
            "Step 406 Loss:  tensor(0.6708)\n",
            "Step 407 Loss:  tensor(0.6707)\n",
            "Step 408 Loss:  tensor(0.6707)\n",
            "Step 409 Loss:  tensor(0.6707)\n",
            "Step 410 Loss:  tensor(0.6706)\n",
            "Step 411 Loss:  tensor(0.6706)\n",
            "Step 412 Loss:  tensor(0.6706)\n",
            "Step 413 Loss:  tensor(0.6706)\n",
            "Step 414 Loss:  tensor(0.6706)\n",
            "Step 415 Loss:  tensor(0.6705)\n",
            "Step 416 Loss:  tensor(0.6705)\n",
            "Step 417 Loss:  tensor(0.6705)\n",
            "Step 418 Loss:  tensor(0.6705)\n",
            "Step 419 Loss:  tensor(0.6705)\n",
            "Step 420 Loss:  tensor(0.6704)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khDoe7RQ0Qx4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "fea4cdb8-525f-4456-8328-140e701c2b1e"
      },
      "source": [
        "x_1 = torch.linspace(-3, 8, 50)\n",
        "x_2 = -w[0]/w[1] * x_1\n",
        "\n",
        "plt.scatter(X_1[:, 0], X_1[:, 1])\n",
        "plt.scatter(X_2[:, 0], X_2[:, 1])\n",
        "plt.plot(x_1, x_2)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df5Ac5Xkn8O+zs6PdWUk7I4GQViutJRuQjIVAsCEkazsGOQGMAJmkOM52qrDvovLFSUBxdBZnn41dqSBKKfuciisple3z1cHF/JLXmB8WxCKXinP4LHmRxC/FPrAklhWIH7tC0qz213t/9PTuTE+/Pd3Tb093z3w/VRTs7KrnHQk9/fbzPs/7ilIKRESUXm1xD4CIiMJhICciSjkGciKilGMgJyJKOQZyIqKUa4/jTc8991y1atWqON6aiCi19u/f/6ZSaonz9VgC+apVq7Bv37443pqIKLVE5Ijb60ytEBGlHAM5EVHKMZATEaUcAzkRUcoxkBMRpRwDOVGrOfgA8I11wF0F698HH4h7RBRSLOWHRBSTgw8AP/ozYLJofT12zPoaANbfEt+4KBTOyIlayU++NhfEbZNF6/W48AkhNM7IiVrJ2KvBXo8anxCM4IycqJXkVwR7PWpJfEJIIQZyolay8ctANlf5WjZnvR6HpD0hpBQDOVErWX8LcMPfAPmVAMT69w1/U18aw0RuO2lPCCnFHDlRq1l/S/j8s6nc9sYvV14HiPcJIaU4Iyei4Ezltk0+IbQwzsiJKDiTuW0TTwgtjjNyIgquVm6bteENxUBORMF5Vb/Y+fOxYwDUXP6cwTwyDOREFJxXbpu14Q3HHDkR1UeX22ZteMNxRk5EZrE2vOEYyInIrCDdo1wUNYKpFSIyy063/ORrVjolv2IuiH9j3dxrF/wecOB/ccMsA0Qp1fA37e/vV/v27Wv4+xJRTJydoAAAAeASf/Irga3PNWpkqSIi+5VS/c7XmVohoui5VbK4BXGAi6J1YCAnovr5zXEHCc5cFA2MgZyI6hOk8UcbnKXyS26YVRcGciKqT5DGH10lS/9nuGGWAaxaIaJqBx+orjpxBlht488xK9VS/ut0lSwM2kYwkBNRJb97jedXlNIqblT1r+Muh5FhaoUozaJoqKmVMrHfc+wYqnLcTtxjpSE4IydKq6hOoPfaK6WqHlxBWw9e63pkDGfkRGkV1S6DXnul6OrB8ytLi5YBrmfyaaLFW/0ZyInSKqpdBr32SvF6z6B7rJjas5z7n5sJ5CJSEJGHROQlEXlRRH7LxHWJyIOfXQbrmal67TXu9Z5Bzt80+TTB/c+N5ci/CeDHSqk/EJF5ALoMXZeIdGqdQK/LoR99Bvjlk95lgLoKk1rv6bcyxeTTBPc/Dx/IRSQP4MMAbgMApdQEgImw1yWiGmrVZutmqvu+M/f12DFg8I/nvq5V522qHlxXulhPe77Ja6VU6N0PReRSALsAvADgEgD7AdyulDrt+LktALYAQF9f3+VHjhwJ9b5EVMNdBXhWk5TLzgcwUz3TjqrT0m03xHrfz+S1Ei7K3Q/bAVwG4O+UUhsAnAaw3flDSqldSql+pVT/kiVLDLwtEXkKMiOdPN3YPHOQfHojr5VSJmbkywA8o5RaVfr6QwC2K6Wu1/0a7kdO1ACue4AHJcBdo/5a9ilykc3IlVLHARwTkTWllzbCSrMQUSPoKlPcZqrz5rtfQzShIL+C5X0pYKpq5U8B3FeqWHkZwKcNXZeIvHh1dwLux60NfhaYmZ77mbYMcNltlceuAXPVKF7lfZyVJ4KRQK6UehZA1XSfiCKmC7JPfAGYKlYH+Es+AetBvCyQow3ou9L6xy19snuL+3u3UHlf0nGvFaI00wXT4tvVr00Wgf3fA9R05eszk1YA3/qc+wyb5X2JxxZ9ojQLGkydQdzmNbsO0npPsWAgJ0ozXZDNLXb/ecm4v+51Q2B5X+IxtUKUZrpOS8C9SeaST+gXNWu9T5DAHaRckaWNoTGQE6WdV5B1C5C6RU039QRZv/ukH3zAWpQtz+eb2lO9xYRuCKoHG4KIEsYtYAP1tb7Pnh7kkF85V844e7qQJv7kFls175ylV9A1BDGQEyVVo1IOur1K2nPu1S/5lVaFi47XHi/ZXH2dppl5wE3fcv/8LZSaiXKvFSIyrVHdlAcfAH7wWfdadLcgDlhj8drjXLdwKpn6twuYngAevaP6dXadAmAgJ4pG2KPHGnFYgh0EdSWJWlIZOHdvAR7987lvu1XSQOp4H4eJ09W/jzxUAgAXO4nMM3EoctjDEpzphgt+r/owCdfzN2txy2sra4/zviutL+3rSqYUvGsczhyEc1sAHioBgIGcyDwTe5OE6aZ0u5E4D5OoZ1fE/Er3Mdl+dAcq9jQ3HcSB6gDNrlMATK0Qmec1S/SbcgnTTelnpm3PmIPY+lypKUh3TZc9zU0GcaA6QLPrFAADOZF5utlgbpH/hbkw3ZR+0wpq2iWXrWEH/UYEyPxKYPXvwJrNl3EL0Ow6BcDyQyLzTJfzBaWr43Z739m67lf1aQrbXWPWv/9qubXw6JsjvdKWBToWWr8Xdh7dHosdgJ3NQrnFwHX3tFyAdtKVHzJHTmSarm3e9Hawuvppt5PunXTph9xi/c3Gtv7Wypy7JwH6P1O90OoVkN1uhFNhTjlqfgzkRFFwa5uf7Wh0qGdhzk9lTK2qFaD6Gpl51ox5ZnLuvZxB/5dPBhioAjZ9Pdhn87tY3EKNQLUwkBM1ittMud6FuVrBzs8mV99YV32N6Yna7fF+0jY2r8VRHT8lhWFLPJvsJsBATtQoupRLPQEkSP30bNA6NpeTnq3xdlF8B/jCK+7fK2/8qaXem5SfksIwJZ4m6vwThoGcqJGCbger47d+2hm07ODt1WWpS/UcfADY911/45O24NUj5Tcc5wKp86YQphGoCc8gZfkhURr5rZ8O2r3pNYv+ydfguy68s2AFRb918xV7pqD0PqXyQ7eSQt3Nxs96g4mu2TDbL0SAM3KipPPK59ZK0wSpiHGWADoFuVbxnWApDNcbjtKXZoZZbzDdNZuAtAzryImSTFeT7jdt4bemfN58INPhXbft91rA3CKnbl/yrc9V3qC0M30B7hp1/1a9C5Zhfk+99lo32QugwW1sieIS5lE87O5+G7+Mqg5JNxOnK+vHi28Dg39cOVbXXQ1d2DPjWlsVlHe56tQ6S3Trc1ag3/qc/xlxFF2zMW/SxdQKUZTCPoqHDRzrbwGOPlNapAz49D0zWbkA6HU+qNvMWFc3D2XtgV5rW9swe6bUmq3Xu+ic0E26OCMnilLYGbU2QCj/s/tNXwdu3lVnTbcjaNmz4Jt3WV/v3mJ9lo1frp4Ze83gPYN4yD1TojxsIqGbdDFHThQlr2PPIP4OP/Zqt5/dt+Sd6mu5zUpdZ+ceW81KBviKo2U/SI65oqTQBxO55qjz2DE2E3GvFaI4eG5EpWqnWirSGS7XmZmcy22XXwsAfvg5q1PT/t7gZ2E9hDuC+OoPA6/8b80QXWbOQeqw7RSG5w2txNTMNuo8tqleAIOYWiGKkp8FwlqpFjud4WfR0r7WE1+YC+K2menKPVQAAAp4+2WrSsWNWzqmnkDpdY6n6e1nw9SYpxQDOVGUnBUSOn5mi34D0dir+oOTdT9/3T3+c7/1BEpdbvnjfx+86qSWhOaxo8RAThS18jI53YKjnyDtt/wvtyjY+KTNWrRsz5Vm5jVmyH4CpbPkEjB7AIRXSWcLHjbBxU6iRgrb4FO+0JZbBEycqkyheB1g4UfNRUuPbXHLF1nDfMZaor5+gukWOxnIiRrNZNWD27V2b4F2YbH/P8wFYGlzX8x0VncEDZxRV43E3F0ZJ1atECWFyaqHIAdY5BZXHvJwV8H9ms58fdDdAqOuGklod2WcjOXIRSQjIkMi8qipaxJRHXQ57OvuqXzN76Jl0MAZddVIC1al1GJysfN2AC8avB4RBWWnWiaLpdI+6Bf7/FZ3BA2cUVeNtGBVSi1GArmIrABwPYBvm7geEdXBuae3mp4LcLpmIz/VHUEDZ9RVIy1YlVKLkcVOEXkIwN0AFgL4C6XUJpef2QJgCwD09fVdfuTIkdDvS0RlolwETOsZl2kdt0Zki50isgnAG0qp/SLyEd3PKaV2AdgFWFUrYd+XqOkFDUJRLgImsC29poQeAhEFE6mVAQA3isivAXwfwNUicq+B6xK1rnp28OMiYKWwO0+mSOhArpS6Uym1Qim1CsCtAPYqpT4VemREzSTo4RL1BCEuAlZqoTJFtugTRa2e2XU9QYiLgJVa6AnFaEOQUuqfAPyTyWsSpV7QhhrA30k0uhy6ycCd5sXCMAc0pwxn5ERR086uj+ln5bXSJFGegmNrxHtEqYWeUNiiTxQ1r8MldFUUuvMxy18POssPqhHvEbU0VtvUgYGcyBRdGsLtEd/mFRi9glAjFvJaaLEw7RjIiUzwU7O8+4/cf209gbERp7kn9MR4qsYcOZEJtcoF198S7lAJp0aUGrKcMTUYyIlM8JOGMBkYG7GQ10KLhWnH1AqRCX7SELUWMINqxEJeiywWph0DOZEJfmuWGRibS0Lq7BnIiUwwPdum5EvQplwM5ESmcLbdWhJUZ8/FTqJWFHQTL6qWoDp7BnKiVpP21vukSNCmXAzkRK1GlxJ44gvxjCetElRnz0BO1Gp0j/7FtzkrDyJBdfZc7CRqNV6beKVpQ6wkSMgCN2fkRK3G69GfG2KlEgM5UatZfwuQW+z+PW6IlUoM5ESt6Lp7zCzUsYwxEZgjJ2pFJjpRE9TZ2OoYyIlaVdiFugR1NrY6plaIqD4J6mxsdQzkRFSfBHU2tjoGciKqT4I6G1sdAzkR1SdBnY2tjoudRFS/hHQ2tjrOyImIUo6BnIgo5RjIiYhSjoGciCjluNjZhAaHhrFzz2G8NlrE8kIO265Zg80beuMeFhFFhIG8yQwODePO3YdQnJwGAAyPFnHn7kMAwGBO1KSYWmkyO/ccng3ituLkNHbuORzTiIgoagzkTea10WKg14ko/UIHchFZKSJPi8gLIvK8iNxuYmBUn+WFXKDXiSj9TMzIpwB8Xil1EYArAXxORC4ycF2qw7Zr1iCXzVS8lstmsO2aNTGNiIiiFnqxUyk1AmCk9N/visiLAHoBvBD22hScvaDJqhWi1iFKKXMXE1kF4J8BrFNKnXR8bwuALQDQ19d3+ZEjR4y9LxFRKxCR/UqpfufrxsoPRWQBgIcB3OEM4gCglNoFYBcA9Pf3m7t7NIFmrvtu5s9GlBRGArmIZGEF8fuUUrtNXLNV6Oq+9x15G0+/dCLVAZA17USNETqQi4gA+A6AF5VSXw8/pNaiq/u+75mjsB9bog6AUc2avWraGciJzDFRtTIA4A8BXC0iz5b++ZiB67YEXX23M/cUtqlncGgYAzv2YvX2xzCwYy8Gh4ZnX79z9yEMjxahMHfTsL8fBmvaiRrDRNXKvwAQA2NpScsLOQz7DGz1BkCvFEeUs2bdZ2NNO5FZ7OyMmVvdt+6uWG8A9ArW9cyadbN7J9a0EzUGA3nMNm/oxd03X4zeQg4CoLeQwyev7DMaAL2CddBO0CCpGLfPdvfNFzM/TmQYdz9MgM0bequCW/97FhtbgPRKcWy7Zk1F2gXwvmkETcW4fba0Y0klJQ0DeQPU8xffZAD0CtZBO0FbfQGTJZWURAzkETP9F7/emwKgD9ZBbhq62X2hKxt6nGnAkkpKIgbyiJn8ix/mpmBqhr/tmjXY9tABTE5XFkieGp/C4NAwNm/obepZa6s/kVAycbEzYib/4ifh0IjNG3oxf171/X9yRs2OIwnjjAq3CaYkYiCPWJC/+LXK+nTBf3i0aKSBx+84xoqTrr/OHl8zz1pZUklJxNRKxPxWhfhJR3g1D7mlLgaHhnHXI89jtBR4F3Vl8ZUbPuCZ3ggzDvvm1MyNQNwmmJKIM/KI+a2l9pOOcJsN6n52cGgY2x48MBvEAeCdM5PY9tAB7ex9cGgYn3/gQF3jKL856cZ5ZmLK6JNDXDZv6MVPt1+NV3Zcj59uv5pBnGLHGXkD+Flo9JOOsK9xx/3P1vzZnXsOY3KmerfgyWnlutBqz8SnNfvTu42jfFZ61dol2LnnMLbe/yyWF3L4/ct78eiBkaobSbMsehIFMTE1g9dPjuP4yXFceN5C5B1VXmExkCeELh2Rz2UxsGNvxWN8r4/UhVc++rVSTr08EJ+ZmKqaieuuDVTenNzSMQ/vH0ZHe/UDH0v1qNnYQXpkbBwjY0WMjI3jeNl/j4yN481TZ2HPkb736d/AR9acZ3QMDOQJ4ZZLz7YJTk9Mzc5q7Xz171/ei4f3D3vm3b3y6YWubFXg9VJrMU+XFtLdGJK46Nmsde8Uztmpabw+dhYjY0UcPzmO10bHcbwsQNtB2mlBRzt68p3oKeTw/mXdWJbvxPJCJ5blc1jfmzc+TgbyhHBLV5yZmMI7ZyorRIqT03j6pRO4++aLPQPPtmvWYNuDB6rSK9mMQCl4zr7LZURq7o8SNDAnbdGzmeveSW98cnp2Jn18bByvjRVLM2lrNn18bBxvnpqo+nULO0tBOp/DRT3d6Cl0Ynk+h2X5TvTkO7Es34mFnWZTJ7UwkCeIM5e+evtjrj/32mixZt7d/p5b1cpWTY7dKZsR7PyDSwCgKr1T/t662f+irizGJ2d87+MSF3ZrNp/xyenZoHz8ZLE0k64M0m+drg7S3Z3t6Mnn0FPoxMW9eSzrzpVm1naQzmFBR/LCZvJG1GK8HunDlvHZwX5waBhf/dHzeOfMJO64/1nfm8fbjT+1Zqu6Esuv3PABAMkv1WvmuvdmND45XRGQy//7tVFrQfFtlyCdz2VnZ8zrVxRKs2prZm3PpucnMEj7kc5RN4laj/RBdybUvYezpd7vydejxUlfs1U/e7kkWTPXvadNcWK6KkDPpT6s/LQz3QhY6z7LujuxvJDDpX0F9HR3lvLSc0G6y6UjuVk07yeLiMlFsVpB0kTzyc49h6v2RbFlRDCjlDawZ0R8z1bTvF2tiRsm1XZmYqqsomMcI6NFjJwsBelRazFx1CVIL+rKYlk+h+X5TlzWV5idRduz6558Drl57v0VrYKBPADTi2J+a8fDBEiv9MCMUnhlx/VYpcnFTyulLXV07naYBPXeZNmtGd6ZiamyPPRcVUd5hYfb1g7nzJ+HZflOrFjUhd9YtXiuuqN7LlB3aprgaE5LBfKws2nTi2KNeKT3KkPM56xgrAvWvaXfo1q7Hbpx/l5ftXYJnn7pRGSBMuxNNs1PFFE7dXbKmj2PjbumPEbGijg5PlX1685dYAXplYu7cMXqUpAuy0cv7WaQNqVlArmJ2bTpRbFGPNLrAjEAnC61zNc6eKK88sVm73bo1iFqL6zahkeLuPeZoxVfmy7vY+VJfd4dn6yYPc/Oqk9aqY/jY+N496xbkO5AT74T7zmnC1e+d7GV+ih0Ylm3lepYmu9ARzuDdKO0TCA38Rfd9Ay6EY/09rX+/IFn4ezYt9v1f7r9as9x1Nrt0Oa8WXoxHWRZeVJJKYV3z05hZLSsosOR6jg+No5TjiAtApwzvwPLC51475L5GDj/3Ln66NJi4tLuTsxz6dql+LRMIDfxFz2KGXQjHuk3b+jV1o7bn99rHH5vYG43Sy8mg2wrVZ4opXByfKoyvWGnPk6WFg7HxnF6ovLPQgRYUppJn79kAT54/rmz3Yd2oGaQTqeWCeRh/6LbOd/i5DQyIrMLgWlZFCt0ZV3Ltvx8fr83sDg7POu5ySaxLV8phZPFqaouQ2f34RldkC7kcOHShfjwhUuqaqSXdncim2GQbkYtE8jDzKadKYNppaoOL06ywaFhnHJZjMpmBFetXeLZtQn4TwF5Law6mV4LCJqmiqMtXymFseJkqWmllOIYnes+tP/b+VTTJsB5C60KjjVLF+IjF55X1W143sIOBukWJkqzbWmU+vv71b59+xr+vvXOwAZ27NVWddj55STTjT+XbQMgFYFDAHzyyj785eaLA7+PLkdeyGWx6ZKeSKtWgjL9Z6qUwuiZSe1M2v56fHKm4te1CbDUbl5x7Ndh10qft7AD7QzSBEBE9iul+p2vt8yMHKg/H520hbSgNyTdOIuOoAJYXZ/3PnMUjx4YwV03ep8m5JSmeuwgf6ZKKbxzZnI291xe0VEesM9OVf5+ZtoESxd2YFm+Exf1dGPj2vOqug2XLGCQpvBaKpDXS5cyaBPxrKWupZ4nhFopAbdr5nPZqvLBWkaL9R0CkZZ6bN2faXcui3t+/FJVY8uEI0i3twmWdlvBeF1vHr970dLZ7kN7Nr1kYQcybX53tiGqX0ulVurlVVaXy2ZqbvPq95p+ruWVEtDtaT4DYNq5nW2btWDrcohQ1XXdUg1JXCh0mplReOv0xGxQLt9P+tDwGF4+cdp1e4JsZi5IO1vB7Y2Wzl3QgTYGaWowXWqFgdwn+zxLt6PQ6smr1srR6gLl6u2P+d70yssiTRWLkwB4Zcf1Fa/pbmx+Dnc2ZWZG4c3TZ+cWCx310SMni3h97Cwmpitn0tmMWEG5O4fpGYXDr7+LU2ensLhrHm4bWIVbr1iJc+czSFMyMUcegC6I1qrFDsIrR+uVPglSGeJl9MyktjW/nFuJoK5e3NSZnNMzCm+dOot/+L9H8T/+9QjePjOBBR3tuGDpArS3CV4bHccb745XdavOy7Rhab4DPfkcLu9bhGVlM2i7DO+c+fMYpKnpMJA71BNE66mH9rqWVxeqW/qkHss1qZhybiWCg0PDnsG/Vsfm9IzCm6fOzi0culR3vH5yHFOOnM+ps1MYOjqKTJtgekZhQUc7brxkKa5d1zOb+jhn/jyIMEhT62EgdwgaROuth/a6Vq2Zf2e2zXcgz7YJIKiavZ4utWaXHxlX6MpCKasl3y3vbd/kahkeLeLxQyNVW5Ue1wTpjva22SqO3yxtrnTfz466bg1g5/pPnZ3C44eO40MXLMG6CM5AJEoTIzlyEbkWwDcBZAB8Wym1w+vnk5wj1+Wg7Vxx0EU+r5/X7RCoPTQ5l8XZqZmqum/dn6C9AAqgaiMrINhC7dT0DD54z9M4fnK85s+W62hvs8rtuq0GFuvfldUdi7qyVTNpv2sBaanlJzIhshy5iGQAfAvA7wJ4FcDPReQRpdQLYa8dh1rpkyDldbVKBcuvNTg07HpYsi2XzUCk+tBkXbDLtknFTWPnnsOuBznv3HMYm9b34I13z1a2gju6D994d7xmhYv9vrcNrMLHN6xAT74TBZcg7YfftYBW3RSLqJyJ1MoVAH6llHoZAETk+wBuApDKQG4yfRJkx8W7HnleG8QzYnVfBsmL29vMXl8K0rqgODxaxIVfeqIqSOeymdkW8IHzz8XyQif+5/854lqPbp80ZLIM0e9aQDNuikUUlIlA3gvgWNnXrwL4TecPicgWAFsAoK+vz8DbRsNkd2KQ7kGvhh23kkc/7CDt9cu75mXwHz+4eq7Co2CV5nXn2qtm0u9bsqCu2vd6OP8cCl1ZnBqfqrjZ8Tg2IkvDFjuVUrsA7AKsHHmj3rceproT495atWteBn/0ofeiJ9+JV948je/9668r2shz2Qz+6uO1g3B5Lj+fy6Iz24bRM+4LoiY5/xzS0IREFAcTgXwYwMqyr1eUXkstUwFDl6bZ+tELcPStM7PdhiNj4+hob6vaq6OWhZ3t+Ni6HhS6stogDVTOanPZjLYqxY0zzz9anEQum8E3/t2lDQ+iaWn/J2o0E4H85wAuEJHVsAL4rQA+YeC6sQi7ven45DReLwVnBYWN7z8PT7/0Bk5PTCObEWTaBH/x0MGqX5cLcHahW6XG+3u6q24+ACo+yztnggfhek5W4syZqLFCB3Kl1JSI/AmAPbDKD7+rlHo+9Mhi4hW4rl23THv4rN0e/vbpiapr5nNZrF3WVbFfh3Pb0vkd7RgcGnY9H7OcLi/sNlsd2LE39PF2QXd+jGOf76B4o6FmYyRHrpR6HMDjJq4Vl+LENI6fHPes7lj7X39c9boIsKy7E2uXLcT6FYWK+ujyIO3H5g292LnnsDaQBz2RyMT2u7Xy/M6geGZiKtGHIKfhRkMUVEt0dhYnprUH0Nqz69EaG0h1zcvgP/3O+zAyNo6H9r86uxmTUta+JTddaiZ/qwuyAgRufDGx2OpVjvmlwUO475mjs7XsXnXfSan3NnEIN1HSpD6Qn5mYKmtiqdz43z6M1i1IL+rKoqe0f/RlfYXZ7sP/d+IUvvMvr2irOwZ27K3aUc9EILBntrpynnoqXUydY1nexl+efy8P4rUkpd47aYeEEJmQqkD+xKER/PMvT1QE7pMuZ1Eunj8PPflOrFiUQ/+qRa556U6PxcULly7U5lCjCARe+50D9ddLmzrH8u6bL656GhjYsdd3EE9SvXfcJaFEUUhVIN9/5B089cLrWJbvxIpFXbiitMFST74Ty7pzWF6wTgr3CtJ+eJW5RREIdNvC2jra6z8KLEjJ3l2PPO877eB14yrkspjf0R7bYqLXYqbJzl2ipEhVIP/i9e/HlzZdFOsYTAeCWtvCArWPXTNRhTE4NKxdZHUL2robmgCBz/o0yc/+NkA6zhUl8itVgfyHz74W+19Ak4HA77awgH5mbKoKY+eew9rvuT1tuN3QBMAnr+yLNSj6WcxkYxE1m9QE8kaVjbnNboHqwG1i69RaKRUnt5mxqSoMr1SJrm7dfv8kzWy5mEmtKDWBvBFlY243i20PHqg4mMHkDSRocMnnsr6vEfTaulTJoq6s9nMmcWbLxUxqRfWvojVYI2ZabjeLyRlVdbqOfQMJq9BVHZgBK3hmXc6VPD0xhcGhym1sdAEqaODads2aqm0CctkMvnLDBwJdB7BuiAM79mL19scwsGNv1ZijpPscXMykZpaaQG4qYHkJclMIewMZHBrGKZfSyWxG8JUbPoAFndUPS5PTquoGYipwbd7Qi7tvvhi9hRwEVhfp3Tdbm25t+NqTWLX9Maza/hgu/eqTnoHZfqoZHi1CwXqC2Xr/s1jVoKCu+xxJe3IgMik1qZVGlI0FOaE+7A1k557DrgdJzJ/Xjs0bemue2x2DcxAAAAcYSURBVGkzmat22zb28w8emD0nE7AqaLY9eKDivZ2fS3eKUaPa4ZOY8iGKUmoCeSMW19xuFm6HF5u4gehm9PaBw4WubNXRbPbrTlEFrq/+6PmKIG6zTx9ye89aTypshycyLzWBHIh+pqW7Wbi9FnYctRbldKf6GDgr2ze3G4lNF7D9PNWwgoTIrFQF8kbQ3SxM30BqpYrGNM05bq/HsS2rLrXk56xNVpAQmcVAHpNaqSK/ZXRR1tcXclltt6cutVT+uYZHixCgYk8WVpAQmSeqkc/qJf39/Wrfvn0Nf980cdtIy+2g44Ede10DvtspQvWMYduDB6oWZT91ZR/+cvPFvq9RfrO6au0SPP3SiUQ1ERGlhYjsV0r1O1/njDyh/C7uRllfb2KBuTxVpXt62HfkbQZ3ohAYyBPMz+Ju0E7GoPl0kwvMuu5c5+EUPLGHKJjUNASRuyANQW7NOnfuPtSwzkvdU4IzuWeqc5aoVTCQp1yQTkav/WoaIUi1CksUifxjaqUJ+E1/xL0z4FVrl1QdD+esarGxRJHIPwbyJuWWC49zZ8DBoWE8vH+4Koj/9vsW4xdHx3hiD1EITK00IV0u/Kq1S2LbGVC3B8uv3ypykyuikDgjb0K6XPjTL53A3TdfHMthEF5pHW5yRRQOA3kTSmLQ5IEPRNFhaqUJNWLv9qB44ANRdBjIm1ASgyYPfCCKDlMrTSipByMzF04UDQbyJsWgSdQ6mFohIko5zsjJmCAbcsVxGAZRs2IgJyOCHHAR5WEYRK2IqRUyIsiGXHFv3kXUbEIFchHZKSIvichBEfmBiBRMDYzSJciGXHFv3kXUbMLOyJ8CsE4ptR7AvwG4M/yQKI2CNCElsWGJKM1CBXKl1JNKqanSl88AWBF+SBSlwaFhDOzYi9XbH8PAjr3GDpUI0oSUxIYlojQzudj5GQD3674pIlsAbAGAvr4+g29LfkW5yBikCSmpDUtEaSVKuW3rX/YDIv8IYJnLt76olPph6We+CKAfwM2q1gUB9Pf3q3379tUxXApjYMde142regs5/HT71TGMiIiCEJH9Sql+5+s1Z+RKqY/WuPBtADYB2OgniFN8uMhI1JzCVq1cC+A/A7hRKXXGzJAoKlxkJGpOYatW/hbAQgBPicizIvL3BsZEEeEiI1FzCrXYqZQ639RAKHpcZCRqTmzRbzHcFZGo+bBFn4go5RjIiYhSjoGciCjlGMiJiFKOgZyIKOVqtuhH8qYiJwAcafgb1+dcAG/GPYiINPNnA5r78/GzpVeYz/cepdQS54uxBPI0EZF9bnsbNINm/mxAc38+frb0iuLzMbVCRJRyDORERCnHQF7brrgHEKFm/mxAc38+frb0Mv75mCMnIko5zsiJiFKOgZyIKOUYyH0QkZ0i8pKIHBSRH4hIIe4xhSUi14rIYRH5lYhsj3s8pojIShF5WkReEJHnReT2uMdkmohkRGRIRB6NeyymiUhBRB4q/X17UUR+K+4xmSIiW0v/Tz4nIv8gIp2mrs1A7s9TANYppdYD+DcAd8Y8nlBEJAPgWwCuA3ARgH8vIhfFOypjpgB8Xil1EYArAXyuiT6b7XYAL8Y9iIh8E8CPlVJrAVyCJvmcItIL4M8A9Cul1gHIALjV1PUZyH1QSj2plJoqffkMgBVxjseAKwD8Sin1slJqAsD3AdwU85iMUEqNKKV+Ufrvd2EFgqbZgF1EVgC4HsC34x6LaSKSB/BhAN8BAKXUhFJqNN5RGdUOICci7QC6ALxm6sIM5MF9BsATcQ8ipF4Ax8q+fhVNFOxsIrIKwAYAP4t3JEb9N1jn5M7EPZAIrAZwAsB/L6WOvi0i8+MelAlKqWEAfw3gKIARAGNKqSdNXZ+BvERE/rGUu3L+c1PZz3wR1qP7ffGNlPwQkQUAHgZwh1LqZNzjMUFENgF4Qym1P+6xRKQdwGUA/k4ptQHAaQBNsX4jIotgPfWuBrAcwHwR+ZSp6/OotxKl1Ee9vi8itwHYBGCjSn/x/TCAlWVfryi91hREJAsriN+nlNod93gMGgBwo4h8DEAngG4RuVcpZSwgxOxVAK8qpewnqIfQJIEcwEcBvKKUOgEAIrIbwG8DuNfExTkj90FEroX1OHujUupM3OMx4OcALhCR1SIyD9aiyyMxj8kIERFYOdYXlVJfj3s8Jiml7lRKrVBKrYL1Z7a3iYI4lFLHARwTkTWllzYCeCHGIZl0FMCVItJV+n90Iwwu5HJG7s/fAugA8JT1Z4BnlFKfjXdI9VNKTYnInwDYA2v1/LtKqedjHpYpAwD+EMAhEXm29Np/UUo9HuOYyL8/BXBfaYLxMoBPxzweI5RSPxORhwD8AlZ6dggGW/XZok9ElHJMrRARpRwDORFRyjGQExGlHAM5EVHKMZATEaUcAzkRUcoxkBMRpdz/B6tKSLuVH6dRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcBwKUAP1sYE"
      },
      "source": [
        "We see that our above classification line cuts through one of the classes. Can you explain what's happening?\n",
        "\n",
        "This is something we can fix fix by adding a bias term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-oL69Uu1ant",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a430d9ab-27c4-4486-9ffe-5568ee6dae7a"
      },
      "source": [
        "X_orig = torch.cat((X_1, X_2))\n",
        "\n",
        "X_extra_dim = torch.cat((torch.ones((X_orig.shape[0],1)), X_orig), dim=1)\n",
        "indices = list(range(X_extra_dim.shape[0]))\n",
        "\n",
        "import random\n",
        "random.shuffle(indices)\n",
        "\n",
        "X = X_extra_dim[indices]\n",
        "Y = Y_orig[indices]\n",
        "print(X)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.0000e+00,  5.1917e+00,  6.2001e+00],\n",
            "        [ 1.0000e+00, -1.0905e-01,  4.5680e-01],\n",
            "        [ 1.0000e+00, -6.1245e-01,  1.3616e+00],\n",
            "        [ 1.0000e+00,  4.3635e+00,  4.2201e+00],\n",
            "        [ 1.0000e+00,  5.2688e-01,  7.6471e-02],\n",
            "        [ 1.0000e+00,  3.1012e+00,  3.4726e+00],\n",
            "        [ 1.0000e+00,  3.4479e+00,  4.3845e+00],\n",
            "        [ 1.0000e+00,  1.5091e+00,  1.2509e-01],\n",
            "        [ 1.0000e+00,  5.2372e+00,  4.9303e+00],\n",
            "        [ 1.0000e+00,  4.7429e+00,  5.2426e+00],\n",
            "        [ 1.0000e+00, -7.5121e-01, -8.4012e-02],\n",
            "        [ 1.0000e+00,  9.6864e-01,  1.1601e+00],\n",
            "        [ 1.0000e+00,  4.6026e+00,  5.9232e+00],\n",
            "        [ 1.0000e+00, -8.0155e-01, -7.7829e-01],\n",
            "        [ 1.0000e+00, -3.1384e-01,  3.0914e-01],\n",
            "        [ 1.0000e+00,  7.3604e-01,  1.0907e+00],\n",
            "        [ 1.0000e+00, -1.7648e-01,  1.1668e+00],\n",
            "        [ 1.0000e+00,  4.6362e+00,  4.5970e+00],\n",
            "        [ 1.0000e+00, -2.3739e+00,  6.4508e-01],\n",
            "        [ 1.0000e+00,  3.2126e+00,  5.1997e+00],\n",
            "        [ 1.0000e+00,  6.4366e+00,  4.8319e+00],\n",
            "        [ 1.0000e+00,  5.3101e+00,  6.4257e+00],\n",
            "        [ 1.0000e+00,  3.7506e-01, -8.3395e-01],\n",
            "        [ 1.0000e+00,  1.6684e+00,  8.0967e-02],\n",
            "        [ 1.0000e+00, -2.0624e+00, -7.1164e-01],\n",
            "        [ 1.0000e+00,  1.2041e+00, -1.8749e+00],\n",
            "        [ 1.0000e+00,  8.0473e-01, -1.8492e+00],\n",
            "        [ 1.0000e+00,  6.5035e-01, -4.2160e-01],\n",
            "        [ 1.0000e+00,  4.5011e+00,  5.1132e+00],\n",
            "        [ 1.0000e+00,  6.1279e+00,  5.2123e+00],\n",
            "        [ 1.0000e+00,  5.2065e+00,  5.2392e+00],\n",
            "        [ 1.0000e+00,  6.7576e+00,  3.8339e+00],\n",
            "        [ 1.0000e+00,  5.3072e+00,  2.7621e+00],\n",
            "        [ 1.0000e+00, -1.7379e+00, -3.5639e-01],\n",
            "        [ 1.0000e+00,  1.5261e-01,  1.7017e+00],\n",
            "        [ 1.0000e+00,  4.6249e+00,  5.6380e+00],\n",
            "        [ 1.0000e+00,  6.3844e+00,  4.1843e+00],\n",
            "        [ 1.0000e+00, -1.7795e+00,  2.6245e-01],\n",
            "        [ 1.0000e+00, -1.8556e+00,  4.5374e-01],\n",
            "        [ 1.0000e+00,  4.1502e+00,  4.3885e+00],\n",
            "        [ 1.0000e+00,  1.0430e+00,  6.8720e-01],\n",
            "        [ 1.0000e+00,  5.8734e+00,  5.2962e+00],\n",
            "        [ 1.0000e+00,  4.8323e+00,  4.5575e+00],\n",
            "        [ 1.0000e+00,  4.5979e+00,  4.3656e+00],\n",
            "        [ 1.0000e+00,  4.7348e+00,  4.8912e+00],\n",
            "        [ 1.0000e+00,  3.3290e+00,  4.5390e+00],\n",
            "        [ 1.0000e+00,  6.4992e+00,  5.2936e+00],\n",
            "        [ 1.0000e+00, -1.0722e+00, -6.4103e-01],\n",
            "        [ 1.0000e+00, -2.4466e-01, -1.4346e+00],\n",
            "        [ 1.0000e+00, -9.3250e-01,  1.2459e+00],\n",
            "        [ 1.0000e+00,  5.6046e+00,  4.6589e+00],\n",
            "        [ 1.0000e+00,  1.1026e+00, -1.1505e+00],\n",
            "        [ 1.0000e+00,  5.4311e+00,  5.0159e+00],\n",
            "        [ 1.0000e+00,  4.7261e+00,  4.2056e+00],\n",
            "        [ 1.0000e+00,  5.2216e+00,  5.1471e+00],\n",
            "        [ 1.0000e+00,  5.9013e+00,  6.6037e+00],\n",
            "        [ 1.0000e+00, -1.0754e+00,  2.6652e-01],\n",
            "        [ 1.0000e+00,  5.6201e+00,  5.2039e+00],\n",
            "        [ 1.0000e+00,  4.9232e+00,  5.0773e+00],\n",
            "        [ 1.0000e+00, -4.0766e-01, -2.2669e-02],\n",
            "        [ 1.0000e+00, -4.4538e-01, -6.1529e-01],\n",
            "        [ 1.0000e+00, -1.8903e+00, -7.3013e-01],\n",
            "        [ 1.0000e+00, -4.5604e-01, -1.0450e+00],\n",
            "        [ 1.0000e+00,  8.6509e-01, -2.6122e-01],\n",
            "        [ 1.0000e+00,  6.0928e+00,  4.9269e+00],\n",
            "        [ 1.0000e+00, -1.3154e+00,  1.4155e+00],\n",
            "        [ 1.0000e+00,  8.7758e-02,  4.4489e-01],\n",
            "        [ 1.0000e+00, -9.5290e-01,  1.4191e-02],\n",
            "        [ 1.0000e+00,  6.0930e+00,  5.3283e+00],\n",
            "        [ 1.0000e+00, -1.1074e-01,  8.7231e-01],\n",
            "        [ 1.0000e+00, -2.0260e+00, -2.2019e-01],\n",
            "        [ 1.0000e+00,  8.2840e-01,  6.1434e-01],\n",
            "        [ 1.0000e+00, -6.1291e-02,  4.3047e-02],\n",
            "        [ 1.0000e+00, -9.3196e-01, -5.0740e-02],\n",
            "        [ 1.0000e+00,  5.5131e+00,  5.4309e+00],\n",
            "        [ 1.0000e+00,  4.0209e+00,  4.4017e+00],\n",
            "        [ 1.0000e+00,  2.8071e-02,  1.0445e+00],\n",
            "        [ 1.0000e+00,  3.3685e+00,  3.7115e+00],\n",
            "        [ 1.0000e+00,  5.5255e+00,  6.8611e+00],\n",
            "        [ 1.0000e+00,  6.2032e-01,  2.5766e-01],\n",
            "        [ 1.0000e+00,  6.7074e-01, -4.0251e-01],\n",
            "        [ 1.0000e+00,  5.3571e+00,  4.8798e+00],\n",
            "        [ 1.0000e+00, -1.1618e-01,  1.6425e+00],\n",
            "        [ 1.0000e+00, -8.1224e-01, -9.1720e-01],\n",
            "        [ 1.0000e+00,  4.1055e+00,  3.6020e+00],\n",
            "        [ 1.0000e+00,  4.4271e-01,  9.0350e-01],\n",
            "        [ 1.0000e+00,  4.6020e+00,  5.9155e+00],\n",
            "        [ 1.0000e+00,  7.4982e+00,  4.6660e+00],\n",
            "        [ 1.0000e+00,  6.1091e+00,  5.2590e+00],\n",
            "        [ 1.0000e+00,  2.2406e-01,  1.1518e+00],\n",
            "        [ 1.0000e+00,  5.9282e+00,  7.2224e+00],\n",
            "        [ 1.0000e+00,  5.8352e+00,  5.3810e+00],\n",
            "        [ 1.0000e+00,  9.5861e-01,  1.8157e+00],\n",
            "        [ 1.0000e+00, -5.8646e-01,  3.1682e-01],\n",
            "        [ 1.0000e+00,  5.2644e+00,  4.8049e+00],\n",
            "        [ 1.0000e+00,  7.6176e+00,  3.2531e+00],\n",
            "        [ 1.0000e+00,  5.1505e+00,  4.7579e+00],\n",
            "        [ 1.0000e+00, -2.9684e+00, -6.6102e-01],\n",
            "        [ 1.0000e+00,  4.7890e+00,  4.5545e+00],\n",
            "        [ 1.0000e+00,  5.8457e+00,  6.0655e+00],\n",
            "        [ 1.0000e+00,  5.4540e+00,  5.1332e+00],\n",
            "        [ 1.0000e+00, -7.7012e-01,  1.0438e+00],\n",
            "        [ 1.0000e+00,  4.8429e+00,  4.4716e+00],\n",
            "        [ 1.0000e+00,  5.8811e-01,  1.5704e+00],\n",
            "        [ 1.0000e+00,  2.0238e+00, -5.8017e-01],\n",
            "        [ 1.0000e+00,  2.3622e-01, -4.7674e-01],\n",
            "        [ 1.0000e+00,  5.5069e+00,  4.1988e+00],\n",
            "        [ 1.0000e+00,  5.9371e-01,  1.3654e+00],\n",
            "        [ 1.0000e+00, -5.3371e-01,  1.2299e+00],\n",
            "        [ 1.0000e+00,  6.6705e+00,  4.4488e+00],\n",
            "        [ 1.0000e+00,  4.4770e+00,  4.7734e+00],\n",
            "        [ 1.0000e+00,  8.9933e-01,  1.3853e+00],\n",
            "        [ 1.0000e+00,  5.8499e+00,  4.5494e+00],\n",
            "        [ 1.0000e+00,  2.6826e-01,  1.2427e+00],\n",
            "        [ 1.0000e+00,  3.8316e+00,  5.2639e+00],\n",
            "        [ 1.0000e+00,  4.9118e+00,  5.1990e+00],\n",
            "        [ 1.0000e+00, -3.0232e-01,  2.1879e+00],\n",
            "        [ 1.0000e+00,  4.7950e+00,  5.3181e+00],\n",
            "        [ 1.0000e+00,  6.0062e+00,  4.7300e+00],\n",
            "        [ 1.0000e+00,  5.0134e+00,  5.3765e+00],\n",
            "        [ 1.0000e+00, -8.2010e-01, -5.9022e-01],\n",
            "        [ 1.0000e+00,  6.6749e-01,  2.1314e-01],\n",
            "        [ 1.0000e+00,  3.8099e-01, -9.2113e-01],\n",
            "        [ 1.0000e+00,  3.4421e+00,  5.0328e+00],\n",
            "        [ 1.0000e+00,  5.8647e+00,  5.2100e+00],\n",
            "        [ 1.0000e+00,  1.9355e+00,  7.0833e-01],\n",
            "        [ 1.0000e+00, -5.8202e-02, -2.0874e+00],\n",
            "        [ 1.0000e+00,  5.7096e+00,  4.4615e+00],\n",
            "        [ 1.0000e+00,  4.5182e+00,  4.7263e+00],\n",
            "        [ 1.0000e+00,  4.5264e-01,  7.0474e-01],\n",
            "        [ 1.0000e+00, -1.0486e+00,  8.6537e-01],\n",
            "        [ 1.0000e+00, -2.0869e-01,  1.1816e+00],\n",
            "        [ 1.0000e+00,  4.4274e+00,  6.0093e+00],\n",
            "        [ 1.0000e+00,  5.2331e+00,  5.2023e+00],\n",
            "        [ 1.0000e+00,  1.7897e-01,  9.4336e-01],\n",
            "        [ 1.0000e+00, -1.0250e+00,  1.7978e+00],\n",
            "        [ 1.0000e+00,  4.9423e+00,  6.2911e+00],\n",
            "        [ 1.0000e+00,  8.5773e-01, -8.2915e-01],\n",
            "        [ 1.0000e+00,  4.3726e-01, -1.8891e-01],\n",
            "        [ 1.0000e+00, -8.6294e-01, -2.9511e-01],\n",
            "        [ 1.0000e+00,  3.9096e+00,  4.6044e+00],\n",
            "        [ 1.0000e+00,  2.1949e-01,  3.1798e-01],\n",
            "        [ 1.0000e+00, -4.4269e-01,  1.8195e-01],\n",
            "        [ 1.0000e+00,  2.2987e+00, -3.3949e-01],\n",
            "        [ 1.0000e+00, -6.3849e-01,  3.5269e-02],\n",
            "        [ 1.0000e+00,  1.2853e+00, -1.2025e+00],\n",
            "        [ 1.0000e+00,  1.9892e-01, -5.6628e-01],\n",
            "        [ 1.0000e+00,  4.7039e+00,  2.9914e+00],\n",
            "        [ 1.0000e+00,  4.1090e+00,  4.8918e+00],\n",
            "        [ 1.0000e+00,  6.5221e+00,  6.0951e+00],\n",
            "        [ 1.0000e+00,  5.7384e+00,  5.3190e+00],\n",
            "        [ 1.0000e+00,  1.6613e-01,  8.6443e-01],\n",
            "        [ 1.0000e+00,  4.6569e+00,  3.0806e+00],\n",
            "        [ 1.0000e+00,  4.4015e-01, -8.3694e-01],\n",
            "        [ 1.0000e+00,  5.9431e+00,  5.2853e+00],\n",
            "        [ 1.0000e+00,  1.5573e-01,  3.6995e-01],\n",
            "        [ 1.0000e+00,  4.1557e+00,  6.1958e+00],\n",
            "        [ 1.0000e+00,  1.0696e+00, -1.3932e+00],\n",
            "        [ 1.0000e+00,  9.5766e-01, -8.3822e-01],\n",
            "        [ 1.0000e+00, -3.2566e-02, -1.2579e+00],\n",
            "        [ 1.0000e+00,  4.3859e+00,  4.3761e+00],\n",
            "        [ 1.0000e+00,  2.5043e+00,  9.9949e-01],\n",
            "        [ 1.0000e+00,  5.6921e+00,  6.5547e+00],\n",
            "        [ 1.0000e+00,  2.3686e+00,  4.7815e-01],\n",
            "        [ 1.0000e+00,  5.3433e+00,  4.7394e+00],\n",
            "        [ 1.0000e+00,  4.4023e+00,  4.3417e+00],\n",
            "        [ 1.0000e+00,  4.1984e+00,  5.6937e+00],\n",
            "        [ 1.0000e+00,  4.3955e+00,  5.6783e+00],\n",
            "        [ 1.0000e+00,  5.0813e+00,  4.1149e+00],\n",
            "        [ 1.0000e+00,  2.9012e+00,  1.4865e+00],\n",
            "        [ 1.0000e+00,  5.2165e+00,  4.4330e+00],\n",
            "        [ 1.0000e+00, -9.1267e-02,  6.7826e-01],\n",
            "        [ 1.0000e+00, -1.7834e+00,  7.6585e-01],\n",
            "        [ 1.0000e+00, -1.6294e-01,  8.6243e-01],\n",
            "        [ 1.0000e+00,  5.2718e+00,  7.0376e+00],\n",
            "        [ 1.0000e+00, -1.5695e-01,  2.9872e-01],\n",
            "        [ 1.0000e+00,  4.3692e+00,  5.1367e+00],\n",
            "        [ 1.0000e+00,  4.6267e+00,  7.3106e+00],\n",
            "        [ 1.0000e+00,  5.7934e+00,  5.2184e+00],\n",
            "        [ 1.0000e+00, -1.1063e+00, -8.2837e-01],\n",
            "        [ 1.0000e+00,  3.4170e+00,  3.5598e+00],\n",
            "        [ 1.0000e+00, -8.5025e-01,  1.7747e+00],\n",
            "        [ 1.0000e+00, -9.7429e-01, -4.5843e-01],\n",
            "        [ 1.0000e+00,  5.0892e+00,  5.7515e+00],\n",
            "        [ 1.0000e+00,  5.4444e+00,  6.2717e+00],\n",
            "        [ 1.0000e+00, -1.7588e-01, -2.2881e-01],\n",
            "        [ 1.0000e+00,  3.8426e+00,  3.7992e+00],\n",
            "        [ 1.0000e+00,  6.9724e+00,  4.7985e+00],\n",
            "        [ 1.0000e+00,  8.8797e-01,  6.4933e-01],\n",
            "        [ 1.0000e+00, -8.3902e-01,  1.8089e-01],\n",
            "        [ 1.0000e+00,  4.3088e+00,  3.9392e+00],\n",
            "        [ 1.0000e+00,  4.5379e+00,  4.7652e+00],\n",
            "        [ 1.0000e+00, -2.0333e+00,  1.2838e-01],\n",
            "        [ 1.0000e+00,  5.2072e+00,  4.7987e+00],\n",
            "        [ 1.0000e+00,  1.0852e-01,  3.3407e-01],\n",
            "        [ 1.0000e+00,  5.9138e+00,  5.4372e+00],\n",
            "        [ 1.0000e+00,  4.8193e+00,  5.8841e+00],\n",
            "        [ 1.0000e+00, -1.3446e+00,  5.4391e-03],\n",
            "        [ 1.0000e+00,  5.0331e+00,  3.5981e+00],\n",
            "        [ 1.0000e+00,  4.6154e+00,  4.7487e+00]])\n",
            "torch.Size([200, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBAC18H12LKQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa63dbab-b7fb-4c69-f090-2607a95e6e5d"
      },
      "source": [
        "# w is the parameter to optimize\n",
        "w = torch.normal(mean=0, std=5, size=(X.shape[1],))\n",
        "\n",
        "LEARNING_RATE = 1e-1\n",
        "\n",
        "N_ITERATIONS = 10000\n",
        "\n",
        "prev_loss = torch.FloatTensor([float('inf')])\n",
        "\n",
        "for i in range(N_ITERATIONS):\n",
        "\n",
        "    Y_hat = torch.sigmoid(X @ w)\n",
        "\n",
        "    loss = cross_entropy_loss(Y, Y_hat)\n",
        "    print(f\"Step {i + 1} Loss: \", loss)\n",
        "    if torch.isclose(prev_loss, loss, atol=1e-6):\n",
        "        break\n",
        "    \n",
        "    w = w - LEARNING_RATE * ((Y_hat - Y) @ X) / Y.shape[0]\n",
        "    prev_loss = loss\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Loss:  tensor(2.7093)\n",
            "Step 2 Loss:  tensor(2.6914)\n",
            "Step 3 Loss:  tensor(2.6736)\n",
            "Step 4 Loss:  tensor(2.6554)\n",
            "Step 5 Loss:  tensor(2.6354)\n",
            "Step 6 Loss:  tensor(2.6172)\n",
            "Step 7 Loss:  tensor(2.6008)\n",
            "Step 8 Loss:  tensor(2.5825)\n",
            "Step 9 Loss:  tensor(2.5615)\n",
            "Step 10 Loss:  tensor(2.5447)\n",
            "Step 11 Loss:  tensor(2.5254)\n",
            "Step 12 Loss:  tensor(2.5080)\n",
            "Step 13 Loss:  tensor(2.4906)\n",
            "Step 14 Loss:  tensor(2.4734)\n",
            "Step 15 Loss:  tensor(2.4544)\n",
            "Step 16 Loss:  tensor(2.4359)\n",
            "Step 17 Loss:  tensor(2.4059)\n",
            "Step 18 Loss:  tensor(2.3889)\n",
            "Step 19 Loss:  tensor(2.3706)\n",
            "Step 20 Loss:  tensor(2.3523)\n",
            "Step 21 Loss:  tensor(2.3347)\n",
            "Step 22 Loss:  tensor(2.3176)\n",
            "Step 23 Loss:  tensor(2.2991)\n",
            "Step 24 Loss:  tensor(2.2822)\n",
            "Step 25 Loss:  tensor(2.2640)\n",
            "Step 26 Loss:  tensor(2.2471)\n",
            "Step 27 Loss:  tensor(2.2294)\n",
            "Step 28 Loss:  tensor(2.2120)\n",
            "Step 29 Loss:  tensor(2.1944)\n",
            "Step 30 Loss:  tensor(2.1741)\n",
            "Step 31 Loss:  tensor(2.1572)\n",
            "Step 32 Loss:  tensor(2.1398)\n",
            "Step 33 Loss:  tensor(2.1222)\n",
            "Step 34 Loss:  tensor(2.1052)\n",
            "Step 35 Loss:  tensor(2.0883)\n",
            "Step 36 Loss:  tensor(2.0694)\n",
            "Step 37 Loss:  tensor(2.0523)\n",
            "Step 38 Loss:  tensor(2.0354)\n",
            "Step 39 Loss:  tensor(2.0185)\n",
            "Step 40 Loss:  tensor(2.0004)\n",
            "Step 41 Loss:  tensor(1.9835)\n",
            "Step 42 Loss:  tensor(1.9668)\n",
            "Step 43 Loss:  tensor(1.9491)\n",
            "Step 44 Loss:  tensor(1.9324)\n",
            "Step 45 Loss:  tensor(1.9151)\n",
            "Step 46 Loss:  tensor(1.8985)\n",
            "Step 47 Loss:  tensor(1.8813)\n",
            "Step 48 Loss:  tensor(1.8649)\n",
            "Step 49 Loss:  tensor(1.8479)\n",
            "Step 50 Loss:  tensor(1.8311)\n",
            "Step 51 Loss:  tensor(1.8148)\n",
            "Step 52 Loss:  tensor(1.7981)\n",
            "Step 53 Loss:  tensor(1.7815)\n",
            "Step 54 Loss:  tensor(1.7650)\n",
            "Step 55 Loss:  tensor(1.7486)\n",
            "Step 56 Loss:  tensor(1.7323)\n",
            "Step 57 Loss:  tensor(1.7160)\n",
            "Step 58 Loss:  tensor(1.6996)\n",
            "Step 59 Loss:  tensor(1.6835)\n",
            "Step 60 Loss:  tensor(1.6673)\n",
            "Step 61 Loss:  tensor(1.6511)\n",
            "Step 62 Loss:  tensor(1.6353)\n",
            "Step 63 Loss:  tensor(1.6194)\n",
            "Step 64 Loss:  tensor(1.6033)\n",
            "Step 65 Loss:  tensor(1.5876)\n",
            "Step 66 Loss:  tensor(1.5718)\n",
            "Step 67 Loss:  tensor(1.5561)\n",
            "Step 68 Loss:  tensor(1.5405)\n",
            "Step 69 Loss:  tensor(1.5249)\n",
            "Step 70 Loss:  tensor(1.5094)\n",
            "Step 71 Loss:  tensor(1.4939)\n",
            "Step 72 Loss:  tensor(1.4786)\n",
            "Step 73 Loss:  tensor(1.4634)\n",
            "Step 74 Loss:  tensor(1.4481)\n",
            "Step 75 Loss:  tensor(1.4329)\n",
            "Step 76 Loss:  tensor(1.4178)\n",
            "Step 77 Loss:  tensor(1.4028)\n",
            "Step 78 Loss:  tensor(1.3879)\n",
            "Step 79 Loss:  tensor(1.3730)\n",
            "Step 80 Loss:  tensor(1.3582)\n",
            "Step 81 Loss:  tensor(1.3434)\n",
            "Step 82 Loss:  tensor(1.3288)\n",
            "Step 83 Loss:  tensor(1.3142)\n",
            "Step 84 Loss:  tensor(1.2997)\n",
            "Step 85 Loss:  tensor(1.2852)\n",
            "Step 86 Loss:  tensor(1.2709)\n",
            "Step 87 Loss:  tensor(1.2566)\n",
            "Step 88 Loss:  tensor(1.2424)\n",
            "Step 89 Loss:  tensor(1.2283)\n",
            "Step 90 Loss:  tensor(1.2142)\n",
            "Step 91 Loss:  tensor(1.2002)\n",
            "Step 92 Loss:  tensor(1.1863)\n",
            "Step 93 Loss:  tensor(1.1725)\n",
            "Step 94 Loss:  tensor(1.1588)\n",
            "Step 95 Loss:  tensor(1.1451)\n",
            "Step 96 Loss:  tensor(1.1316)\n",
            "Step 97 Loss:  tensor(1.1181)\n",
            "Step 98 Loss:  tensor(1.1047)\n",
            "Step 99 Loss:  tensor(1.0914)\n",
            "Step 100 Loss:  tensor(1.0781)\n",
            "Step 101 Loss:  tensor(1.0650)\n",
            "Step 102 Loss:  tensor(1.0519)\n",
            "Step 103 Loss:  tensor(1.0389)\n",
            "Step 104 Loss:  tensor(1.0260)\n",
            "Step 105 Loss:  tensor(1.0132)\n",
            "Step 106 Loss:  tensor(1.0005)\n",
            "Step 107 Loss:  tensor(0.9879)\n",
            "Step 108 Loss:  tensor(0.9753)\n",
            "Step 109 Loss:  tensor(0.9629)\n",
            "Step 110 Loss:  tensor(0.9505)\n",
            "Step 111 Loss:  tensor(0.9382)\n",
            "Step 112 Loss:  tensor(0.9260)\n",
            "Step 113 Loss:  tensor(0.9139)\n",
            "Step 114 Loss:  tensor(0.9019)\n",
            "Step 115 Loss:  tensor(0.8900)\n",
            "Step 116 Loss:  tensor(0.8782)\n",
            "Step 117 Loss:  tensor(0.8665)\n",
            "Step 118 Loss:  tensor(0.8548)\n",
            "Step 119 Loss:  tensor(0.8433)\n",
            "Step 120 Loss:  tensor(0.8318)\n",
            "Step 121 Loss:  tensor(0.8205)\n",
            "Step 122 Loss:  tensor(0.8092)\n",
            "Step 123 Loss:  tensor(0.7981)\n",
            "Step 124 Loss:  tensor(0.7870)\n",
            "Step 125 Loss:  tensor(0.7760)\n",
            "Step 126 Loss:  tensor(0.7651)\n",
            "Step 127 Loss:  tensor(0.7544)\n",
            "Step 128 Loss:  tensor(0.7437)\n",
            "Step 129 Loss:  tensor(0.7331)\n",
            "Step 130 Loss:  tensor(0.7226)\n",
            "Step 131 Loss:  tensor(0.7122)\n",
            "Step 132 Loss:  tensor(0.7019)\n",
            "Step 133 Loss:  tensor(0.6917)\n",
            "Step 134 Loss:  tensor(0.6816)\n",
            "Step 135 Loss:  tensor(0.6716)\n",
            "Step 136 Loss:  tensor(0.6617)\n",
            "Step 137 Loss:  tensor(0.6519)\n",
            "Step 138 Loss:  tensor(0.6422)\n",
            "Step 139 Loss:  tensor(0.6326)\n",
            "Step 140 Loss:  tensor(0.6231)\n",
            "Step 141 Loss:  tensor(0.6137)\n",
            "Step 142 Loss:  tensor(0.6044)\n",
            "Step 143 Loss:  tensor(0.5952)\n",
            "Step 144 Loss:  tensor(0.5860)\n",
            "Step 145 Loss:  tensor(0.5770)\n",
            "Step 146 Loss:  tensor(0.5681)\n",
            "Step 147 Loss:  tensor(0.5593)\n",
            "Step 148 Loss:  tensor(0.5506)\n",
            "Step 149 Loss:  tensor(0.5420)\n",
            "Step 150 Loss:  tensor(0.5335)\n",
            "Step 151 Loss:  tensor(0.5251)\n",
            "Step 152 Loss:  tensor(0.5168)\n",
            "Step 153 Loss:  tensor(0.5086)\n",
            "Step 154 Loss:  tensor(0.5005)\n",
            "Step 155 Loss:  tensor(0.4925)\n",
            "Step 156 Loss:  tensor(0.4846)\n",
            "Step 157 Loss:  tensor(0.4768)\n",
            "Step 158 Loss:  tensor(0.4691)\n",
            "Step 159 Loss:  tensor(0.4614)\n",
            "Step 160 Loss:  tensor(0.4539)\n",
            "Step 161 Loss:  tensor(0.4465)\n",
            "Step 162 Loss:  tensor(0.4392)\n",
            "Step 163 Loss:  tensor(0.4320)\n",
            "Step 164 Loss:  tensor(0.4249)\n",
            "Step 165 Loss:  tensor(0.4179)\n",
            "Step 166 Loss:  tensor(0.4110)\n",
            "Step 167 Loss:  tensor(0.4042)\n",
            "Step 168 Loss:  tensor(0.3975)\n",
            "Step 169 Loss:  tensor(0.3909)\n",
            "Step 170 Loss:  tensor(0.3843)\n",
            "Step 171 Loss:  tensor(0.3779)\n",
            "Step 172 Loss:  tensor(0.3716)\n",
            "Step 173 Loss:  tensor(0.3654)\n",
            "Step 174 Loss:  tensor(0.3592)\n",
            "Step 175 Loss:  tensor(0.3532)\n",
            "Step 176 Loss:  tensor(0.3473)\n",
            "Step 177 Loss:  tensor(0.3414)\n",
            "Step 178 Loss:  tensor(0.3356)\n",
            "Step 179 Loss:  tensor(0.3300)\n",
            "Step 180 Loss:  tensor(0.3244)\n",
            "Step 181 Loss:  tensor(0.3189)\n",
            "Step 182 Loss:  tensor(0.3136)\n",
            "Step 183 Loss:  tensor(0.3083)\n",
            "Step 184 Loss:  tensor(0.3031)\n",
            "Step 185 Loss:  tensor(0.2980)\n",
            "Step 186 Loss:  tensor(0.2929)\n",
            "Step 187 Loss:  tensor(0.2880)\n",
            "Step 188 Loss:  tensor(0.2832)\n",
            "Step 189 Loss:  tensor(0.2784)\n",
            "Step 190 Loss:  tensor(0.2737)\n",
            "Step 191 Loss:  tensor(0.2692)\n",
            "Step 192 Loss:  tensor(0.2647)\n",
            "Step 193 Loss:  tensor(0.2602)\n",
            "Step 194 Loss:  tensor(0.2559)\n",
            "Step 195 Loss:  tensor(0.2516)\n",
            "Step 196 Loss:  tensor(0.2475)\n",
            "Step 197 Loss:  tensor(0.2434)\n",
            "Step 198 Loss:  tensor(0.2394)\n",
            "Step 199 Loss:  tensor(0.2354)\n",
            "Step 200 Loss:  tensor(0.2316)\n",
            "Step 201 Loss:  tensor(0.2278)\n",
            "Step 202 Loss:  tensor(0.2241)\n",
            "Step 203 Loss:  tensor(0.2205)\n",
            "Step 204 Loss:  tensor(0.2169)\n",
            "Step 205 Loss:  tensor(0.2135)\n",
            "Step 206 Loss:  tensor(0.2101)\n",
            "Step 207 Loss:  tensor(0.2067)\n",
            "Step 208 Loss:  tensor(0.2034)\n",
            "Step 209 Loss:  tensor(0.2003)\n",
            "Step 210 Loss:  tensor(0.1971)\n",
            "Step 211 Loss:  tensor(0.1941)\n",
            "Step 212 Loss:  tensor(0.1911)\n",
            "Step 213 Loss:  tensor(0.1881)\n",
            "Step 214 Loss:  tensor(0.1853)\n",
            "Step 215 Loss:  tensor(0.1824)\n",
            "Step 216 Loss:  tensor(0.1797)\n",
            "Step 217 Loss:  tensor(0.1770)\n",
            "Step 218 Loss:  tensor(0.1744)\n",
            "Step 219 Loss:  tensor(0.1718)\n",
            "Step 220 Loss:  tensor(0.1693)\n",
            "Step 221 Loss:  tensor(0.1669)\n",
            "Step 222 Loss:  tensor(0.1645)\n",
            "Step 223 Loss:  tensor(0.1621)\n",
            "Step 224 Loss:  tensor(0.1598)\n",
            "Step 225 Loss:  tensor(0.1576)\n",
            "Step 226 Loss:  tensor(0.1554)\n",
            "Step 227 Loss:  tensor(0.1532)\n",
            "Step 228 Loss:  tensor(0.1511)\n",
            "Step 229 Loss:  tensor(0.1491)\n",
            "Step 230 Loss:  tensor(0.1471)\n",
            "Step 231 Loss:  tensor(0.1451)\n",
            "Step 232 Loss:  tensor(0.1432)\n",
            "Step 233 Loss:  tensor(0.1413)\n",
            "Step 234 Loss:  tensor(0.1395)\n",
            "Step 235 Loss:  tensor(0.1377)\n",
            "Step 236 Loss:  tensor(0.1360)\n",
            "Step 237 Loss:  tensor(0.1343)\n",
            "Step 238 Loss:  tensor(0.1326)\n",
            "Step 239 Loss:  tensor(0.1310)\n",
            "Step 240 Loss:  tensor(0.1294)\n",
            "Step 241 Loss:  tensor(0.1278)\n",
            "Step 242 Loss:  tensor(0.1263)\n",
            "Step 243 Loss:  tensor(0.1248)\n",
            "Step 244 Loss:  tensor(0.1234)\n",
            "Step 245 Loss:  tensor(0.1219)\n",
            "Step 246 Loss:  tensor(0.1205)\n",
            "Step 247 Loss:  tensor(0.1192)\n",
            "Step 248 Loss:  tensor(0.1178)\n",
            "Step 249 Loss:  tensor(0.1165)\n",
            "Step 250 Loss:  tensor(0.1152)\n",
            "Step 251 Loss:  tensor(0.1140)\n",
            "Step 252 Loss:  tensor(0.1128)\n",
            "Step 253 Loss:  tensor(0.1116)\n",
            "Step 254 Loss:  tensor(0.1104)\n",
            "Step 255 Loss:  tensor(0.1092)\n",
            "Step 256 Loss:  tensor(0.1081)\n",
            "Step 257 Loss:  tensor(0.1070)\n",
            "Step 258 Loss:  tensor(0.1059)\n",
            "Step 259 Loss:  tensor(0.1049)\n",
            "Step 260 Loss:  tensor(0.1038)\n",
            "Step 261 Loss:  tensor(0.1028)\n",
            "Step 262 Loss:  tensor(0.1018)\n",
            "Step 263 Loss:  tensor(0.1009)\n",
            "Step 264 Loss:  tensor(0.0999)\n",
            "Step 265 Loss:  tensor(0.0990)\n",
            "Step 266 Loss:  tensor(0.0980)\n",
            "Step 267 Loss:  tensor(0.0971)\n",
            "Step 268 Loss:  tensor(0.0963)\n",
            "Step 269 Loss:  tensor(0.0954)\n",
            "Step 270 Loss:  tensor(0.0945)\n",
            "Step 271 Loss:  tensor(0.0937)\n",
            "Step 272 Loss:  tensor(0.0929)\n",
            "Step 273 Loss:  tensor(0.0921)\n",
            "Step 274 Loss:  tensor(0.0913)\n",
            "Step 275 Loss:  tensor(0.0905)\n",
            "Step 276 Loss:  tensor(0.0897)\n",
            "Step 277 Loss:  tensor(0.0890)\n",
            "Step 278 Loss:  tensor(0.0883)\n",
            "Step 279 Loss:  tensor(0.0875)\n",
            "Step 280 Loss:  tensor(0.0868)\n",
            "Step 281 Loss:  tensor(0.0861)\n",
            "Step 282 Loss:  tensor(0.0854)\n",
            "Step 283 Loss:  tensor(0.0848)\n",
            "Step 284 Loss:  tensor(0.0841)\n",
            "Step 285 Loss:  tensor(0.0835)\n",
            "Step 286 Loss:  tensor(0.0828)\n",
            "Step 287 Loss:  tensor(0.0822)\n",
            "Step 288 Loss:  tensor(0.0816)\n",
            "Step 289 Loss:  tensor(0.0810)\n",
            "Step 290 Loss:  tensor(0.0804)\n",
            "Step 291 Loss:  tensor(0.0798)\n",
            "Step 292 Loss:  tensor(0.0792)\n",
            "Step 293 Loss:  tensor(0.0786)\n",
            "Step 294 Loss:  tensor(0.0781)\n",
            "Step 295 Loss:  tensor(0.0775)\n",
            "Step 296 Loss:  tensor(0.0770)\n",
            "Step 297 Loss:  tensor(0.0764)\n",
            "Step 298 Loss:  tensor(0.0759)\n",
            "Step 299 Loss:  tensor(0.0754)\n",
            "Step 300 Loss:  tensor(0.0749)\n",
            "Step 301 Loss:  tensor(0.0744)\n",
            "Step 302 Loss:  tensor(0.0739)\n",
            "Step 303 Loss:  tensor(0.0734)\n",
            "Step 304 Loss:  tensor(0.0729)\n",
            "Step 305 Loss:  tensor(0.0725)\n",
            "Step 306 Loss:  tensor(0.0720)\n",
            "Step 307 Loss:  tensor(0.0715)\n",
            "Step 308 Loss:  tensor(0.0711)\n",
            "Step 309 Loss:  tensor(0.0706)\n",
            "Step 310 Loss:  tensor(0.0702)\n",
            "Step 311 Loss:  tensor(0.0698)\n",
            "Step 312 Loss:  tensor(0.0693)\n",
            "Step 313 Loss:  tensor(0.0689)\n",
            "Step 314 Loss:  tensor(0.0685)\n",
            "Step 315 Loss:  tensor(0.0681)\n",
            "Step 316 Loss:  tensor(0.0677)\n",
            "Step 317 Loss:  tensor(0.0673)\n",
            "Step 318 Loss:  tensor(0.0669)\n",
            "Step 319 Loss:  tensor(0.0665)\n",
            "Step 320 Loss:  tensor(0.0661)\n",
            "Step 321 Loss:  tensor(0.0658)\n",
            "Step 322 Loss:  tensor(0.0654)\n",
            "Step 323 Loss:  tensor(0.0650)\n",
            "Step 324 Loss:  tensor(0.0647)\n",
            "Step 325 Loss:  tensor(0.0643)\n",
            "Step 326 Loss:  tensor(0.0640)\n",
            "Step 327 Loss:  tensor(0.0636)\n",
            "Step 328 Loss:  tensor(0.0633)\n",
            "Step 329 Loss:  tensor(0.0629)\n",
            "Step 330 Loss:  tensor(0.0626)\n",
            "Step 331 Loss:  tensor(0.0623)\n",
            "Step 332 Loss:  tensor(0.0619)\n",
            "Step 333 Loss:  tensor(0.0616)\n",
            "Step 334 Loss:  tensor(0.0613)\n",
            "Step 335 Loss:  tensor(0.0610)\n",
            "Step 336 Loss:  tensor(0.0607)\n",
            "Step 337 Loss:  tensor(0.0604)\n",
            "Step 338 Loss:  tensor(0.0601)\n",
            "Step 339 Loss:  tensor(0.0598)\n",
            "Step 340 Loss:  tensor(0.0595)\n",
            "Step 341 Loss:  tensor(0.0592)\n",
            "Step 342 Loss:  tensor(0.0589)\n",
            "Step 343 Loss:  tensor(0.0586)\n",
            "Step 344 Loss:  tensor(0.0583)\n",
            "Step 345 Loss:  tensor(0.0581)\n",
            "Step 346 Loss:  tensor(0.0578)\n",
            "Step 347 Loss:  tensor(0.0575)\n",
            "Step 348 Loss:  tensor(0.0573)\n",
            "Step 349 Loss:  tensor(0.0570)\n",
            "Step 350 Loss:  tensor(0.0567)\n",
            "Step 351 Loss:  tensor(0.0565)\n",
            "Step 352 Loss:  tensor(0.0562)\n",
            "Step 353 Loss:  tensor(0.0560)\n",
            "Step 354 Loss:  tensor(0.0557)\n",
            "Step 355 Loss:  tensor(0.0555)\n",
            "Step 356 Loss:  tensor(0.0552)\n",
            "Step 357 Loss:  tensor(0.0550)\n",
            "Step 358 Loss:  tensor(0.0547)\n",
            "Step 359 Loss:  tensor(0.0545)\n",
            "Step 360 Loss:  tensor(0.0543)\n",
            "Step 361 Loss:  tensor(0.0540)\n",
            "Step 362 Loss:  tensor(0.0538)\n",
            "Step 363 Loss:  tensor(0.0536)\n",
            "Step 364 Loss:  tensor(0.0534)\n",
            "Step 365 Loss:  tensor(0.0532)\n",
            "Step 366 Loss:  tensor(0.0529)\n",
            "Step 367 Loss:  tensor(0.0527)\n",
            "Step 368 Loss:  tensor(0.0525)\n",
            "Step 369 Loss:  tensor(0.0523)\n",
            "Step 370 Loss:  tensor(0.0521)\n",
            "Step 371 Loss:  tensor(0.0519)\n",
            "Step 372 Loss:  tensor(0.0517)\n",
            "Step 373 Loss:  tensor(0.0515)\n",
            "Step 374 Loss:  tensor(0.0513)\n",
            "Step 375 Loss:  tensor(0.0511)\n",
            "Step 376 Loss:  tensor(0.0509)\n",
            "Step 377 Loss:  tensor(0.0507)\n",
            "Step 378 Loss:  tensor(0.0505)\n",
            "Step 379 Loss:  tensor(0.0503)\n",
            "Step 380 Loss:  tensor(0.0501)\n",
            "Step 381 Loss:  tensor(0.0499)\n",
            "Step 382 Loss:  tensor(0.0497)\n",
            "Step 383 Loss:  tensor(0.0496)\n",
            "Step 384 Loss:  tensor(0.0494)\n",
            "Step 385 Loss:  tensor(0.0492)\n",
            "Step 386 Loss:  tensor(0.0490)\n",
            "Step 387 Loss:  tensor(0.0488)\n",
            "Step 388 Loss:  tensor(0.0487)\n",
            "Step 389 Loss:  tensor(0.0485)\n",
            "Step 390 Loss:  tensor(0.0483)\n",
            "Step 391 Loss:  tensor(0.0482)\n",
            "Step 392 Loss:  tensor(0.0480)\n",
            "Step 393 Loss:  tensor(0.0478)\n",
            "Step 394 Loss:  tensor(0.0477)\n",
            "Step 395 Loss:  tensor(0.0475)\n",
            "Step 396 Loss:  tensor(0.0473)\n",
            "Step 397 Loss:  tensor(0.0472)\n",
            "Step 398 Loss:  tensor(0.0470)\n",
            "Step 399 Loss:  tensor(0.0469)\n",
            "Step 400 Loss:  tensor(0.0467)\n",
            "Step 401 Loss:  tensor(0.0466)\n",
            "Step 402 Loss:  tensor(0.0464)\n",
            "Step 403 Loss:  tensor(0.0462)\n",
            "Step 404 Loss:  tensor(0.0461)\n",
            "Step 405 Loss:  tensor(0.0460)\n",
            "Step 406 Loss:  tensor(0.0458)\n",
            "Step 407 Loss:  tensor(0.0457)\n",
            "Step 408 Loss:  tensor(0.0455)\n",
            "Step 409 Loss:  tensor(0.0454)\n",
            "Step 410 Loss:  tensor(0.0452)\n",
            "Step 411 Loss:  tensor(0.0451)\n",
            "Step 412 Loss:  tensor(0.0449)\n",
            "Step 413 Loss:  tensor(0.0448)\n",
            "Step 414 Loss:  tensor(0.0447)\n",
            "Step 415 Loss:  tensor(0.0445)\n",
            "Step 416 Loss:  tensor(0.0444)\n",
            "Step 417 Loss:  tensor(0.0443)\n",
            "Step 418 Loss:  tensor(0.0441)\n",
            "Step 419 Loss:  tensor(0.0440)\n",
            "Step 420 Loss:  tensor(0.0439)\n",
            "Step 421 Loss:  tensor(0.0437)\n",
            "Step 422 Loss:  tensor(0.0436)\n",
            "Step 423 Loss:  tensor(0.0435)\n",
            "Step 424 Loss:  tensor(0.0434)\n",
            "Step 425 Loss:  tensor(0.0432)\n",
            "Step 426 Loss:  tensor(0.0431)\n",
            "Step 427 Loss:  tensor(0.0430)\n",
            "Step 428 Loss:  tensor(0.0429)\n",
            "Step 429 Loss:  tensor(0.0427)\n",
            "Step 430 Loss:  tensor(0.0426)\n",
            "Step 431 Loss:  tensor(0.0425)\n",
            "Step 432 Loss:  tensor(0.0424)\n",
            "Step 433 Loss:  tensor(0.0423)\n",
            "Step 434 Loss:  tensor(0.0422)\n",
            "Step 435 Loss:  tensor(0.0420)\n",
            "Step 436 Loss:  tensor(0.0419)\n",
            "Step 437 Loss:  tensor(0.0418)\n",
            "Step 438 Loss:  tensor(0.0417)\n",
            "Step 439 Loss:  tensor(0.0416)\n",
            "Step 440 Loss:  tensor(0.0415)\n",
            "Step 441 Loss:  tensor(0.0414)\n",
            "Step 442 Loss:  tensor(0.0413)\n",
            "Step 443 Loss:  tensor(0.0411)\n",
            "Step 444 Loss:  tensor(0.0410)\n",
            "Step 445 Loss:  tensor(0.0409)\n",
            "Step 446 Loss:  tensor(0.0408)\n",
            "Step 447 Loss:  tensor(0.0407)\n",
            "Step 448 Loss:  tensor(0.0406)\n",
            "Step 449 Loss:  tensor(0.0405)\n",
            "Step 450 Loss:  tensor(0.0404)\n",
            "Step 451 Loss:  tensor(0.0403)\n",
            "Step 452 Loss:  tensor(0.0402)\n",
            "Step 453 Loss:  tensor(0.0401)\n",
            "Step 454 Loss:  tensor(0.0400)\n",
            "Step 455 Loss:  tensor(0.0399)\n",
            "Step 456 Loss:  tensor(0.0398)\n",
            "Step 457 Loss:  tensor(0.0397)\n",
            "Step 458 Loss:  tensor(0.0396)\n",
            "Step 459 Loss:  tensor(0.0395)\n",
            "Step 460 Loss:  tensor(0.0394)\n",
            "Step 461 Loss:  tensor(0.0393)\n",
            "Step 462 Loss:  tensor(0.0392)\n",
            "Step 463 Loss:  tensor(0.0391)\n",
            "Step 464 Loss:  tensor(0.0390)\n",
            "Step 465 Loss:  tensor(0.0390)\n",
            "Step 466 Loss:  tensor(0.0389)\n",
            "Step 467 Loss:  tensor(0.0388)\n",
            "Step 468 Loss:  tensor(0.0387)\n",
            "Step 469 Loss:  tensor(0.0386)\n",
            "Step 470 Loss:  tensor(0.0385)\n",
            "Step 471 Loss:  tensor(0.0384)\n",
            "Step 472 Loss:  tensor(0.0383)\n",
            "Step 473 Loss:  tensor(0.0382)\n",
            "Step 474 Loss:  tensor(0.0381)\n",
            "Step 475 Loss:  tensor(0.0381)\n",
            "Step 476 Loss:  tensor(0.0380)\n",
            "Step 477 Loss:  tensor(0.0379)\n",
            "Step 478 Loss:  tensor(0.0378)\n",
            "Step 479 Loss:  tensor(0.0377)\n",
            "Step 480 Loss:  tensor(0.0376)\n",
            "Step 481 Loss:  tensor(0.0376)\n",
            "Step 482 Loss:  tensor(0.0375)\n",
            "Step 483 Loss:  tensor(0.0374)\n",
            "Step 484 Loss:  tensor(0.0373)\n",
            "Step 485 Loss:  tensor(0.0372)\n",
            "Step 486 Loss:  tensor(0.0371)\n",
            "Step 487 Loss:  tensor(0.0371)\n",
            "Step 488 Loss:  tensor(0.0370)\n",
            "Step 489 Loss:  tensor(0.0369)\n",
            "Step 490 Loss:  tensor(0.0368)\n",
            "Step 491 Loss:  tensor(0.0368)\n",
            "Step 492 Loss:  tensor(0.0367)\n",
            "Step 493 Loss:  tensor(0.0366)\n",
            "Step 494 Loss:  tensor(0.0365)\n",
            "Step 495 Loss:  tensor(0.0364)\n",
            "Step 496 Loss:  tensor(0.0364)\n",
            "Step 497 Loss:  tensor(0.0363)\n",
            "Step 498 Loss:  tensor(0.0362)\n",
            "Step 499 Loss:  tensor(0.0361)\n",
            "Step 500 Loss:  tensor(0.0361)\n",
            "Step 501 Loss:  tensor(0.0360)\n",
            "Step 502 Loss:  tensor(0.0359)\n",
            "Step 503 Loss:  tensor(0.0359)\n",
            "Step 504 Loss:  tensor(0.0358)\n",
            "Step 505 Loss:  tensor(0.0357)\n",
            "Step 506 Loss:  tensor(0.0356)\n",
            "Step 507 Loss:  tensor(0.0356)\n",
            "Step 508 Loss:  tensor(0.0355)\n",
            "Step 509 Loss:  tensor(0.0354)\n",
            "Step 510 Loss:  tensor(0.0354)\n",
            "Step 511 Loss:  tensor(0.0353)\n",
            "Step 512 Loss:  tensor(0.0352)\n",
            "Step 513 Loss:  tensor(0.0352)\n",
            "Step 514 Loss:  tensor(0.0351)\n",
            "Step 515 Loss:  tensor(0.0350)\n",
            "Step 516 Loss:  tensor(0.0350)\n",
            "Step 517 Loss:  tensor(0.0349)\n",
            "Step 518 Loss:  tensor(0.0348)\n",
            "Step 519 Loss:  tensor(0.0348)\n",
            "Step 520 Loss:  tensor(0.0347)\n",
            "Step 521 Loss:  tensor(0.0346)\n",
            "Step 522 Loss:  tensor(0.0346)\n",
            "Step 523 Loss:  tensor(0.0345)\n",
            "Step 524 Loss:  tensor(0.0344)\n",
            "Step 525 Loss:  tensor(0.0344)\n",
            "Step 526 Loss:  tensor(0.0343)\n",
            "Step 527 Loss:  tensor(0.0342)\n",
            "Step 528 Loss:  tensor(0.0342)\n",
            "Step 529 Loss:  tensor(0.0341)\n",
            "Step 530 Loss:  tensor(0.0341)\n",
            "Step 531 Loss:  tensor(0.0340)\n",
            "Step 532 Loss:  tensor(0.0339)\n",
            "Step 533 Loss:  tensor(0.0339)\n",
            "Step 534 Loss:  tensor(0.0338)\n",
            "Step 535 Loss:  tensor(0.0338)\n",
            "Step 536 Loss:  tensor(0.0337)\n",
            "Step 537 Loss:  tensor(0.0336)\n",
            "Step 538 Loss:  tensor(0.0336)\n",
            "Step 539 Loss:  tensor(0.0335)\n",
            "Step 540 Loss:  tensor(0.0335)\n",
            "Step 541 Loss:  tensor(0.0334)\n",
            "Step 542 Loss:  tensor(0.0333)\n",
            "Step 543 Loss:  tensor(0.0333)\n",
            "Step 544 Loss:  tensor(0.0332)\n",
            "Step 545 Loss:  tensor(0.0332)\n",
            "Step 546 Loss:  tensor(0.0331)\n",
            "Step 547 Loss:  tensor(0.0331)\n",
            "Step 548 Loss:  tensor(0.0330)\n",
            "Step 549 Loss:  tensor(0.0329)\n",
            "Step 550 Loss:  tensor(0.0329)\n",
            "Step 551 Loss:  tensor(0.0328)\n",
            "Step 552 Loss:  tensor(0.0328)\n",
            "Step 553 Loss:  tensor(0.0327)\n",
            "Step 554 Loss:  tensor(0.0327)\n",
            "Step 555 Loss:  tensor(0.0326)\n",
            "Step 556 Loss:  tensor(0.0326)\n",
            "Step 557 Loss:  tensor(0.0325)\n",
            "Step 558 Loss:  tensor(0.0325)\n",
            "Step 559 Loss:  tensor(0.0324)\n",
            "Step 560 Loss:  tensor(0.0323)\n",
            "Step 561 Loss:  tensor(0.0323)\n",
            "Step 562 Loss:  tensor(0.0322)\n",
            "Step 563 Loss:  tensor(0.0322)\n",
            "Step 564 Loss:  tensor(0.0321)\n",
            "Step 565 Loss:  tensor(0.0321)\n",
            "Step 566 Loss:  tensor(0.0320)\n",
            "Step 567 Loss:  tensor(0.0320)\n",
            "Step 568 Loss:  tensor(0.0319)\n",
            "Step 569 Loss:  tensor(0.0319)\n",
            "Step 570 Loss:  tensor(0.0318)\n",
            "Step 571 Loss:  tensor(0.0318)\n",
            "Step 572 Loss:  tensor(0.0317)\n",
            "Step 573 Loss:  tensor(0.0317)\n",
            "Step 574 Loss:  tensor(0.0316)\n",
            "Step 575 Loss:  tensor(0.0316)\n",
            "Step 576 Loss:  tensor(0.0315)\n",
            "Step 577 Loss:  tensor(0.0315)\n",
            "Step 578 Loss:  tensor(0.0314)\n",
            "Step 579 Loss:  tensor(0.0314)\n",
            "Step 580 Loss:  tensor(0.0313)\n",
            "Step 581 Loss:  tensor(0.0313)\n",
            "Step 582 Loss:  tensor(0.0312)\n",
            "Step 583 Loss:  tensor(0.0312)\n",
            "Step 584 Loss:  tensor(0.0312)\n",
            "Step 585 Loss:  tensor(0.0311)\n",
            "Step 586 Loss:  tensor(0.0311)\n",
            "Step 587 Loss:  tensor(0.0310)\n",
            "Step 588 Loss:  tensor(0.0310)\n",
            "Step 589 Loss:  tensor(0.0309)\n",
            "Step 590 Loss:  tensor(0.0309)\n",
            "Step 591 Loss:  tensor(0.0308)\n",
            "Step 592 Loss:  tensor(0.0308)\n",
            "Step 593 Loss:  tensor(0.0307)\n",
            "Step 594 Loss:  tensor(0.0307)\n",
            "Step 595 Loss:  tensor(0.0306)\n",
            "Step 596 Loss:  tensor(0.0306)\n",
            "Step 597 Loss:  tensor(0.0306)\n",
            "Step 598 Loss:  tensor(0.0305)\n",
            "Step 599 Loss:  tensor(0.0305)\n",
            "Step 600 Loss:  tensor(0.0304)\n",
            "Step 601 Loss:  tensor(0.0304)\n",
            "Step 602 Loss:  tensor(0.0303)\n",
            "Step 603 Loss:  tensor(0.0303)\n",
            "Step 604 Loss:  tensor(0.0303)\n",
            "Step 605 Loss:  tensor(0.0302)\n",
            "Step 606 Loss:  tensor(0.0302)\n",
            "Step 607 Loss:  tensor(0.0301)\n",
            "Step 608 Loss:  tensor(0.0301)\n",
            "Step 609 Loss:  tensor(0.0300)\n",
            "Step 610 Loss:  tensor(0.0300)\n",
            "Step 611 Loss:  tensor(0.0300)\n",
            "Step 612 Loss:  tensor(0.0299)\n",
            "Step 613 Loss:  tensor(0.0299)\n",
            "Step 614 Loss:  tensor(0.0298)\n",
            "Step 615 Loss:  tensor(0.0298)\n",
            "Step 616 Loss:  tensor(0.0298)\n",
            "Step 617 Loss:  tensor(0.0297)\n",
            "Step 618 Loss:  tensor(0.0297)\n",
            "Step 619 Loss:  tensor(0.0296)\n",
            "Step 620 Loss:  tensor(0.0296)\n",
            "Step 621 Loss:  tensor(0.0295)\n",
            "Step 622 Loss:  tensor(0.0295)\n",
            "Step 623 Loss:  tensor(0.0295)\n",
            "Step 624 Loss:  tensor(0.0294)\n",
            "Step 625 Loss:  tensor(0.0294)\n",
            "Step 626 Loss:  tensor(0.0294)\n",
            "Step 627 Loss:  tensor(0.0293)\n",
            "Step 628 Loss:  tensor(0.0293)\n",
            "Step 629 Loss:  tensor(0.0292)\n",
            "Step 630 Loss:  tensor(0.0292)\n",
            "Step 631 Loss:  tensor(0.0292)\n",
            "Step 632 Loss:  tensor(0.0291)\n",
            "Step 633 Loss:  tensor(0.0291)\n",
            "Step 634 Loss:  tensor(0.0290)\n",
            "Step 635 Loss:  tensor(0.0290)\n",
            "Step 636 Loss:  tensor(0.0290)\n",
            "Step 637 Loss:  tensor(0.0289)\n",
            "Step 638 Loss:  tensor(0.0289)\n",
            "Step 639 Loss:  tensor(0.0289)\n",
            "Step 640 Loss:  tensor(0.0288)\n",
            "Step 641 Loss:  tensor(0.0288)\n",
            "Step 642 Loss:  tensor(0.0287)\n",
            "Step 643 Loss:  tensor(0.0287)\n",
            "Step 644 Loss:  tensor(0.0287)\n",
            "Step 645 Loss:  tensor(0.0286)\n",
            "Step 646 Loss:  tensor(0.0286)\n",
            "Step 647 Loss:  tensor(0.0286)\n",
            "Step 648 Loss:  tensor(0.0285)\n",
            "Step 649 Loss:  tensor(0.0285)\n",
            "Step 650 Loss:  tensor(0.0285)\n",
            "Step 651 Loss:  tensor(0.0284)\n",
            "Step 652 Loss:  tensor(0.0284)\n",
            "Step 653 Loss:  tensor(0.0283)\n",
            "Step 654 Loss:  tensor(0.0283)\n",
            "Step 655 Loss:  tensor(0.0283)\n",
            "Step 656 Loss:  tensor(0.0282)\n",
            "Step 657 Loss:  tensor(0.0282)\n",
            "Step 658 Loss:  tensor(0.0282)\n",
            "Step 659 Loss:  tensor(0.0281)\n",
            "Step 660 Loss:  tensor(0.0281)\n",
            "Step 661 Loss:  tensor(0.0281)\n",
            "Step 662 Loss:  tensor(0.0280)\n",
            "Step 663 Loss:  tensor(0.0280)\n",
            "Step 664 Loss:  tensor(0.0280)\n",
            "Step 665 Loss:  tensor(0.0279)\n",
            "Step 666 Loss:  tensor(0.0279)\n",
            "Step 667 Loss:  tensor(0.0279)\n",
            "Step 668 Loss:  tensor(0.0278)\n",
            "Step 669 Loss:  tensor(0.0278)\n",
            "Step 670 Loss:  tensor(0.0278)\n",
            "Step 671 Loss:  tensor(0.0277)\n",
            "Step 672 Loss:  tensor(0.0277)\n",
            "Step 673 Loss:  tensor(0.0277)\n",
            "Step 674 Loss:  tensor(0.0276)\n",
            "Step 675 Loss:  tensor(0.0276)\n",
            "Step 676 Loss:  tensor(0.0276)\n",
            "Step 677 Loss:  tensor(0.0275)\n",
            "Step 678 Loss:  tensor(0.0275)\n",
            "Step 679 Loss:  tensor(0.0275)\n",
            "Step 680 Loss:  tensor(0.0274)\n",
            "Step 681 Loss:  tensor(0.0274)\n",
            "Step 682 Loss:  tensor(0.0274)\n",
            "Step 683 Loss:  tensor(0.0273)\n",
            "Step 684 Loss:  tensor(0.0273)\n",
            "Step 685 Loss:  tensor(0.0273)\n",
            "Step 686 Loss:  tensor(0.0273)\n",
            "Step 687 Loss:  tensor(0.0272)\n",
            "Step 688 Loss:  tensor(0.0272)\n",
            "Step 689 Loss:  tensor(0.0272)\n",
            "Step 690 Loss:  tensor(0.0271)\n",
            "Step 691 Loss:  tensor(0.0271)\n",
            "Step 692 Loss:  tensor(0.0271)\n",
            "Step 693 Loss:  tensor(0.0270)\n",
            "Step 694 Loss:  tensor(0.0270)\n",
            "Step 695 Loss:  tensor(0.0270)\n",
            "Step 696 Loss:  tensor(0.0269)\n",
            "Step 697 Loss:  tensor(0.0269)\n",
            "Step 698 Loss:  tensor(0.0269)\n",
            "Step 699 Loss:  tensor(0.0269)\n",
            "Step 700 Loss:  tensor(0.0268)\n",
            "Step 701 Loss:  tensor(0.0268)\n",
            "Step 702 Loss:  tensor(0.0268)\n",
            "Step 703 Loss:  tensor(0.0267)\n",
            "Step 704 Loss:  tensor(0.0267)\n",
            "Step 705 Loss:  tensor(0.0267)\n",
            "Step 706 Loss:  tensor(0.0266)\n",
            "Step 707 Loss:  tensor(0.0266)\n",
            "Step 708 Loss:  tensor(0.0266)\n",
            "Step 709 Loss:  tensor(0.0266)\n",
            "Step 710 Loss:  tensor(0.0265)\n",
            "Step 711 Loss:  tensor(0.0265)\n",
            "Step 712 Loss:  tensor(0.0265)\n",
            "Step 713 Loss:  tensor(0.0264)\n",
            "Step 714 Loss:  tensor(0.0264)\n",
            "Step 715 Loss:  tensor(0.0264)\n",
            "Step 716 Loss:  tensor(0.0264)\n",
            "Step 717 Loss:  tensor(0.0263)\n",
            "Step 718 Loss:  tensor(0.0263)\n",
            "Step 719 Loss:  tensor(0.0263)\n",
            "Step 720 Loss:  tensor(0.0262)\n",
            "Step 721 Loss:  tensor(0.0262)\n",
            "Step 722 Loss:  tensor(0.0262)\n",
            "Step 723 Loss:  tensor(0.0262)\n",
            "Step 724 Loss:  tensor(0.0261)\n",
            "Step 725 Loss:  tensor(0.0261)\n",
            "Step 726 Loss:  tensor(0.0261)\n",
            "Step 727 Loss:  tensor(0.0261)\n",
            "Step 728 Loss:  tensor(0.0260)\n",
            "Step 729 Loss:  tensor(0.0260)\n",
            "Step 730 Loss:  tensor(0.0260)\n",
            "Step 731 Loss:  tensor(0.0259)\n",
            "Step 732 Loss:  tensor(0.0259)\n",
            "Step 733 Loss:  tensor(0.0259)\n",
            "Step 734 Loss:  tensor(0.0259)\n",
            "Step 735 Loss:  tensor(0.0258)\n",
            "Step 736 Loss:  tensor(0.0258)\n",
            "Step 737 Loss:  tensor(0.0258)\n",
            "Step 738 Loss:  tensor(0.0258)\n",
            "Step 739 Loss:  tensor(0.0257)\n",
            "Step 740 Loss:  tensor(0.0257)\n",
            "Step 741 Loss:  tensor(0.0257)\n",
            "Step 742 Loss:  tensor(0.0257)\n",
            "Step 743 Loss:  tensor(0.0256)\n",
            "Step 744 Loss:  tensor(0.0256)\n",
            "Step 745 Loss:  tensor(0.0256)\n",
            "Step 746 Loss:  tensor(0.0256)\n",
            "Step 747 Loss:  tensor(0.0255)\n",
            "Step 748 Loss:  tensor(0.0255)\n",
            "Step 749 Loss:  tensor(0.0255)\n",
            "Step 750 Loss:  tensor(0.0254)\n",
            "Step 751 Loss:  tensor(0.0254)\n",
            "Step 752 Loss:  tensor(0.0254)\n",
            "Step 753 Loss:  tensor(0.0254)\n",
            "Step 754 Loss:  tensor(0.0253)\n",
            "Step 755 Loss:  tensor(0.0253)\n",
            "Step 756 Loss:  tensor(0.0253)\n",
            "Step 757 Loss:  tensor(0.0253)\n",
            "Step 758 Loss:  tensor(0.0252)\n",
            "Step 759 Loss:  tensor(0.0252)\n",
            "Step 760 Loss:  tensor(0.0252)\n",
            "Step 761 Loss:  tensor(0.0252)\n",
            "Step 762 Loss:  tensor(0.0251)\n",
            "Step 763 Loss:  tensor(0.0251)\n",
            "Step 764 Loss:  tensor(0.0251)\n",
            "Step 765 Loss:  tensor(0.0251)\n",
            "Step 766 Loss:  tensor(0.0251)\n",
            "Step 767 Loss:  tensor(0.0250)\n",
            "Step 768 Loss:  tensor(0.0250)\n",
            "Step 769 Loss:  tensor(0.0250)\n",
            "Step 770 Loss:  tensor(0.0250)\n",
            "Step 771 Loss:  tensor(0.0249)\n",
            "Step 772 Loss:  tensor(0.0249)\n",
            "Step 773 Loss:  tensor(0.0249)\n",
            "Step 774 Loss:  tensor(0.0249)\n",
            "Step 775 Loss:  tensor(0.0248)\n",
            "Step 776 Loss:  tensor(0.0248)\n",
            "Step 777 Loss:  tensor(0.0248)\n",
            "Step 778 Loss:  tensor(0.0248)\n",
            "Step 779 Loss:  tensor(0.0247)\n",
            "Step 780 Loss:  tensor(0.0247)\n",
            "Step 781 Loss:  tensor(0.0247)\n",
            "Step 782 Loss:  tensor(0.0247)\n",
            "Step 783 Loss:  tensor(0.0246)\n",
            "Step 784 Loss:  tensor(0.0246)\n",
            "Step 785 Loss:  tensor(0.0246)\n",
            "Step 786 Loss:  tensor(0.0246)\n",
            "Step 787 Loss:  tensor(0.0246)\n",
            "Step 788 Loss:  tensor(0.0245)\n",
            "Step 789 Loss:  tensor(0.0245)\n",
            "Step 790 Loss:  tensor(0.0245)\n",
            "Step 791 Loss:  tensor(0.0245)\n",
            "Step 792 Loss:  tensor(0.0244)\n",
            "Step 793 Loss:  tensor(0.0244)\n",
            "Step 794 Loss:  tensor(0.0244)\n",
            "Step 795 Loss:  tensor(0.0244)\n",
            "Step 796 Loss:  tensor(0.0244)\n",
            "Step 797 Loss:  tensor(0.0243)\n",
            "Step 798 Loss:  tensor(0.0243)\n",
            "Step 799 Loss:  tensor(0.0243)\n",
            "Step 800 Loss:  tensor(0.0243)\n",
            "Step 801 Loss:  tensor(0.0242)\n",
            "Step 802 Loss:  tensor(0.0242)\n",
            "Step 803 Loss:  tensor(0.0242)\n",
            "Step 804 Loss:  tensor(0.0242)\n",
            "Step 805 Loss:  tensor(0.0242)\n",
            "Step 806 Loss:  tensor(0.0241)\n",
            "Step 807 Loss:  tensor(0.0241)\n",
            "Step 808 Loss:  tensor(0.0241)\n",
            "Step 809 Loss:  tensor(0.0241)\n",
            "Step 810 Loss:  tensor(0.0240)\n",
            "Step 811 Loss:  tensor(0.0240)\n",
            "Step 812 Loss:  tensor(0.0240)\n",
            "Step 813 Loss:  tensor(0.0240)\n",
            "Step 814 Loss:  tensor(0.0240)\n",
            "Step 815 Loss:  tensor(0.0239)\n",
            "Step 816 Loss:  tensor(0.0239)\n",
            "Step 817 Loss:  tensor(0.0239)\n",
            "Step 818 Loss:  tensor(0.0239)\n",
            "Step 819 Loss:  tensor(0.0239)\n",
            "Step 820 Loss:  tensor(0.0238)\n",
            "Step 821 Loss:  tensor(0.0238)\n",
            "Step 822 Loss:  tensor(0.0238)\n",
            "Step 823 Loss:  tensor(0.0238)\n",
            "Step 824 Loss:  tensor(0.0237)\n",
            "Step 825 Loss:  tensor(0.0237)\n",
            "Step 826 Loss:  tensor(0.0237)\n",
            "Step 827 Loss:  tensor(0.0237)\n",
            "Step 828 Loss:  tensor(0.0237)\n",
            "Step 829 Loss:  tensor(0.0236)\n",
            "Step 830 Loss:  tensor(0.0236)\n",
            "Step 831 Loss:  tensor(0.0236)\n",
            "Step 832 Loss:  tensor(0.0236)\n",
            "Step 833 Loss:  tensor(0.0236)\n",
            "Step 834 Loss:  tensor(0.0235)\n",
            "Step 835 Loss:  tensor(0.0235)\n",
            "Step 836 Loss:  tensor(0.0235)\n",
            "Step 837 Loss:  tensor(0.0235)\n",
            "Step 838 Loss:  tensor(0.0235)\n",
            "Step 839 Loss:  tensor(0.0234)\n",
            "Step 840 Loss:  tensor(0.0234)\n",
            "Step 841 Loss:  tensor(0.0234)\n",
            "Step 842 Loss:  tensor(0.0234)\n",
            "Step 843 Loss:  tensor(0.0234)\n",
            "Step 844 Loss:  tensor(0.0233)\n",
            "Step 845 Loss:  tensor(0.0233)\n",
            "Step 846 Loss:  tensor(0.0233)\n",
            "Step 847 Loss:  tensor(0.0233)\n",
            "Step 848 Loss:  tensor(0.0233)\n",
            "Step 849 Loss:  tensor(0.0232)\n",
            "Step 850 Loss:  tensor(0.0232)\n",
            "Step 851 Loss:  tensor(0.0232)\n",
            "Step 852 Loss:  tensor(0.0232)\n",
            "Step 853 Loss:  tensor(0.0232)\n",
            "Step 854 Loss:  tensor(0.0231)\n",
            "Step 855 Loss:  tensor(0.0231)\n",
            "Step 856 Loss:  tensor(0.0231)\n",
            "Step 857 Loss:  tensor(0.0231)\n",
            "Step 858 Loss:  tensor(0.0231)\n",
            "Step 859 Loss:  tensor(0.0231)\n",
            "Step 860 Loss:  tensor(0.0230)\n",
            "Step 861 Loss:  tensor(0.0230)\n",
            "Step 862 Loss:  tensor(0.0230)\n",
            "Step 863 Loss:  tensor(0.0230)\n",
            "Step 864 Loss:  tensor(0.0230)\n",
            "Step 865 Loss:  tensor(0.0229)\n",
            "Step 866 Loss:  tensor(0.0229)\n",
            "Step 867 Loss:  tensor(0.0229)\n",
            "Step 868 Loss:  tensor(0.0229)\n",
            "Step 869 Loss:  tensor(0.0229)\n",
            "Step 870 Loss:  tensor(0.0228)\n",
            "Step 871 Loss:  tensor(0.0228)\n",
            "Step 872 Loss:  tensor(0.0228)\n",
            "Step 873 Loss:  tensor(0.0228)\n",
            "Step 874 Loss:  tensor(0.0228)\n",
            "Step 875 Loss:  tensor(0.0228)\n",
            "Step 876 Loss:  tensor(0.0227)\n",
            "Step 877 Loss:  tensor(0.0227)\n",
            "Step 878 Loss:  tensor(0.0227)\n",
            "Step 879 Loss:  tensor(0.0227)\n",
            "Step 880 Loss:  tensor(0.0227)\n",
            "Step 881 Loss:  tensor(0.0226)\n",
            "Step 882 Loss:  tensor(0.0226)\n",
            "Step 883 Loss:  tensor(0.0226)\n",
            "Step 884 Loss:  tensor(0.0226)\n",
            "Step 885 Loss:  tensor(0.0226)\n",
            "Step 886 Loss:  tensor(0.0226)\n",
            "Step 887 Loss:  tensor(0.0225)\n",
            "Step 888 Loss:  tensor(0.0225)\n",
            "Step 889 Loss:  tensor(0.0225)\n",
            "Step 890 Loss:  tensor(0.0225)\n",
            "Step 891 Loss:  tensor(0.0225)\n",
            "Step 892 Loss:  tensor(0.0224)\n",
            "Step 893 Loss:  tensor(0.0224)\n",
            "Step 894 Loss:  tensor(0.0224)\n",
            "Step 895 Loss:  tensor(0.0224)\n",
            "Step 896 Loss:  tensor(0.0224)\n",
            "Step 897 Loss:  tensor(0.0224)\n",
            "Step 898 Loss:  tensor(0.0223)\n",
            "Step 899 Loss:  tensor(0.0223)\n",
            "Step 900 Loss:  tensor(0.0223)\n",
            "Step 901 Loss:  tensor(0.0223)\n",
            "Step 902 Loss:  tensor(0.0223)\n",
            "Step 903 Loss:  tensor(0.0223)\n",
            "Step 904 Loss:  tensor(0.0222)\n",
            "Step 905 Loss:  tensor(0.0222)\n",
            "Step 906 Loss:  tensor(0.0222)\n",
            "Step 907 Loss:  tensor(0.0222)\n",
            "Step 908 Loss:  tensor(0.0222)\n",
            "Step 909 Loss:  tensor(0.0222)\n",
            "Step 910 Loss:  tensor(0.0221)\n",
            "Step 911 Loss:  tensor(0.0221)\n",
            "Step 912 Loss:  tensor(0.0221)\n",
            "Step 913 Loss:  tensor(0.0221)\n",
            "Step 914 Loss:  tensor(0.0221)\n",
            "Step 915 Loss:  tensor(0.0221)\n",
            "Step 916 Loss:  tensor(0.0220)\n",
            "Step 917 Loss:  tensor(0.0220)\n",
            "Step 918 Loss:  tensor(0.0220)\n",
            "Step 919 Loss:  tensor(0.0220)\n",
            "Step 920 Loss:  tensor(0.0220)\n",
            "Step 921 Loss:  tensor(0.0220)\n",
            "Step 922 Loss:  tensor(0.0219)\n",
            "Step 923 Loss:  tensor(0.0219)\n",
            "Step 924 Loss:  tensor(0.0219)\n",
            "Step 925 Loss:  tensor(0.0219)\n",
            "Step 926 Loss:  tensor(0.0219)\n",
            "Step 927 Loss:  tensor(0.0219)\n",
            "Step 928 Loss:  tensor(0.0218)\n",
            "Step 929 Loss:  tensor(0.0218)\n",
            "Step 930 Loss:  tensor(0.0218)\n",
            "Step 931 Loss:  tensor(0.0218)\n",
            "Step 932 Loss:  tensor(0.0218)\n",
            "Step 933 Loss:  tensor(0.0218)\n",
            "Step 934 Loss:  tensor(0.0217)\n",
            "Step 935 Loss:  tensor(0.0217)\n",
            "Step 936 Loss:  tensor(0.0217)\n",
            "Step 937 Loss:  tensor(0.0217)\n",
            "Step 938 Loss:  tensor(0.0217)\n",
            "Step 939 Loss:  tensor(0.0217)\n",
            "Step 940 Loss:  tensor(0.0216)\n",
            "Step 941 Loss:  tensor(0.0216)\n",
            "Step 942 Loss:  tensor(0.0216)\n",
            "Step 943 Loss:  tensor(0.0216)\n",
            "Step 944 Loss:  tensor(0.0216)\n",
            "Step 945 Loss:  tensor(0.0216)\n",
            "Step 946 Loss:  tensor(0.0215)\n",
            "Step 947 Loss:  tensor(0.0215)\n",
            "Step 948 Loss:  tensor(0.0215)\n",
            "Step 949 Loss:  tensor(0.0215)\n",
            "Step 950 Loss:  tensor(0.0215)\n",
            "Step 951 Loss:  tensor(0.0215)\n",
            "Step 952 Loss:  tensor(0.0215)\n",
            "Step 953 Loss:  tensor(0.0214)\n",
            "Step 954 Loss:  tensor(0.0214)\n",
            "Step 955 Loss:  tensor(0.0214)\n",
            "Step 956 Loss:  tensor(0.0214)\n",
            "Step 957 Loss:  tensor(0.0214)\n",
            "Step 958 Loss:  tensor(0.0214)\n",
            "Step 959 Loss:  tensor(0.0213)\n",
            "Step 960 Loss:  tensor(0.0213)\n",
            "Step 961 Loss:  tensor(0.0213)\n",
            "Step 962 Loss:  tensor(0.0213)\n",
            "Step 963 Loss:  tensor(0.0213)\n",
            "Step 964 Loss:  tensor(0.0213)\n",
            "Step 965 Loss:  tensor(0.0213)\n",
            "Step 966 Loss:  tensor(0.0212)\n",
            "Step 967 Loss:  tensor(0.0212)\n",
            "Step 968 Loss:  tensor(0.0212)\n",
            "Step 969 Loss:  tensor(0.0212)\n",
            "Step 970 Loss:  tensor(0.0212)\n",
            "Step 971 Loss:  tensor(0.0212)\n",
            "Step 972 Loss:  tensor(0.0211)\n",
            "Step 973 Loss:  tensor(0.0211)\n",
            "Step 974 Loss:  tensor(0.0211)\n",
            "Step 975 Loss:  tensor(0.0211)\n",
            "Step 976 Loss:  tensor(0.0211)\n",
            "Step 977 Loss:  tensor(0.0211)\n",
            "Step 978 Loss:  tensor(0.0211)\n",
            "Step 979 Loss:  tensor(0.0210)\n",
            "Step 980 Loss:  tensor(0.0210)\n",
            "Step 981 Loss:  tensor(0.0210)\n",
            "Step 982 Loss:  tensor(0.0210)\n",
            "Step 983 Loss:  tensor(0.0210)\n",
            "Step 984 Loss:  tensor(0.0210)\n",
            "Step 985 Loss:  tensor(0.0210)\n",
            "Step 986 Loss:  tensor(0.0209)\n",
            "Step 987 Loss:  tensor(0.0209)\n",
            "Step 988 Loss:  tensor(0.0209)\n",
            "Step 989 Loss:  tensor(0.0209)\n",
            "Step 990 Loss:  tensor(0.0209)\n",
            "Step 991 Loss:  tensor(0.0209)\n",
            "Step 992 Loss:  tensor(0.0209)\n",
            "Step 993 Loss:  tensor(0.0208)\n",
            "Step 994 Loss:  tensor(0.0208)\n",
            "Step 995 Loss:  tensor(0.0208)\n",
            "Step 996 Loss:  tensor(0.0208)\n",
            "Step 997 Loss:  tensor(0.0208)\n",
            "Step 998 Loss:  tensor(0.0208)\n",
            "Step 999 Loss:  tensor(0.0208)\n",
            "Step 1000 Loss:  tensor(0.0207)\n",
            "Step 1001 Loss:  tensor(0.0207)\n",
            "Step 1002 Loss:  tensor(0.0207)\n",
            "Step 1003 Loss:  tensor(0.0207)\n",
            "Step 1004 Loss:  tensor(0.0207)\n",
            "Step 1005 Loss:  tensor(0.0207)\n",
            "Step 1006 Loss:  tensor(0.0207)\n",
            "Step 1007 Loss:  tensor(0.0206)\n",
            "Step 1008 Loss:  tensor(0.0206)\n",
            "Step 1009 Loss:  tensor(0.0206)\n",
            "Step 1010 Loss:  tensor(0.0206)\n",
            "Step 1011 Loss:  tensor(0.0206)\n",
            "Step 1012 Loss:  tensor(0.0206)\n",
            "Step 1013 Loss:  tensor(0.0206)\n",
            "Step 1014 Loss:  tensor(0.0205)\n",
            "Step 1015 Loss:  tensor(0.0205)\n",
            "Step 1016 Loss:  tensor(0.0205)\n",
            "Step 1017 Loss:  tensor(0.0205)\n",
            "Step 1018 Loss:  tensor(0.0205)\n",
            "Step 1019 Loss:  tensor(0.0205)\n",
            "Step 1020 Loss:  tensor(0.0205)\n",
            "Step 1021 Loss:  tensor(0.0204)\n",
            "Step 1022 Loss:  tensor(0.0204)\n",
            "Step 1023 Loss:  tensor(0.0204)\n",
            "Step 1024 Loss:  tensor(0.0204)\n",
            "Step 1025 Loss:  tensor(0.0204)\n",
            "Step 1026 Loss:  tensor(0.0204)\n",
            "Step 1027 Loss:  tensor(0.0204)\n",
            "Step 1028 Loss:  tensor(0.0204)\n",
            "Step 1029 Loss:  tensor(0.0203)\n",
            "Step 1030 Loss:  tensor(0.0203)\n",
            "Step 1031 Loss:  tensor(0.0203)\n",
            "Step 1032 Loss:  tensor(0.0203)\n",
            "Step 1033 Loss:  tensor(0.0203)\n",
            "Step 1034 Loss:  tensor(0.0203)\n",
            "Step 1035 Loss:  tensor(0.0203)\n",
            "Step 1036 Loss:  tensor(0.0202)\n",
            "Step 1037 Loss:  tensor(0.0202)\n",
            "Step 1038 Loss:  tensor(0.0202)\n",
            "Step 1039 Loss:  tensor(0.0202)\n",
            "Step 1040 Loss:  tensor(0.0202)\n",
            "Step 1041 Loss:  tensor(0.0202)\n",
            "Step 1042 Loss:  tensor(0.0202)\n",
            "Step 1043 Loss:  tensor(0.0202)\n",
            "Step 1044 Loss:  tensor(0.0201)\n",
            "Step 1045 Loss:  tensor(0.0201)\n",
            "Step 1046 Loss:  tensor(0.0201)\n",
            "Step 1047 Loss:  tensor(0.0201)\n",
            "Step 1048 Loss:  tensor(0.0201)\n",
            "Step 1049 Loss:  tensor(0.0201)\n",
            "Step 1050 Loss:  tensor(0.0201)\n",
            "Step 1051 Loss:  tensor(0.0201)\n",
            "Step 1052 Loss:  tensor(0.0200)\n",
            "Step 1053 Loss:  tensor(0.0200)\n",
            "Step 1054 Loss:  tensor(0.0200)\n",
            "Step 1055 Loss:  tensor(0.0200)\n",
            "Step 1056 Loss:  tensor(0.0200)\n",
            "Step 1057 Loss:  tensor(0.0200)\n",
            "Step 1058 Loss:  tensor(0.0200)\n",
            "Step 1059 Loss:  tensor(0.0199)\n",
            "Step 1060 Loss:  tensor(0.0199)\n",
            "Step 1061 Loss:  tensor(0.0199)\n",
            "Step 1062 Loss:  tensor(0.0199)\n",
            "Step 1063 Loss:  tensor(0.0199)\n",
            "Step 1064 Loss:  tensor(0.0199)\n",
            "Step 1065 Loss:  tensor(0.0199)\n",
            "Step 1066 Loss:  tensor(0.0199)\n",
            "Step 1067 Loss:  tensor(0.0198)\n",
            "Step 1068 Loss:  tensor(0.0198)\n",
            "Step 1069 Loss:  tensor(0.0198)\n",
            "Step 1070 Loss:  tensor(0.0198)\n",
            "Step 1071 Loss:  tensor(0.0198)\n",
            "Step 1072 Loss:  tensor(0.0198)\n",
            "Step 1073 Loss:  tensor(0.0198)\n",
            "Step 1074 Loss:  tensor(0.0198)\n",
            "Step 1075 Loss:  tensor(0.0197)\n",
            "Step 1076 Loss:  tensor(0.0197)\n",
            "Step 1077 Loss:  tensor(0.0197)\n",
            "Step 1078 Loss:  tensor(0.0197)\n",
            "Step 1079 Loss:  tensor(0.0197)\n",
            "Step 1080 Loss:  tensor(0.0197)\n",
            "Step 1081 Loss:  tensor(0.0197)\n",
            "Step 1082 Loss:  tensor(0.0197)\n",
            "Step 1083 Loss:  tensor(0.0197)\n",
            "Step 1084 Loss:  tensor(0.0196)\n",
            "Step 1085 Loss:  tensor(0.0196)\n",
            "Step 1086 Loss:  tensor(0.0196)\n",
            "Step 1087 Loss:  tensor(0.0196)\n",
            "Step 1088 Loss:  tensor(0.0196)\n",
            "Step 1089 Loss:  tensor(0.0196)\n",
            "Step 1090 Loss:  tensor(0.0196)\n",
            "Step 1091 Loss:  tensor(0.0196)\n",
            "Step 1092 Loss:  tensor(0.0195)\n",
            "Step 1093 Loss:  tensor(0.0195)\n",
            "Step 1094 Loss:  tensor(0.0195)\n",
            "Step 1095 Loss:  tensor(0.0195)\n",
            "Step 1096 Loss:  tensor(0.0195)\n",
            "Step 1097 Loss:  tensor(0.0195)\n",
            "Step 1098 Loss:  tensor(0.0195)\n",
            "Step 1099 Loss:  tensor(0.0195)\n",
            "Step 1100 Loss:  tensor(0.0194)\n",
            "Step 1101 Loss:  tensor(0.0194)\n",
            "Step 1102 Loss:  tensor(0.0194)\n",
            "Step 1103 Loss:  tensor(0.0194)\n",
            "Step 1104 Loss:  tensor(0.0194)\n",
            "Step 1105 Loss:  tensor(0.0194)\n",
            "Step 1106 Loss:  tensor(0.0194)\n",
            "Step 1107 Loss:  tensor(0.0194)\n",
            "Step 1108 Loss:  tensor(0.0194)\n",
            "Step 1109 Loss:  tensor(0.0193)\n",
            "Step 1110 Loss:  tensor(0.0193)\n",
            "Step 1111 Loss:  tensor(0.0193)\n",
            "Step 1112 Loss:  tensor(0.0193)\n",
            "Step 1113 Loss:  tensor(0.0193)\n",
            "Step 1114 Loss:  tensor(0.0193)\n",
            "Step 1115 Loss:  tensor(0.0193)\n",
            "Step 1116 Loss:  tensor(0.0193)\n",
            "Step 1117 Loss:  tensor(0.0192)\n",
            "Step 1118 Loss:  tensor(0.0192)\n",
            "Step 1119 Loss:  tensor(0.0192)\n",
            "Step 1120 Loss:  tensor(0.0192)\n",
            "Step 1121 Loss:  tensor(0.0192)\n",
            "Step 1122 Loss:  tensor(0.0192)\n",
            "Step 1123 Loss:  tensor(0.0192)\n",
            "Step 1124 Loss:  tensor(0.0192)\n",
            "Step 1125 Loss:  tensor(0.0192)\n",
            "Step 1126 Loss:  tensor(0.0191)\n",
            "Step 1127 Loss:  tensor(0.0191)\n",
            "Step 1128 Loss:  tensor(0.0191)\n",
            "Step 1129 Loss:  tensor(0.0191)\n",
            "Step 1130 Loss:  tensor(0.0191)\n",
            "Step 1131 Loss:  tensor(0.0191)\n",
            "Step 1132 Loss:  tensor(0.0191)\n",
            "Step 1133 Loss:  tensor(0.0191)\n",
            "Step 1134 Loss:  tensor(0.0191)\n",
            "Step 1135 Loss:  tensor(0.0190)\n",
            "Step 1136 Loss:  tensor(0.0190)\n",
            "Step 1137 Loss:  tensor(0.0190)\n",
            "Step 1138 Loss:  tensor(0.0190)\n",
            "Step 1139 Loss:  tensor(0.0190)\n",
            "Step 1140 Loss:  tensor(0.0190)\n",
            "Step 1141 Loss:  tensor(0.0190)\n",
            "Step 1142 Loss:  tensor(0.0190)\n",
            "Step 1143 Loss:  tensor(0.0190)\n",
            "Step 1144 Loss:  tensor(0.0189)\n",
            "Step 1145 Loss:  tensor(0.0189)\n",
            "Step 1146 Loss:  tensor(0.0189)\n",
            "Step 1147 Loss:  tensor(0.0189)\n",
            "Step 1148 Loss:  tensor(0.0189)\n",
            "Step 1149 Loss:  tensor(0.0189)\n",
            "Step 1150 Loss:  tensor(0.0189)\n",
            "Step 1151 Loss:  tensor(0.0189)\n",
            "Step 1152 Loss:  tensor(0.0189)\n",
            "Step 1153 Loss:  tensor(0.0188)\n",
            "Step 1154 Loss:  tensor(0.0188)\n",
            "Step 1155 Loss:  tensor(0.0188)\n",
            "Step 1156 Loss:  tensor(0.0188)\n",
            "Step 1157 Loss:  tensor(0.0188)\n",
            "Step 1158 Loss:  tensor(0.0188)\n",
            "Step 1159 Loss:  tensor(0.0188)\n",
            "Step 1160 Loss:  tensor(0.0188)\n",
            "Step 1161 Loss:  tensor(0.0188)\n",
            "Step 1162 Loss:  tensor(0.0187)\n",
            "Step 1163 Loss:  tensor(0.0187)\n",
            "Step 1164 Loss:  tensor(0.0187)\n",
            "Step 1165 Loss:  tensor(0.0187)\n",
            "Step 1166 Loss:  tensor(0.0187)\n",
            "Step 1167 Loss:  tensor(0.0187)\n",
            "Step 1168 Loss:  tensor(0.0187)\n",
            "Step 1169 Loss:  tensor(0.0187)\n",
            "Step 1170 Loss:  tensor(0.0187)\n",
            "Step 1171 Loss:  tensor(0.0186)\n",
            "Step 1172 Loss:  tensor(0.0186)\n",
            "Step 1173 Loss:  tensor(0.0186)\n",
            "Step 1174 Loss:  tensor(0.0186)\n",
            "Step 1175 Loss:  tensor(0.0186)\n",
            "Step 1176 Loss:  tensor(0.0186)\n",
            "Step 1177 Loss:  tensor(0.0186)\n",
            "Step 1178 Loss:  tensor(0.0186)\n",
            "Step 1179 Loss:  tensor(0.0186)\n",
            "Step 1180 Loss:  tensor(0.0186)\n",
            "Step 1181 Loss:  tensor(0.0185)\n",
            "Step 1182 Loss:  tensor(0.0185)\n",
            "Step 1183 Loss:  tensor(0.0185)\n",
            "Step 1184 Loss:  tensor(0.0185)\n",
            "Step 1185 Loss:  tensor(0.0185)\n",
            "Step 1186 Loss:  tensor(0.0185)\n",
            "Step 1187 Loss:  tensor(0.0185)\n",
            "Step 1188 Loss:  tensor(0.0185)\n",
            "Step 1189 Loss:  tensor(0.0185)\n",
            "Step 1190 Loss:  tensor(0.0184)\n",
            "Step 1191 Loss:  tensor(0.0184)\n",
            "Step 1192 Loss:  tensor(0.0184)\n",
            "Step 1193 Loss:  tensor(0.0184)\n",
            "Step 1194 Loss:  tensor(0.0184)\n",
            "Step 1195 Loss:  tensor(0.0184)\n",
            "Step 1196 Loss:  tensor(0.0184)\n",
            "Step 1197 Loss:  tensor(0.0184)\n",
            "Step 1198 Loss:  tensor(0.0184)\n",
            "Step 1199 Loss:  tensor(0.0184)\n",
            "Step 1200 Loss:  tensor(0.0183)\n",
            "Step 1201 Loss:  tensor(0.0183)\n",
            "Step 1202 Loss:  tensor(0.0183)\n",
            "Step 1203 Loss:  tensor(0.0183)\n",
            "Step 1204 Loss:  tensor(0.0183)\n",
            "Step 1205 Loss:  tensor(0.0183)\n",
            "Step 1206 Loss:  tensor(0.0183)\n",
            "Step 1207 Loss:  tensor(0.0183)\n",
            "Step 1208 Loss:  tensor(0.0183)\n",
            "Step 1209 Loss:  tensor(0.0183)\n",
            "Step 1210 Loss:  tensor(0.0182)\n",
            "Step 1211 Loss:  tensor(0.0182)\n",
            "Step 1212 Loss:  tensor(0.0182)\n",
            "Step 1213 Loss:  tensor(0.0182)\n",
            "Step 1214 Loss:  tensor(0.0182)\n",
            "Step 1215 Loss:  tensor(0.0182)\n",
            "Step 1216 Loss:  tensor(0.0182)\n",
            "Step 1217 Loss:  tensor(0.0182)\n",
            "Step 1218 Loss:  tensor(0.0182)\n",
            "Step 1219 Loss:  tensor(0.0182)\n",
            "Step 1220 Loss:  tensor(0.0181)\n",
            "Step 1221 Loss:  tensor(0.0181)\n",
            "Step 1222 Loss:  tensor(0.0181)\n",
            "Step 1223 Loss:  tensor(0.0181)\n",
            "Step 1224 Loss:  tensor(0.0181)\n",
            "Step 1225 Loss:  tensor(0.0181)\n",
            "Step 1226 Loss:  tensor(0.0181)\n",
            "Step 1227 Loss:  tensor(0.0181)\n",
            "Step 1228 Loss:  tensor(0.0181)\n",
            "Step 1229 Loss:  tensor(0.0181)\n",
            "Step 1230 Loss:  tensor(0.0180)\n",
            "Step 1231 Loss:  tensor(0.0180)\n",
            "Step 1232 Loss:  tensor(0.0180)\n",
            "Step 1233 Loss:  tensor(0.0180)\n",
            "Step 1234 Loss:  tensor(0.0180)\n",
            "Step 1235 Loss:  tensor(0.0180)\n",
            "Step 1236 Loss:  tensor(0.0180)\n",
            "Step 1237 Loss:  tensor(0.0180)\n",
            "Step 1238 Loss:  tensor(0.0180)\n",
            "Step 1239 Loss:  tensor(0.0180)\n",
            "Step 1240 Loss:  tensor(0.0180)\n",
            "Step 1241 Loss:  tensor(0.0179)\n",
            "Step 1242 Loss:  tensor(0.0179)\n",
            "Step 1243 Loss:  tensor(0.0179)\n",
            "Step 1244 Loss:  tensor(0.0179)\n",
            "Step 1245 Loss:  tensor(0.0179)\n",
            "Step 1246 Loss:  tensor(0.0179)\n",
            "Step 1247 Loss:  tensor(0.0179)\n",
            "Step 1248 Loss:  tensor(0.0179)\n",
            "Step 1249 Loss:  tensor(0.0179)\n",
            "Step 1250 Loss:  tensor(0.0179)\n",
            "Step 1251 Loss:  tensor(0.0178)\n",
            "Step 1252 Loss:  tensor(0.0178)\n",
            "Step 1253 Loss:  tensor(0.0178)\n",
            "Step 1254 Loss:  tensor(0.0178)\n",
            "Step 1255 Loss:  tensor(0.0178)\n",
            "Step 1256 Loss:  tensor(0.0178)\n",
            "Step 1257 Loss:  tensor(0.0178)\n",
            "Step 1258 Loss:  tensor(0.0178)\n",
            "Step 1259 Loss:  tensor(0.0178)\n",
            "Step 1260 Loss:  tensor(0.0178)\n",
            "Step 1261 Loss:  tensor(0.0178)\n",
            "Step 1262 Loss:  tensor(0.0177)\n",
            "Step 1263 Loss:  tensor(0.0177)\n",
            "Step 1264 Loss:  tensor(0.0177)\n",
            "Step 1265 Loss:  tensor(0.0177)\n",
            "Step 1266 Loss:  tensor(0.0177)\n",
            "Step 1267 Loss:  tensor(0.0177)\n",
            "Step 1268 Loss:  tensor(0.0177)\n",
            "Step 1269 Loss:  tensor(0.0177)\n",
            "Step 1270 Loss:  tensor(0.0177)\n",
            "Step 1271 Loss:  tensor(0.0177)\n",
            "Step 1272 Loss:  tensor(0.0176)\n",
            "Step 1273 Loss:  tensor(0.0176)\n",
            "Step 1274 Loss:  tensor(0.0176)\n",
            "Step 1275 Loss:  tensor(0.0176)\n",
            "Step 1276 Loss:  tensor(0.0176)\n",
            "Step 1277 Loss:  tensor(0.0176)\n",
            "Step 1278 Loss:  tensor(0.0176)\n",
            "Step 1279 Loss:  tensor(0.0176)\n",
            "Step 1280 Loss:  tensor(0.0176)\n",
            "Step 1281 Loss:  tensor(0.0176)\n",
            "Step 1282 Loss:  tensor(0.0176)\n",
            "Step 1283 Loss:  tensor(0.0175)\n",
            "Step 1284 Loss:  tensor(0.0175)\n",
            "Step 1285 Loss:  tensor(0.0175)\n",
            "Step 1286 Loss:  tensor(0.0175)\n",
            "Step 1287 Loss:  tensor(0.0175)\n",
            "Step 1288 Loss:  tensor(0.0175)\n",
            "Step 1289 Loss:  tensor(0.0175)\n",
            "Step 1290 Loss:  tensor(0.0175)\n",
            "Step 1291 Loss:  tensor(0.0175)\n",
            "Step 1292 Loss:  tensor(0.0175)\n",
            "Step 1293 Loss:  tensor(0.0175)\n",
            "Step 1294 Loss:  tensor(0.0175)\n",
            "Step 1295 Loss:  tensor(0.0174)\n",
            "Step 1296 Loss:  tensor(0.0174)\n",
            "Step 1297 Loss:  tensor(0.0174)\n",
            "Step 1298 Loss:  tensor(0.0174)\n",
            "Step 1299 Loss:  tensor(0.0174)\n",
            "Step 1300 Loss:  tensor(0.0174)\n",
            "Step 1301 Loss:  tensor(0.0174)\n",
            "Step 1302 Loss:  tensor(0.0174)\n",
            "Step 1303 Loss:  tensor(0.0174)\n",
            "Step 1304 Loss:  tensor(0.0174)\n",
            "Step 1305 Loss:  tensor(0.0174)\n",
            "Step 1306 Loss:  tensor(0.0173)\n",
            "Step 1307 Loss:  tensor(0.0173)\n",
            "Step 1308 Loss:  tensor(0.0173)\n",
            "Step 1309 Loss:  tensor(0.0173)\n",
            "Step 1310 Loss:  tensor(0.0173)\n",
            "Step 1311 Loss:  tensor(0.0173)\n",
            "Step 1312 Loss:  tensor(0.0173)\n",
            "Step 1313 Loss:  tensor(0.0173)\n",
            "Step 1314 Loss:  tensor(0.0173)\n",
            "Step 1315 Loss:  tensor(0.0173)\n",
            "Step 1316 Loss:  tensor(0.0173)\n",
            "Step 1317 Loss:  tensor(0.0172)\n",
            "Step 1318 Loss:  tensor(0.0172)\n",
            "Step 1319 Loss:  tensor(0.0172)\n",
            "Step 1320 Loss:  tensor(0.0172)\n",
            "Step 1321 Loss:  tensor(0.0172)\n",
            "Step 1322 Loss:  tensor(0.0172)\n",
            "Step 1323 Loss:  tensor(0.0172)\n",
            "Step 1324 Loss:  tensor(0.0172)\n",
            "Step 1325 Loss:  tensor(0.0172)\n",
            "Step 1326 Loss:  tensor(0.0172)\n",
            "Step 1327 Loss:  tensor(0.0172)\n",
            "Step 1328 Loss:  tensor(0.0172)\n",
            "Step 1329 Loss:  tensor(0.0171)\n",
            "Step 1330 Loss:  tensor(0.0171)\n",
            "Step 1331 Loss:  tensor(0.0171)\n",
            "Step 1332 Loss:  tensor(0.0171)\n",
            "Step 1333 Loss:  tensor(0.0171)\n",
            "Step 1334 Loss:  tensor(0.0171)\n",
            "Step 1335 Loss:  tensor(0.0171)\n",
            "Step 1336 Loss:  tensor(0.0171)\n",
            "Step 1337 Loss:  tensor(0.0171)\n",
            "Step 1338 Loss:  tensor(0.0171)\n",
            "Step 1339 Loss:  tensor(0.0171)\n",
            "Step 1340 Loss:  tensor(0.0171)\n",
            "Step 1341 Loss:  tensor(0.0170)\n",
            "Step 1342 Loss:  tensor(0.0170)\n",
            "Step 1343 Loss:  tensor(0.0170)\n",
            "Step 1344 Loss:  tensor(0.0170)\n",
            "Step 1345 Loss:  tensor(0.0170)\n",
            "Step 1346 Loss:  tensor(0.0170)\n",
            "Step 1347 Loss:  tensor(0.0170)\n",
            "Step 1348 Loss:  tensor(0.0170)\n",
            "Step 1349 Loss:  tensor(0.0170)\n",
            "Step 1350 Loss:  tensor(0.0170)\n",
            "Step 1351 Loss:  tensor(0.0170)\n",
            "Step 1352 Loss:  tensor(0.0170)\n",
            "Step 1353 Loss:  tensor(0.0169)\n",
            "Step 1354 Loss:  tensor(0.0169)\n",
            "Step 1355 Loss:  tensor(0.0169)\n",
            "Step 1356 Loss:  tensor(0.0169)\n",
            "Step 1357 Loss:  tensor(0.0169)\n",
            "Step 1358 Loss:  tensor(0.0169)\n",
            "Step 1359 Loss:  tensor(0.0169)\n",
            "Step 1360 Loss:  tensor(0.0169)\n",
            "Step 1361 Loss:  tensor(0.0169)\n",
            "Step 1362 Loss:  tensor(0.0169)\n",
            "Step 1363 Loss:  tensor(0.0169)\n",
            "Step 1364 Loss:  tensor(0.0169)\n",
            "Step 1365 Loss:  tensor(0.0168)\n",
            "Step 1366 Loss:  tensor(0.0168)\n",
            "Step 1367 Loss:  tensor(0.0168)\n",
            "Step 1368 Loss:  tensor(0.0168)\n",
            "Step 1369 Loss:  tensor(0.0168)\n",
            "Step 1370 Loss:  tensor(0.0168)\n",
            "Step 1371 Loss:  tensor(0.0168)\n",
            "Step 1372 Loss:  tensor(0.0168)\n",
            "Step 1373 Loss:  tensor(0.0168)\n",
            "Step 1374 Loss:  tensor(0.0168)\n",
            "Step 1375 Loss:  tensor(0.0168)\n",
            "Step 1376 Loss:  tensor(0.0168)\n",
            "Step 1377 Loss:  tensor(0.0167)\n",
            "Step 1378 Loss:  tensor(0.0167)\n",
            "Step 1379 Loss:  tensor(0.0167)\n",
            "Step 1380 Loss:  tensor(0.0167)\n",
            "Step 1381 Loss:  tensor(0.0167)\n",
            "Step 1382 Loss:  tensor(0.0167)\n",
            "Step 1383 Loss:  tensor(0.0167)\n",
            "Step 1384 Loss:  tensor(0.0167)\n",
            "Step 1385 Loss:  tensor(0.0167)\n",
            "Step 1386 Loss:  tensor(0.0167)\n",
            "Step 1387 Loss:  tensor(0.0167)\n",
            "Step 1388 Loss:  tensor(0.0167)\n",
            "Step 1389 Loss:  tensor(0.0167)\n",
            "Step 1390 Loss:  tensor(0.0166)\n",
            "Step 1391 Loss:  tensor(0.0166)\n",
            "Step 1392 Loss:  tensor(0.0166)\n",
            "Step 1393 Loss:  tensor(0.0166)\n",
            "Step 1394 Loss:  tensor(0.0166)\n",
            "Step 1395 Loss:  tensor(0.0166)\n",
            "Step 1396 Loss:  tensor(0.0166)\n",
            "Step 1397 Loss:  tensor(0.0166)\n",
            "Step 1398 Loss:  tensor(0.0166)\n",
            "Step 1399 Loss:  tensor(0.0166)\n",
            "Step 1400 Loss:  tensor(0.0166)\n",
            "Step 1401 Loss:  tensor(0.0166)\n",
            "Step 1402 Loss:  tensor(0.0165)\n",
            "Step 1403 Loss:  tensor(0.0165)\n",
            "Step 1404 Loss:  tensor(0.0165)\n",
            "Step 1405 Loss:  tensor(0.0165)\n",
            "Step 1406 Loss:  tensor(0.0165)\n",
            "Step 1407 Loss:  tensor(0.0165)\n",
            "Step 1408 Loss:  tensor(0.0165)\n",
            "Step 1409 Loss:  tensor(0.0165)\n",
            "Step 1410 Loss:  tensor(0.0165)\n",
            "Step 1411 Loss:  tensor(0.0165)\n",
            "Step 1412 Loss:  tensor(0.0165)\n",
            "Step 1413 Loss:  tensor(0.0165)\n",
            "Step 1414 Loss:  tensor(0.0165)\n",
            "Step 1415 Loss:  tensor(0.0164)\n",
            "Step 1416 Loss:  tensor(0.0164)\n",
            "Step 1417 Loss:  tensor(0.0164)\n",
            "Step 1418 Loss:  tensor(0.0164)\n",
            "Step 1419 Loss:  tensor(0.0164)\n",
            "Step 1420 Loss:  tensor(0.0164)\n",
            "Step 1421 Loss:  tensor(0.0164)\n",
            "Step 1422 Loss:  tensor(0.0164)\n",
            "Step 1423 Loss:  tensor(0.0164)\n",
            "Step 1424 Loss:  tensor(0.0164)\n",
            "Step 1425 Loss:  tensor(0.0164)\n",
            "Step 1426 Loss:  tensor(0.0164)\n",
            "Step 1427 Loss:  tensor(0.0164)\n",
            "Step 1428 Loss:  tensor(0.0163)\n",
            "Step 1429 Loss:  tensor(0.0163)\n",
            "Step 1430 Loss:  tensor(0.0163)\n",
            "Step 1431 Loss:  tensor(0.0163)\n",
            "Step 1432 Loss:  tensor(0.0163)\n",
            "Step 1433 Loss:  tensor(0.0163)\n",
            "Step 1434 Loss:  tensor(0.0163)\n",
            "Step 1435 Loss:  tensor(0.0163)\n",
            "Step 1436 Loss:  tensor(0.0163)\n",
            "Step 1437 Loss:  tensor(0.0163)\n",
            "Step 1438 Loss:  tensor(0.0163)\n",
            "Step 1439 Loss:  tensor(0.0163)\n",
            "Step 1440 Loss:  tensor(0.0163)\n",
            "Step 1441 Loss:  tensor(0.0163)\n",
            "Step 1442 Loss:  tensor(0.0162)\n",
            "Step 1443 Loss:  tensor(0.0162)\n",
            "Step 1444 Loss:  tensor(0.0162)\n",
            "Step 1445 Loss:  tensor(0.0162)\n",
            "Step 1446 Loss:  tensor(0.0162)\n",
            "Step 1447 Loss:  tensor(0.0162)\n",
            "Step 1448 Loss:  tensor(0.0162)\n",
            "Step 1449 Loss:  tensor(0.0162)\n",
            "Step 1450 Loss:  tensor(0.0162)\n",
            "Step 1451 Loss:  tensor(0.0162)\n",
            "Step 1452 Loss:  tensor(0.0162)\n",
            "Step 1453 Loss:  tensor(0.0162)\n",
            "Step 1454 Loss:  tensor(0.0162)\n",
            "Step 1455 Loss:  tensor(0.0161)\n",
            "Step 1456 Loss:  tensor(0.0161)\n",
            "Step 1457 Loss:  tensor(0.0161)\n",
            "Step 1458 Loss:  tensor(0.0161)\n",
            "Step 1459 Loss:  tensor(0.0161)\n",
            "Step 1460 Loss:  tensor(0.0161)\n",
            "Step 1461 Loss:  tensor(0.0161)\n",
            "Step 1462 Loss:  tensor(0.0161)\n",
            "Step 1463 Loss:  tensor(0.0161)\n",
            "Step 1464 Loss:  tensor(0.0161)\n",
            "Step 1465 Loss:  tensor(0.0161)\n",
            "Step 1466 Loss:  tensor(0.0161)\n",
            "Step 1467 Loss:  tensor(0.0161)\n",
            "Step 1468 Loss:  tensor(0.0161)\n",
            "Step 1469 Loss:  tensor(0.0160)\n",
            "Step 1470 Loss:  tensor(0.0160)\n",
            "Step 1471 Loss:  tensor(0.0160)\n",
            "Step 1472 Loss:  tensor(0.0160)\n",
            "Step 1473 Loss:  tensor(0.0160)\n",
            "Step 1474 Loss:  tensor(0.0160)\n",
            "Step 1475 Loss:  tensor(0.0160)\n",
            "Step 1476 Loss:  tensor(0.0160)\n",
            "Step 1477 Loss:  tensor(0.0160)\n",
            "Step 1478 Loss:  tensor(0.0160)\n",
            "Step 1479 Loss:  tensor(0.0160)\n",
            "Step 1480 Loss:  tensor(0.0160)\n",
            "Step 1481 Loss:  tensor(0.0160)\n",
            "Step 1482 Loss:  tensor(0.0160)\n",
            "Step 1483 Loss:  tensor(0.0159)\n",
            "Step 1484 Loss:  tensor(0.0159)\n",
            "Step 1485 Loss:  tensor(0.0159)\n",
            "Step 1486 Loss:  tensor(0.0159)\n",
            "Step 1487 Loss:  tensor(0.0159)\n",
            "Step 1488 Loss:  tensor(0.0159)\n",
            "Step 1489 Loss:  tensor(0.0159)\n",
            "Step 1490 Loss:  tensor(0.0159)\n",
            "Step 1491 Loss:  tensor(0.0159)\n",
            "Step 1492 Loss:  tensor(0.0159)\n",
            "Step 1493 Loss:  tensor(0.0159)\n",
            "Step 1494 Loss:  tensor(0.0159)\n",
            "Step 1495 Loss:  tensor(0.0159)\n",
            "Step 1496 Loss:  tensor(0.0159)\n",
            "Step 1497 Loss:  tensor(0.0158)\n",
            "Step 1498 Loss:  tensor(0.0158)\n",
            "Step 1499 Loss:  tensor(0.0158)\n",
            "Step 1500 Loss:  tensor(0.0158)\n",
            "Step 1501 Loss:  tensor(0.0158)\n",
            "Step 1502 Loss:  tensor(0.0158)\n",
            "Step 1503 Loss:  tensor(0.0158)\n",
            "Step 1504 Loss:  tensor(0.0158)\n",
            "Step 1505 Loss:  tensor(0.0158)\n",
            "Step 1506 Loss:  tensor(0.0158)\n",
            "Step 1507 Loss:  tensor(0.0158)\n",
            "Step 1508 Loss:  tensor(0.0158)\n",
            "Step 1509 Loss:  tensor(0.0158)\n",
            "Step 1510 Loss:  tensor(0.0158)\n",
            "Step 1511 Loss:  tensor(0.0157)\n",
            "Step 1512 Loss:  tensor(0.0157)\n",
            "Step 1513 Loss:  tensor(0.0157)\n",
            "Step 1514 Loss:  tensor(0.0157)\n",
            "Step 1515 Loss:  tensor(0.0157)\n",
            "Step 1516 Loss:  tensor(0.0157)\n",
            "Step 1517 Loss:  tensor(0.0157)\n",
            "Step 1518 Loss:  tensor(0.0157)\n",
            "Step 1519 Loss:  tensor(0.0157)\n",
            "Step 1520 Loss:  tensor(0.0157)\n",
            "Step 1521 Loss:  tensor(0.0157)\n",
            "Step 1522 Loss:  tensor(0.0157)\n",
            "Step 1523 Loss:  tensor(0.0157)\n",
            "Step 1524 Loss:  tensor(0.0157)\n",
            "Step 1525 Loss:  tensor(0.0157)\n",
            "Step 1526 Loss:  tensor(0.0156)\n",
            "Step 1527 Loss:  tensor(0.0156)\n",
            "Step 1528 Loss:  tensor(0.0156)\n",
            "Step 1529 Loss:  tensor(0.0156)\n",
            "Step 1530 Loss:  tensor(0.0156)\n",
            "Step 1531 Loss:  tensor(0.0156)\n",
            "Step 1532 Loss:  tensor(0.0156)\n",
            "Step 1533 Loss:  tensor(0.0156)\n",
            "Step 1534 Loss:  tensor(0.0156)\n",
            "Step 1535 Loss:  tensor(0.0156)\n",
            "Step 1536 Loss:  tensor(0.0156)\n",
            "Step 1537 Loss:  tensor(0.0156)\n",
            "Step 1538 Loss:  tensor(0.0156)\n",
            "Step 1539 Loss:  tensor(0.0156)\n",
            "Step 1540 Loss:  tensor(0.0156)\n",
            "Step 1541 Loss:  tensor(0.0155)\n",
            "Step 1542 Loss:  tensor(0.0155)\n",
            "Step 1543 Loss:  tensor(0.0155)\n",
            "Step 1544 Loss:  tensor(0.0155)\n",
            "Step 1545 Loss:  tensor(0.0155)\n",
            "Step 1546 Loss:  tensor(0.0155)\n",
            "Step 1547 Loss:  tensor(0.0155)\n",
            "Step 1548 Loss:  tensor(0.0155)\n",
            "Step 1549 Loss:  tensor(0.0155)\n",
            "Step 1550 Loss:  tensor(0.0155)\n",
            "Step 1551 Loss:  tensor(0.0155)\n",
            "Step 1552 Loss:  tensor(0.0155)\n",
            "Step 1553 Loss:  tensor(0.0155)\n",
            "Step 1554 Loss:  tensor(0.0155)\n",
            "Step 1555 Loss:  tensor(0.0155)\n",
            "Step 1556 Loss:  tensor(0.0154)\n",
            "Step 1557 Loss:  tensor(0.0154)\n",
            "Step 1558 Loss:  tensor(0.0154)\n",
            "Step 1559 Loss:  tensor(0.0154)\n",
            "Step 1560 Loss:  tensor(0.0154)\n",
            "Step 1561 Loss:  tensor(0.0154)\n",
            "Step 1562 Loss:  tensor(0.0154)\n",
            "Step 1563 Loss:  tensor(0.0154)\n",
            "Step 1564 Loss:  tensor(0.0154)\n",
            "Step 1565 Loss:  tensor(0.0154)\n",
            "Step 1566 Loss:  tensor(0.0154)\n",
            "Step 1567 Loss:  tensor(0.0154)\n",
            "Step 1568 Loss:  tensor(0.0154)\n",
            "Step 1569 Loss:  tensor(0.0154)\n",
            "Step 1570 Loss:  tensor(0.0154)\n",
            "Step 1571 Loss:  tensor(0.0153)\n",
            "Step 1572 Loss:  tensor(0.0153)\n",
            "Step 1573 Loss:  tensor(0.0153)\n",
            "Step 1574 Loss:  tensor(0.0153)\n",
            "Step 1575 Loss:  tensor(0.0153)\n",
            "Step 1576 Loss:  tensor(0.0153)\n",
            "Step 1577 Loss:  tensor(0.0153)\n",
            "Step 1578 Loss:  tensor(0.0153)\n",
            "Step 1579 Loss:  tensor(0.0153)\n",
            "Step 1580 Loss:  tensor(0.0153)\n",
            "Step 1581 Loss:  tensor(0.0153)\n",
            "Step 1582 Loss:  tensor(0.0153)\n",
            "Step 1583 Loss:  tensor(0.0153)\n",
            "Step 1584 Loss:  tensor(0.0153)\n",
            "Step 1585 Loss:  tensor(0.0153)\n",
            "Step 1586 Loss:  tensor(0.0153)\n",
            "Step 1587 Loss:  tensor(0.0152)\n",
            "Step 1588 Loss:  tensor(0.0152)\n",
            "Step 1589 Loss:  tensor(0.0152)\n",
            "Step 1590 Loss:  tensor(0.0152)\n",
            "Step 1591 Loss:  tensor(0.0152)\n",
            "Step 1592 Loss:  tensor(0.0152)\n",
            "Step 1593 Loss:  tensor(0.0152)\n",
            "Step 1594 Loss:  tensor(0.0152)\n",
            "Step 1595 Loss:  tensor(0.0152)\n",
            "Step 1596 Loss:  tensor(0.0152)\n",
            "Step 1597 Loss:  tensor(0.0152)\n",
            "Step 1598 Loss:  tensor(0.0152)\n",
            "Step 1599 Loss:  tensor(0.0152)\n",
            "Step 1600 Loss:  tensor(0.0152)\n",
            "Step 1601 Loss:  tensor(0.0152)\n",
            "Step 1602 Loss:  tensor(0.0152)\n",
            "Step 1603 Loss:  tensor(0.0151)\n",
            "Step 1604 Loss:  tensor(0.0151)\n",
            "Step 1605 Loss:  tensor(0.0151)\n",
            "Step 1606 Loss:  tensor(0.0151)\n",
            "Step 1607 Loss:  tensor(0.0151)\n",
            "Step 1608 Loss:  tensor(0.0151)\n",
            "Step 1609 Loss:  tensor(0.0151)\n",
            "Step 1610 Loss:  tensor(0.0151)\n",
            "Step 1611 Loss:  tensor(0.0151)\n",
            "Step 1612 Loss:  tensor(0.0151)\n",
            "Step 1613 Loss:  tensor(0.0151)\n",
            "Step 1614 Loss:  tensor(0.0151)\n",
            "Step 1615 Loss:  tensor(0.0151)\n",
            "Step 1616 Loss:  tensor(0.0151)\n",
            "Step 1617 Loss:  tensor(0.0151)\n",
            "Step 1618 Loss:  tensor(0.0151)\n",
            "Step 1619 Loss:  tensor(0.0150)\n",
            "Step 1620 Loss:  tensor(0.0150)\n",
            "Step 1621 Loss:  tensor(0.0150)\n",
            "Step 1622 Loss:  tensor(0.0150)\n",
            "Step 1623 Loss:  tensor(0.0150)\n",
            "Step 1624 Loss:  tensor(0.0150)\n",
            "Step 1625 Loss:  tensor(0.0150)\n",
            "Step 1626 Loss:  tensor(0.0150)\n",
            "Step 1627 Loss:  tensor(0.0150)\n",
            "Step 1628 Loss:  tensor(0.0150)\n",
            "Step 1629 Loss:  tensor(0.0150)\n",
            "Step 1630 Loss:  tensor(0.0150)\n",
            "Step 1631 Loss:  tensor(0.0150)\n",
            "Step 1632 Loss:  tensor(0.0150)\n",
            "Step 1633 Loss:  tensor(0.0150)\n",
            "Step 1634 Loss:  tensor(0.0150)\n",
            "Step 1635 Loss:  tensor(0.0149)\n",
            "Step 1636 Loss:  tensor(0.0149)\n",
            "Step 1637 Loss:  tensor(0.0149)\n",
            "Step 1638 Loss:  tensor(0.0149)\n",
            "Step 1639 Loss:  tensor(0.0149)\n",
            "Step 1640 Loss:  tensor(0.0149)\n",
            "Step 1641 Loss:  tensor(0.0149)\n",
            "Step 1642 Loss:  tensor(0.0149)\n",
            "Step 1643 Loss:  tensor(0.0149)\n",
            "Step 1644 Loss:  tensor(0.0149)\n",
            "Step 1645 Loss:  tensor(0.0149)\n",
            "Step 1646 Loss:  tensor(0.0149)\n",
            "Step 1647 Loss:  tensor(0.0149)\n",
            "Step 1648 Loss:  tensor(0.0149)\n",
            "Step 1649 Loss:  tensor(0.0149)\n",
            "Step 1650 Loss:  tensor(0.0149)\n",
            "Step 1651 Loss:  tensor(0.0149)\n",
            "Step 1652 Loss:  tensor(0.0148)\n",
            "Step 1653 Loss:  tensor(0.0148)\n",
            "Step 1654 Loss:  tensor(0.0148)\n",
            "Step 1655 Loss:  tensor(0.0148)\n",
            "Step 1656 Loss:  tensor(0.0148)\n",
            "Step 1657 Loss:  tensor(0.0148)\n",
            "Step 1658 Loss:  tensor(0.0148)\n",
            "Step 1659 Loss:  tensor(0.0148)\n",
            "Step 1660 Loss:  tensor(0.0148)\n",
            "Step 1661 Loss:  tensor(0.0148)\n",
            "Step 1662 Loss:  tensor(0.0148)\n",
            "Step 1663 Loss:  tensor(0.0148)\n",
            "Step 1664 Loss:  tensor(0.0148)\n",
            "Step 1665 Loss:  tensor(0.0148)\n",
            "Step 1666 Loss:  tensor(0.0148)\n",
            "Step 1667 Loss:  tensor(0.0148)\n",
            "Step 1668 Loss:  tensor(0.0148)\n",
            "Step 1669 Loss:  tensor(0.0147)\n",
            "Step 1670 Loss:  tensor(0.0147)\n",
            "Step 1671 Loss:  tensor(0.0147)\n",
            "Step 1672 Loss:  tensor(0.0147)\n",
            "Step 1673 Loss:  tensor(0.0147)\n",
            "Step 1674 Loss:  tensor(0.0147)\n",
            "Step 1675 Loss:  tensor(0.0147)\n",
            "Step 1676 Loss:  tensor(0.0147)\n",
            "Step 1677 Loss:  tensor(0.0147)\n",
            "Step 1678 Loss:  tensor(0.0147)\n",
            "Step 1679 Loss:  tensor(0.0147)\n",
            "Step 1680 Loss:  tensor(0.0147)\n",
            "Step 1681 Loss:  tensor(0.0147)\n",
            "Step 1682 Loss:  tensor(0.0147)\n",
            "Step 1683 Loss:  tensor(0.0147)\n",
            "Step 1684 Loss:  tensor(0.0147)\n",
            "Step 1685 Loss:  tensor(0.0147)\n",
            "Step 1686 Loss:  tensor(0.0146)\n",
            "Step 1687 Loss:  tensor(0.0146)\n",
            "Step 1688 Loss:  tensor(0.0146)\n",
            "Step 1689 Loss:  tensor(0.0146)\n",
            "Step 1690 Loss:  tensor(0.0146)\n",
            "Step 1691 Loss:  tensor(0.0146)\n",
            "Step 1692 Loss:  tensor(0.0146)\n",
            "Step 1693 Loss:  tensor(0.0146)\n",
            "Step 1694 Loss:  tensor(0.0146)\n",
            "Step 1695 Loss:  tensor(0.0146)\n",
            "Step 1696 Loss:  tensor(0.0146)\n",
            "Step 1697 Loss:  tensor(0.0146)\n",
            "Step 1698 Loss:  tensor(0.0146)\n",
            "Step 1699 Loss:  tensor(0.0146)\n",
            "Step 1700 Loss:  tensor(0.0146)\n",
            "Step 1701 Loss:  tensor(0.0146)\n",
            "Step 1702 Loss:  tensor(0.0146)\n",
            "Step 1703 Loss:  tensor(0.0145)\n",
            "Step 1704 Loss:  tensor(0.0145)\n",
            "Step 1705 Loss:  tensor(0.0145)\n",
            "Step 1706 Loss:  tensor(0.0145)\n",
            "Step 1707 Loss:  tensor(0.0145)\n",
            "Step 1708 Loss:  tensor(0.0145)\n",
            "Step 1709 Loss:  tensor(0.0145)\n",
            "Step 1710 Loss:  tensor(0.0145)\n",
            "Step 1711 Loss:  tensor(0.0145)\n",
            "Step 1712 Loss:  tensor(0.0145)\n",
            "Step 1713 Loss:  tensor(0.0145)\n",
            "Step 1714 Loss:  tensor(0.0145)\n",
            "Step 1715 Loss:  tensor(0.0145)\n",
            "Step 1716 Loss:  tensor(0.0145)\n",
            "Step 1717 Loss:  tensor(0.0145)\n",
            "Step 1718 Loss:  tensor(0.0145)\n",
            "Step 1719 Loss:  tensor(0.0145)\n",
            "Step 1720 Loss:  tensor(0.0145)\n",
            "Step 1721 Loss:  tensor(0.0144)\n",
            "Step 1722 Loss:  tensor(0.0144)\n",
            "Step 1723 Loss:  tensor(0.0144)\n",
            "Step 1724 Loss:  tensor(0.0144)\n",
            "Step 1725 Loss:  tensor(0.0144)\n",
            "Step 1726 Loss:  tensor(0.0144)\n",
            "Step 1727 Loss:  tensor(0.0144)\n",
            "Step 1728 Loss:  tensor(0.0144)\n",
            "Step 1729 Loss:  tensor(0.0144)\n",
            "Step 1730 Loss:  tensor(0.0144)\n",
            "Step 1731 Loss:  tensor(0.0144)\n",
            "Step 1732 Loss:  tensor(0.0144)\n",
            "Step 1733 Loss:  tensor(0.0144)\n",
            "Step 1734 Loss:  tensor(0.0144)\n",
            "Step 1735 Loss:  tensor(0.0144)\n",
            "Step 1736 Loss:  tensor(0.0144)\n",
            "Step 1737 Loss:  tensor(0.0144)\n",
            "Step 1738 Loss:  tensor(0.0144)\n",
            "Step 1739 Loss:  tensor(0.0143)\n",
            "Step 1740 Loss:  tensor(0.0143)\n",
            "Step 1741 Loss:  tensor(0.0143)\n",
            "Step 1742 Loss:  tensor(0.0143)\n",
            "Step 1743 Loss:  tensor(0.0143)\n",
            "Step 1744 Loss:  tensor(0.0143)\n",
            "Step 1745 Loss:  tensor(0.0143)\n",
            "Step 1746 Loss:  tensor(0.0143)\n",
            "Step 1747 Loss:  tensor(0.0143)\n",
            "Step 1748 Loss:  tensor(0.0143)\n",
            "Step 1749 Loss:  tensor(0.0143)\n",
            "Step 1750 Loss:  tensor(0.0143)\n",
            "Step 1751 Loss:  tensor(0.0143)\n",
            "Step 1752 Loss:  tensor(0.0143)\n",
            "Step 1753 Loss:  tensor(0.0143)\n",
            "Step 1754 Loss:  tensor(0.0143)\n",
            "Step 1755 Loss:  tensor(0.0143)\n",
            "Step 1756 Loss:  tensor(0.0143)\n",
            "Step 1757 Loss:  tensor(0.0143)\n",
            "Step 1758 Loss:  tensor(0.0142)\n",
            "Step 1759 Loss:  tensor(0.0142)\n",
            "Step 1760 Loss:  tensor(0.0142)\n",
            "Step 1761 Loss:  tensor(0.0142)\n",
            "Step 1762 Loss:  tensor(0.0142)\n",
            "Step 1763 Loss:  tensor(0.0142)\n",
            "Step 1764 Loss:  tensor(0.0142)\n",
            "Step 1765 Loss:  tensor(0.0142)\n",
            "Step 1766 Loss:  tensor(0.0142)\n",
            "Step 1767 Loss:  tensor(0.0142)\n",
            "Step 1768 Loss:  tensor(0.0142)\n",
            "Step 1769 Loss:  tensor(0.0142)\n",
            "Step 1770 Loss:  tensor(0.0142)\n",
            "Step 1771 Loss:  tensor(0.0142)\n",
            "Step 1772 Loss:  tensor(0.0142)\n",
            "Step 1773 Loss:  tensor(0.0142)\n",
            "Step 1774 Loss:  tensor(0.0142)\n",
            "Step 1775 Loss:  tensor(0.0142)\n",
            "Step 1776 Loss:  tensor(0.0141)\n",
            "Step 1777 Loss:  tensor(0.0141)\n",
            "Step 1778 Loss:  tensor(0.0141)\n",
            "Step 1779 Loss:  tensor(0.0141)\n",
            "Step 1780 Loss:  tensor(0.0141)\n",
            "Step 1781 Loss:  tensor(0.0141)\n",
            "Step 1782 Loss:  tensor(0.0141)\n",
            "Step 1783 Loss:  tensor(0.0141)\n",
            "Step 1784 Loss:  tensor(0.0141)\n",
            "Step 1785 Loss:  tensor(0.0141)\n",
            "Step 1786 Loss:  tensor(0.0141)\n",
            "Step 1787 Loss:  tensor(0.0141)\n",
            "Step 1788 Loss:  tensor(0.0141)\n",
            "Step 1789 Loss:  tensor(0.0141)\n",
            "Step 1790 Loss:  tensor(0.0141)\n",
            "Step 1791 Loss:  tensor(0.0141)\n",
            "Step 1792 Loss:  tensor(0.0141)\n",
            "Step 1793 Loss:  tensor(0.0141)\n",
            "Step 1794 Loss:  tensor(0.0141)\n",
            "Step 1795 Loss:  tensor(0.0140)\n",
            "Step 1796 Loss:  tensor(0.0140)\n",
            "Step 1797 Loss:  tensor(0.0140)\n",
            "Step 1798 Loss:  tensor(0.0140)\n",
            "Step 1799 Loss:  tensor(0.0140)\n",
            "Step 1800 Loss:  tensor(0.0140)\n",
            "Step 1801 Loss:  tensor(0.0140)\n",
            "Step 1802 Loss:  tensor(0.0140)\n",
            "Step 1803 Loss:  tensor(0.0140)\n",
            "Step 1804 Loss:  tensor(0.0140)\n",
            "Step 1805 Loss:  tensor(0.0140)\n",
            "Step 1806 Loss:  tensor(0.0140)\n",
            "Step 1807 Loss:  tensor(0.0140)\n",
            "Step 1808 Loss:  tensor(0.0140)\n",
            "Step 1809 Loss:  tensor(0.0140)\n",
            "Step 1810 Loss:  tensor(0.0140)\n",
            "Step 1811 Loss:  tensor(0.0140)\n",
            "Step 1812 Loss:  tensor(0.0140)\n",
            "Step 1813 Loss:  tensor(0.0140)\n",
            "Step 1814 Loss:  tensor(0.0140)\n",
            "Step 1815 Loss:  tensor(0.0139)\n",
            "Step 1816 Loss:  tensor(0.0139)\n",
            "Step 1817 Loss:  tensor(0.0139)\n",
            "Step 1818 Loss:  tensor(0.0139)\n",
            "Step 1819 Loss:  tensor(0.0139)\n",
            "Step 1820 Loss:  tensor(0.0139)\n",
            "Step 1821 Loss:  tensor(0.0139)\n",
            "Step 1822 Loss:  tensor(0.0139)\n",
            "Step 1823 Loss:  tensor(0.0139)\n",
            "Step 1824 Loss:  tensor(0.0139)\n",
            "Step 1825 Loss:  tensor(0.0139)\n",
            "Step 1826 Loss:  tensor(0.0139)\n",
            "Step 1827 Loss:  tensor(0.0139)\n",
            "Step 1828 Loss:  tensor(0.0139)\n",
            "Step 1829 Loss:  tensor(0.0139)\n",
            "Step 1830 Loss:  tensor(0.0139)\n",
            "Step 1831 Loss:  tensor(0.0139)\n",
            "Step 1832 Loss:  tensor(0.0139)\n",
            "Step 1833 Loss:  tensor(0.0139)\n",
            "Step 1834 Loss:  tensor(0.0139)\n",
            "Step 1835 Loss:  tensor(0.0138)\n",
            "Step 1836 Loss:  tensor(0.0138)\n",
            "Step 1837 Loss:  tensor(0.0138)\n",
            "Step 1838 Loss:  tensor(0.0138)\n",
            "Step 1839 Loss:  tensor(0.0138)\n",
            "Step 1840 Loss:  tensor(0.0138)\n",
            "Step 1841 Loss:  tensor(0.0138)\n",
            "Step 1842 Loss:  tensor(0.0138)\n",
            "Step 1843 Loss:  tensor(0.0138)\n",
            "Step 1844 Loss:  tensor(0.0138)\n",
            "Step 1845 Loss:  tensor(0.0138)\n",
            "Step 1846 Loss:  tensor(0.0138)\n",
            "Step 1847 Loss:  tensor(0.0138)\n",
            "Step 1848 Loss:  tensor(0.0138)\n",
            "Step 1849 Loss:  tensor(0.0138)\n",
            "Step 1850 Loss:  tensor(0.0138)\n",
            "Step 1851 Loss:  tensor(0.0138)\n",
            "Step 1852 Loss:  tensor(0.0138)\n",
            "Step 1853 Loss:  tensor(0.0138)\n",
            "Step 1854 Loss:  tensor(0.0138)\n",
            "Step 1855 Loss:  tensor(0.0137)\n",
            "Step 1856 Loss:  tensor(0.0137)\n",
            "Step 1857 Loss:  tensor(0.0137)\n",
            "Step 1858 Loss:  tensor(0.0137)\n",
            "Step 1859 Loss:  tensor(0.0137)\n",
            "Step 1860 Loss:  tensor(0.0137)\n",
            "Step 1861 Loss:  tensor(0.0137)\n",
            "Step 1862 Loss:  tensor(0.0137)\n",
            "Step 1863 Loss:  tensor(0.0137)\n",
            "Step 1864 Loss:  tensor(0.0137)\n",
            "Step 1865 Loss:  tensor(0.0137)\n",
            "Step 1866 Loss:  tensor(0.0137)\n",
            "Step 1867 Loss:  tensor(0.0137)\n",
            "Step 1868 Loss:  tensor(0.0137)\n",
            "Step 1869 Loss:  tensor(0.0137)\n",
            "Step 1870 Loss:  tensor(0.0137)\n",
            "Step 1871 Loss:  tensor(0.0137)\n",
            "Step 1872 Loss:  tensor(0.0137)\n",
            "Step 1873 Loss:  tensor(0.0137)\n",
            "Step 1874 Loss:  tensor(0.0137)\n",
            "Step 1875 Loss:  tensor(0.0136)\n",
            "Step 1876 Loss:  tensor(0.0136)\n",
            "Step 1877 Loss:  tensor(0.0136)\n",
            "Step 1878 Loss:  tensor(0.0136)\n",
            "Step 1879 Loss:  tensor(0.0136)\n",
            "Step 1880 Loss:  tensor(0.0136)\n",
            "Step 1881 Loss:  tensor(0.0136)\n",
            "Step 1882 Loss:  tensor(0.0136)\n",
            "Step 1883 Loss:  tensor(0.0136)\n",
            "Step 1884 Loss:  tensor(0.0136)\n",
            "Step 1885 Loss:  tensor(0.0136)\n",
            "Step 1886 Loss:  tensor(0.0136)\n",
            "Step 1887 Loss:  tensor(0.0136)\n",
            "Step 1888 Loss:  tensor(0.0136)\n",
            "Step 1889 Loss:  tensor(0.0136)\n",
            "Step 1890 Loss:  tensor(0.0136)\n",
            "Step 1891 Loss:  tensor(0.0136)\n",
            "Step 1892 Loss:  tensor(0.0136)\n",
            "Step 1893 Loss:  tensor(0.0136)\n",
            "Step 1894 Loss:  tensor(0.0136)\n",
            "Step 1895 Loss:  tensor(0.0136)\n",
            "Step 1896 Loss:  tensor(0.0135)\n",
            "Step 1897 Loss:  tensor(0.0135)\n",
            "Step 1898 Loss:  tensor(0.0135)\n",
            "Step 1899 Loss:  tensor(0.0135)\n",
            "Step 1900 Loss:  tensor(0.0135)\n",
            "Step 1901 Loss:  tensor(0.0135)\n",
            "Step 1902 Loss:  tensor(0.0135)\n",
            "Step 1903 Loss:  tensor(0.0135)\n",
            "Step 1904 Loss:  tensor(0.0135)\n",
            "Step 1905 Loss:  tensor(0.0135)\n",
            "Step 1906 Loss:  tensor(0.0135)\n",
            "Step 1907 Loss:  tensor(0.0135)\n",
            "Step 1908 Loss:  tensor(0.0135)\n",
            "Step 1909 Loss:  tensor(0.0135)\n",
            "Step 1910 Loss:  tensor(0.0135)\n",
            "Step 1911 Loss:  tensor(0.0135)\n",
            "Step 1912 Loss:  tensor(0.0135)\n",
            "Step 1913 Loss:  tensor(0.0135)\n",
            "Step 1914 Loss:  tensor(0.0135)\n",
            "Step 1915 Loss:  tensor(0.0135)\n",
            "Step 1916 Loss:  tensor(0.0135)\n",
            "Step 1917 Loss:  tensor(0.0134)\n",
            "Step 1918 Loss:  tensor(0.0134)\n",
            "Step 1919 Loss:  tensor(0.0134)\n",
            "Step 1920 Loss:  tensor(0.0134)\n",
            "Step 1921 Loss:  tensor(0.0134)\n",
            "Step 1922 Loss:  tensor(0.0134)\n",
            "Step 1923 Loss:  tensor(0.0134)\n",
            "Step 1924 Loss:  tensor(0.0134)\n",
            "Step 1925 Loss:  tensor(0.0134)\n",
            "Step 1926 Loss:  tensor(0.0134)\n",
            "Step 1927 Loss:  tensor(0.0134)\n",
            "Step 1928 Loss:  tensor(0.0134)\n",
            "Step 1929 Loss:  tensor(0.0134)\n",
            "Step 1930 Loss:  tensor(0.0134)\n",
            "Step 1931 Loss:  tensor(0.0134)\n",
            "Step 1932 Loss:  tensor(0.0134)\n",
            "Step 1933 Loss:  tensor(0.0134)\n",
            "Step 1934 Loss:  tensor(0.0134)\n",
            "Step 1935 Loss:  tensor(0.0134)\n",
            "Step 1936 Loss:  tensor(0.0134)\n",
            "Step 1937 Loss:  tensor(0.0134)\n",
            "Step 1938 Loss:  tensor(0.0133)\n",
            "Step 1939 Loss:  tensor(0.0133)\n",
            "Step 1940 Loss:  tensor(0.0133)\n",
            "Step 1941 Loss:  tensor(0.0133)\n",
            "Step 1942 Loss:  tensor(0.0133)\n",
            "Step 1943 Loss:  tensor(0.0133)\n",
            "Step 1944 Loss:  tensor(0.0133)\n",
            "Step 1945 Loss:  tensor(0.0133)\n",
            "Step 1946 Loss:  tensor(0.0133)\n",
            "Step 1947 Loss:  tensor(0.0133)\n",
            "Step 1948 Loss:  tensor(0.0133)\n",
            "Step 1949 Loss:  tensor(0.0133)\n",
            "Step 1950 Loss:  tensor(0.0133)\n",
            "Step 1951 Loss:  tensor(0.0133)\n",
            "Step 1952 Loss:  tensor(0.0133)\n",
            "Step 1953 Loss:  tensor(0.0133)\n",
            "Step 1954 Loss:  tensor(0.0133)\n",
            "Step 1955 Loss:  tensor(0.0133)\n",
            "Step 1956 Loss:  tensor(0.0133)\n",
            "Step 1957 Loss:  tensor(0.0133)\n",
            "Step 1958 Loss:  tensor(0.0133)\n",
            "Step 1959 Loss:  tensor(0.0133)\n",
            "Step 1960 Loss:  tensor(0.0132)\n",
            "Step 1961 Loss:  tensor(0.0132)\n",
            "Step 1962 Loss:  tensor(0.0132)\n",
            "Step 1963 Loss:  tensor(0.0132)\n",
            "Step 1964 Loss:  tensor(0.0132)\n",
            "Step 1965 Loss:  tensor(0.0132)\n",
            "Step 1966 Loss:  tensor(0.0132)\n",
            "Step 1967 Loss:  tensor(0.0132)\n",
            "Step 1968 Loss:  tensor(0.0132)\n",
            "Step 1969 Loss:  tensor(0.0132)\n",
            "Step 1970 Loss:  tensor(0.0132)\n",
            "Step 1971 Loss:  tensor(0.0132)\n",
            "Step 1972 Loss:  tensor(0.0132)\n",
            "Step 1973 Loss:  tensor(0.0132)\n",
            "Step 1974 Loss:  tensor(0.0132)\n",
            "Step 1975 Loss:  tensor(0.0132)\n",
            "Step 1976 Loss:  tensor(0.0132)\n",
            "Step 1977 Loss:  tensor(0.0132)\n",
            "Step 1978 Loss:  tensor(0.0132)\n",
            "Step 1979 Loss:  tensor(0.0132)\n",
            "Step 1980 Loss:  tensor(0.0132)\n",
            "Step 1981 Loss:  tensor(0.0132)\n",
            "Step 1982 Loss:  tensor(0.0132)\n",
            "Step 1983 Loss:  tensor(0.0131)\n",
            "Step 1984 Loss:  tensor(0.0131)\n",
            "Step 1985 Loss:  tensor(0.0131)\n",
            "Step 1986 Loss:  tensor(0.0131)\n",
            "Step 1987 Loss:  tensor(0.0131)\n",
            "Step 1988 Loss:  tensor(0.0131)\n",
            "Step 1989 Loss:  tensor(0.0131)\n",
            "Step 1990 Loss:  tensor(0.0131)\n",
            "Step 1991 Loss:  tensor(0.0131)\n",
            "Step 1992 Loss:  tensor(0.0131)\n",
            "Step 1993 Loss:  tensor(0.0131)\n",
            "Step 1994 Loss:  tensor(0.0131)\n",
            "Step 1995 Loss:  tensor(0.0131)\n",
            "Step 1996 Loss:  tensor(0.0131)\n",
            "Step 1997 Loss:  tensor(0.0131)\n",
            "Step 1998 Loss:  tensor(0.0131)\n",
            "Step 1999 Loss:  tensor(0.0131)\n",
            "Step 2000 Loss:  tensor(0.0131)\n",
            "Step 2001 Loss:  tensor(0.0131)\n",
            "Step 2002 Loss:  tensor(0.0131)\n",
            "Step 2003 Loss:  tensor(0.0131)\n",
            "Step 2004 Loss:  tensor(0.0131)\n",
            "Step 2005 Loss:  tensor(0.0131)\n",
            "Step 2006 Loss:  tensor(0.0130)\n",
            "Step 2007 Loss:  tensor(0.0130)\n",
            "Step 2008 Loss:  tensor(0.0130)\n",
            "Step 2009 Loss:  tensor(0.0130)\n",
            "Step 2010 Loss:  tensor(0.0130)\n",
            "Step 2011 Loss:  tensor(0.0130)\n",
            "Step 2012 Loss:  tensor(0.0130)\n",
            "Step 2013 Loss:  tensor(0.0130)\n",
            "Step 2014 Loss:  tensor(0.0130)\n",
            "Step 2015 Loss:  tensor(0.0130)\n",
            "Step 2016 Loss:  tensor(0.0130)\n",
            "Step 2017 Loss:  tensor(0.0130)\n",
            "Step 2018 Loss:  tensor(0.0130)\n",
            "Step 2019 Loss:  tensor(0.0130)\n",
            "Step 2020 Loss:  tensor(0.0130)\n",
            "Step 2021 Loss:  tensor(0.0130)\n",
            "Step 2022 Loss:  tensor(0.0130)\n",
            "Step 2023 Loss:  tensor(0.0130)\n",
            "Step 2024 Loss:  tensor(0.0130)\n",
            "Step 2025 Loss:  tensor(0.0130)\n",
            "Step 2026 Loss:  tensor(0.0130)\n",
            "Step 2027 Loss:  tensor(0.0130)\n",
            "Step 2028 Loss:  tensor(0.0130)\n",
            "Step 2029 Loss:  tensor(0.0129)\n",
            "Step 2030 Loss:  tensor(0.0129)\n",
            "Step 2031 Loss:  tensor(0.0129)\n",
            "Step 2032 Loss:  tensor(0.0129)\n",
            "Step 2033 Loss:  tensor(0.0129)\n",
            "Step 2034 Loss:  tensor(0.0129)\n",
            "Step 2035 Loss:  tensor(0.0129)\n",
            "Step 2036 Loss:  tensor(0.0129)\n",
            "Step 2037 Loss:  tensor(0.0129)\n",
            "Step 2038 Loss:  tensor(0.0129)\n",
            "Step 2039 Loss:  tensor(0.0129)\n",
            "Step 2040 Loss:  tensor(0.0129)\n",
            "Step 2041 Loss:  tensor(0.0129)\n",
            "Step 2042 Loss:  tensor(0.0129)\n",
            "Step 2043 Loss:  tensor(0.0129)\n",
            "Step 2044 Loss:  tensor(0.0129)\n",
            "Step 2045 Loss:  tensor(0.0129)\n",
            "Step 2046 Loss:  tensor(0.0129)\n",
            "Step 2047 Loss:  tensor(0.0129)\n",
            "Step 2048 Loss:  tensor(0.0129)\n",
            "Step 2049 Loss:  tensor(0.0129)\n",
            "Step 2050 Loss:  tensor(0.0129)\n",
            "Step 2051 Loss:  tensor(0.0129)\n",
            "Step 2052 Loss:  tensor(0.0128)\n",
            "Step 2053 Loss:  tensor(0.0128)\n",
            "Step 2054 Loss:  tensor(0.0128)\n",
            "Step 2055 Loss:  tensor(0.0128)\n",
            "Step 2056 Loss:  tensor(0.0128)\n",
            "Step 2057 Loss:  tensor(0.0128)\n",
            "Step 2058 Loss:  tensor(0.0128)\n",
            "Step 2059 Loss:  tensor(0.0128)\n",
            "Step 2060 Loss:  tensor(0.0128)\n",
            "Step 2061 Loss:  tensor(0.0128)\n",
            "Step 2062 Loss:  tensor(0.0128)\n",
            "Step 2063 Loss:  tensor(0.0128)\n",
            "Step 2064 Loss:  tensor(0.0128)\n",
            "Step 2065 Loss:  tensor(0.0128)\n",
            "Step 2066 Loss:  tensor(0.0128)\n",
            "Step 2067 Loss:  tensor(0.0128)\n",
            "Step 2068 Loss:  tensor(0.0128)\n",
            "Step 2069 Loss:  tensor(0.0128)\n",
            "Step 2070 Loss:  tensor(0.0128)\n",
            "Step 2071 Loss:  tensor(0.0128)\n",
            "Step 2072 Loss:  tensor(0.0128)\n",
            "Step 2073 Loss:  tensor(0.0128)\n",
            "Step 2074 Loss:  tensor(0.0128)\n",
            "Step 2075 Loss:  tensor(0.0128)\n",
            "Step 2076 Loss:  tensor(0.0127)\n",
            "Step 2077 Loss:  tensor(0.0127)\n",
            "Step 2078 Loss:  tensor(0.0127)\n",
            "Step 2079 Loss:  tensor(0.0127)\n",
            "Step 2080 Loss:  tensor(0.0127)\n",
            "Step 2081 Loss:  tensor(0.0127)\n",
            "Step 2082 Loss:  tensor(0.0127)\n",
            "Step 2083 Loss:  tensor(0.0127)\n",
            "Step 2084 Loss:  tensor(0.0127)\n",
            "Step 2085 Loss:  tensor(0.0127)\n",
            "Step 2086 Loss:  tensor(0.0127)\n",
            "Step 2087 Loss:  tensor(0.0127)\n",
            "Step 2088 Loss:  tensor(0.0127)\n",
            "Step 2089 Loss:  tensor(0.0127)\n",
            "Step 2090 Loss:  tensor(0.0127)\n",
            "Step 2091 Loss:  tensor(0.0127)\n",
            "Step 2092 Loss:  tensor(0.0127)\n",
            "Step 2093 Loss:  tensor(0.0127)\n",
            "Step 2094 Loss:  tensor(0.0127)\n",
            "Step 2095 Loss:  tensor(0.0127)\n",
            "Step 2096 Loss:  tensor(0.0127)\n",
            "Step 2097 Loss:  tensor(0.0127)\n",
            "Step 2098 Loss:  tensor(0.0127)\n",
            "Step 2099 Loss:  tensor(0.0127)\n",
            "Step 2100 Loss:  tensor(0.0127)\n",
            "Step 2101 Loss:  tensor(0.0126)\n",
            "Step 2102 Loss:  tensor(0.0126)\n",
            "Step 2103 Loss:  tensor(0.0126)\n",
            "Step 2104 Loss:  tensor(0.0126)\n",
            "Step 2105 Loss:  tensor(0.0126)\n",
            "Step 2106 Loss:  tensor(0.0126)\n",
            "Step 2107 Loss:  tensor(0.0126)\n",
            "Step 2108 Loss:  tensor(0.0126)\n",
            "Step 2109 Loss:  tensor(0.0126)\n",
            "Step 2110 Loss:  tensor(0.0126)\n",
            "Step 2111 Loss:  tensor(0.0126)\n",
            "Step 2112 Loss:  tensor(0.0126)\n",
            "Step 2113 Loss:  tensor(0.0126)\n",
            "Step 2114 Loss:  tensor(0.0126)\n",
            "Step 2115 Loss:  tensor(0.0126)\n",
            "Step 2116 Loss:  tensor(0.0126)\n",
            "Step 2117 Loss:  tensor(0.0126)\n",
            "Step 2118 Loss:  tensor(0.0126)\n",
            "Step 2119 Loss:  tensor(0.0126)\n",
            "Step 2120 Loss:  tensor(0.0126)\n",
            "Step 2121 Loss:  tensor(0.0126)\n",
            "Step 2122 Loss:  tensor(0.0126)\n",
            "Step 2123 Loss:  tensor(0.0126)\n",
            "Step 2124 Loss:  tensor(0.0126)\n",
            "Step 2125 Loss:  tensor(0.0126)\n",
            "Step 2126 Loss:  tensor(0.0125)\n",
            "Step 2127 Loss:  tensor(0.0125)\n",
            "Step 2128 Loss:  tensor(0.0125)\n",
            "Step 2129 Loss:  tensor(0.0125)\n",
            "Step 2130 Loss:  tensor(0.0125)\n",
            "Step 2131 Loss:  tensor(0.0125)\n",
            "Step 2132 Loss:  tensor(0.0125)\n",
            "Step 2133 Loss:  tensor(0.0125)\n",
            "Step 2134 Loss:  tensor(0.0125)\n",
            "Step 2135 Loss:  tensor(0.0125)\n",
            "Step 2136 Loss:  tensor(0.0125)\n",
            "Step 2137 Loss:  tensor(0.0125)\n",
            "Step 2138 Loss:  tensor(0.0125)\n",
            "Step 2139 Loss:  tensor(0.0125)\n",
            "Step 2140 Loss:  tensor(0.0125)\n",
            "Step 2141 Loss:  tensor(0.0125)\n",
            "Step 2142 Loss:  tensor(0.0125)\n",
            "Step 2143 Loss:  tensor(0.0125)\n",
            "Step 2144 Loss:  tensor(0.0125)\n",
            "Step 2145 Loss:  tensor(0.0125)\n",
            "Step 2146 Loss:  tensor(0.0125)\n",
            "Step 2147 Loss:  tensor(0.0125)\n",
            "Step 2148 Loss:  tensor(0.0125)\n",
            "Step 2149 Loss:  tensor(0.0125)\n",
            "Step 2150 Loss:  tensor(0.0125)\n",
            "Step 2151 Loss:  tensor(0.0124)\n",
            "Step 2152 Loss:  tensor(0.0124)\n",
            "Step 2153 Loss:  tensor(0.0124)\n",
            "Step 2154 Loss:  tensor(0.0124)\n",
            "Step 2155 Loss:  tensor(0.0124)\n",
            "Step 2156 Loss:  tensor(0.0124)\n",
            "Step 2157 Loss:  tensor(0.0124)\n",
            "Step 2158 Loss:  tensor(0.0124)\n",
            "Step 2159 Loss:  tensor(0.0124)\n",
            "Step 2160 Loss:  tensor(0.0124)\n",
            "Step 2161 Loss:  tensor(0.0124)\n",
            "Step 2162 Loss:  tensor(0.0124)\n",
            "Step 2163 Loss:  tensor(0.0124)\n",
            "Step 2164 Loss:  tensor(0.0124)\n",
            "Step 2165 Loss:  tensor(0.0124)\n",
            "Step 2166 Loss:  tensor(0.0124)\n",
            "Step 2167 Loss:  tensor(0.0124)\n",
            "Step 2168 Loss:  tensor(0.0124)\n",
            "Step 2169 Loss:  tensor(0.0124)\n",
            "Step 2170 Loss:  tensor(0.0124)\n",
            "Step 2171 Loss:  tensor(0.0124)\n",
            "Step 2172 Loss:  tensor(0.0124)\n",
            "Step 2173 Loss:  tensor(0.0124)\n",
            "Step 2174 Loss:  tensor(0.0124)\n",
            "Step 2175 Loss:  tensor(0.0124)\n",
            "Step 2176 Loss:  tensor(0.0124)\n",
            "Step 2177 Loss:  tensor(0.0123)\n",
            "Step 2178 Loss:  tensor(0.0123)\n",
            "Step 2179 Loss:  tensor(0.0123)\n",
            "Step 2180 Loss:  tensor(0.0123)\n",
            "Step 2181 Loss:  tensor(0.0123)\n",
            "Step 2182 Loss:  tensor(0.0123)\n",
            "Step 2183 Loss:  tensor(0.0123)\n",
            "Step 2184 Loss:  tensor(0.0123)\n",
            "Step 2185 Loss:  tensor(0.0123)\n",
            "Step 2186 Loss:  tensor(0.0123)\n",
            "Step 2187 Loss:  tensor(0.0123)\n",
            "Step 2188 Loss:  tensor(0.0123)\n",
            "Step 2189 Loss:  tensor(0.0123)\n",
            "Step 2190 Loss:  tensor(0.0123)\n",
            "Step 2191 Loss:  tensor(0.0123)\n",
            "Step 2192 Loss:  tensor(0.0123)\n",
            "Step 2193 Loss:  tensor(0.0123)\n",
            "Step 2194 Loss:  tensor(0.0123)\n",
            "Step 2195 Loss:  tensor(0.0123)\n",
            "Step 2196 Loss:  tensor(0.0123)\n",
            "Step 2197 Loss:  tensor(0.0123)\n",
            "Step 2198 Loss:  tensor(0.0123)\n",
            "Step 2199 Loss:  tensor(0.0123)\n",
            "Step 2200 Loss:  tensor(0.0123)\n",
            "Step 2201 Loss:  tensor(0.0123)\n",
            "Step 2202 Loss:  tensor(0.0123)\n",
            "Step 2203 Loss:  tensor(0.0123)\n",
            "Step 2204 Loss:  tensor(0.0122)\n",
            "Step 2205 Loss:  tensor(0.0122)\n",
            "Step 2206 Loss:  tensor(0.0122)\n",
            "Step 2207 Loss:  tensor(0.0122)\n",
            "Step 2208 Loss:  tensor(0.0122)\n",
            "Step 2209 Loss:  tensor(0.0122)\n",
            "Step 2210 Loss:  tensor(0.0122)\n",
            "Step 2211 Loss:  tensor(0.0122)\n",
            "Step 2212 Loss:  tensor(0.0122)\n",
            "Step 2213 Loss:  tensor(0.0122)\n",
            "Step 2214 Loss:  tensor(0.0122)\n",
            "Step 2215 Loss:  tensor(0.0122)\n",
            "Step 2216 Loss:  tensor(0.0122)\n",
            "Step 2217 Loss:  tensor(0.0122)\n",
            "Step 2218 Loss:  tensor(0.0122)\n",
            "Step 2219 Loss:  tensor(0.0122)\n",
            "Step 2220 Loss:  tensor(0.0122)\n",
            "Step 2221 Loss:  tensor(0.0122)\n",
            "Step 2222 Loss:  tensor(0.0122)\n",
            "Step 2223 Loss:  tensor(0.0122)\n",
            "Step 2224 Loss:  tensor(0.0122)\n",
            "Step 2225 Loss:  tensor(0.0122)\n",
            "Step 2226 Loss:  tensor(0.0122)\n",
            "Step 2227 Loss:  tensor(0.0122)\n",
            "Step 2228 Loss:  tensor(0.0122)\n",
            "Step 2229 Loss:  tensor(0.0122)\n",
            "Step 2230 Loss:  tensor(0.0122)\n",
            "Step 2231 Loss:  tensor(0.0121)\n",
            "Step 2232 Loss:  tensor(0.0121)\n",
            "Step 2233 Loss:  tensor(0.0121)\n",
            "Step 2234 Loss:  tensor(0.0121)\n",
            "Step 2235 Loss:  tensor(0.0121)\n",
            "Step 2236 Loss:  tensor(0.0121)\n",
            "Step 2237 Loss:  tensor(0.0121)\n",
            "Step 2238 Loss:  tensor(0.0121)\n",
            "Step 2239 Loss:  tensor(0.0121)\n",
            "Step 2240 Loss:  tensor(0.0121)\n",
            "Step 2241 Loss:  tensor(0.0121)\n",
            "Step 2242 Loss:  tensor(0.0121)\n",
            "Step 2243 Loss:  tensor(0.0121)\n",
            "Step 2244 Loss:  tensor(0.0121)\n",
            "Step 2245 Loss:  tensor(0.0121)\n",
            "Step 2246 Loss:  tensor(0.0121)\n",
            "Step 2247 Loss:  tensor(0.0121)\n",
            "Step 2248 Loss:  tensor(0.0121)\n",
            "Step 2249 Loss:  tensor(0.0121)\n",
            "Step 2250 Loss:  tensor(0.0121)\n",
            "Step 2251 Loss:  tensor(0.0121)\n",
            "Step 2252 Loss:  tensor(0.0121)\n",
            "Step 2253 Loss:  tensor(0.0121)\n",
            "Step 2254 Loss:  tensor(0.0121)\n",
            "Step 2255 Loss:  tensor(0.0121)\n",
            "Step 2256 Loss:  tensor(0.0121)\n",
            "Step 2257 Loss:  tensor(0.0121)\n",
            "Step 2258 Loss:  tensor(0.0120)\n",
            "Step 2259 Loss:  tensor(0.0120)\n",
            "Step 2260 Loss:  tensor(0.0120)\n",
            "Step 2261 Loss:  tensor(0.0120)\n",
            "Step 2262 Loss:  tensor(0.0120)\n",
            "Step 2263 Loss:  tensor(0.0120)\n",
            "Step 2264 Loss:  tensor(0.0120)\n",
            "Step 2265 Loss:  tensor(0.0120)\n",
            "Step 2266 Loss:  tensor(0.0120)\n",
            "Step 2267 Loss:  tensor(0.0120)\n",
            "Step 2268 Loss:  tensor(0.0120)\n",
            "Step 2269 Loss:  tensor(0.0120)\n",
            "Step 2270 Loss:  tensor(0.0120)\n",
            "Step 2271 Loss:  tensor(0.0120)\n",
            "Step 2272 Loss:  tensor(0.0120)\n",
            "Step 2273 Loss:  tensor(0.0120)\n",
            "Step 2274 Loss:  tensor(0.0120)\n",
            "Step 2275 Loss:  tensor(0.0120)\n",
            "Step 2276 Loss:  tensor(0.0120)\n",
            "Step 2277 Loss:  tensor(0.0120)\n",
            "Step 2278 Loss:  tensor(0.0120)\n",
            "Step 2279 Loss:  tensor(0.0120)\n",
            "Step 2280 Loss:  tensor(0.0120)\n",
            "Step 2281 Loss:  tensor(0.0120)\n",
            "Step 2282 Loss:  tensor(0.0120)\n",
            "Step 2283 Loss:  tensor(0.0120)\n",
            "Step 2284 Loss:  tensor(0.0120)\n",
            "Step 2285 Loss:  tensor(0.0120)\n",
            "Step 2286 Loss:  tensor(0.0119)\n",
            "Step 2287 Loss:  tensor(0.0119)\n",
            "Step 2288 Loss:  tensor(0.0119)\n",
            "Step 2289 Loss:  tensor(0.0119)\n",
            "Step 2290 Loss:  tensor(0.0119)\n",
            "Step 2291 Loss:  tensor(0.0119)\n",
            "Step 2292 Loss:  tensor(0.0119)\n",
            "Step 2293 Loss:  tensor(0.0119)\n",
            "Step 2294 Loss:  tensor(0.0119)\n",
            "Step 2295 Loss:  tensor(0.0119)\n",
            "Step 2296 Loss:  tensor(0.0119)\n",
            "Step 2297 Loss:  tensor(0.0119)\n",
            "Step 2298 Loss:  tensor(0.0119)\n",
            "Step 2299 Loss:  tensor(0.0119)\n",
            "Step 2300 Loss:  tensor(0.0119)\n",
            "Step 2301 Loss:  tensor(0.0119)\n",
            "Step 2302 Loss:  tensor(0.0119)\n",
            "Step 2303 Loss:  tensor(0.0119)\n",
            "Step 2304 Loss:  tensor(0.0119)\n",
            "Step 2305 Loss:  tensor(0.0119)\n",
            "Step 2306 Loss:  tensor(0.0119)\n",
            "Step 2307 Loss:  tensor(0.0119)\n",
            "Step 2308 Loss:  tensor(0.0119)\n",
            "Step 2309 Loss:  tensor(0.0119)\n",
            "Step 2310 Loss:  tensor(0.0119)\n",
            "Step 2311 Loss:  tensor(0.0119)\n",
            "Step 2312 Loss:  tensor(0.0119)\n",
            "Step 2313 Loss:  tensor(0.0119)\n",
            "Step 2314 Loss:  tensor(0.0119)\n",
            "Step 2315 Loss:  tensor(0.0118)\n",
            "Step 2316 Loss:  tensor(0.0118)\n",
            "Step 2317 Loss:  tensor(0.0118)\n",
            "Step 2318 Loss:  tensor(0.0118)\n",
            "Step 2319 Loss:  tensor(0.0118)\n",
            "Step 2320 Loss:  tensor(0.0118)\n",
            "Step 2321 Loss:  tensor(0.0118)\n",
            "Step 2322 Loss:  tensor(0.0118)\n",
            "Step 2323 Loss:  tensor(0.0118)\n",
            "Step 2324 Loss:  tensor(0.0118)\n",
            "Step 2325 Loss:  tensor(0.0118)\n",
            "Step 2326 Loss:  tensor(0.0118)\n",
            "Step 2327 Loss:  tensor(0.0118)\n",
            "Step 2328 Loss:  tensor(0.0118)\n",
            "Step 2329 Loss:  tensor(0.0118)\n",
            "Step 2330 Loss:  tensor(0.0118)\n",
            "Step 2331 Loss:  tensor(0.0118)\n",
            "Step 2332 Loss:  tensor(0.0118)\n",
            "Step 2333 Loss:  tensor(0.0118)\n",
            "Step 2334 Loss:  tensor(0.0118)\n",
            "Step 2335 Loss:  tensor(0.0118)\n",
            "Step 2336 Loss:  tensor(0.0118)\n",
            "Step 2337 Loss:  tensor(0.0118)\n",
            "Step 2338 Loss:  tensor(0.0118)\n",
            "Step 2339 Loss:  tensor(0.0118)\n",
            "Step 2340 Loss:  tensor(0.0118)\n",
            "Step 2341 Loss:  tensor(0.0118)\n",
            "Step 2342 Loss:  tensor(0.0118)\n",
            "Step 2343 Loss:  tensor(0.0118)\n",
            "Step 2344 Loss:  tensor(0.0117)\n",
            "Step 2345 Loss:  tensor(0.0117)\n",
            "Step 2346 Loss:  tensor(0.0117)\n",
            "Step 2347 Loss:  tensor(0.0117)\n",
            "Step 2348 Loss:  tensor(0.0117)\n",
            "Step 2349 Loss:  tensor(0.0117)\n",
            "Step 2350 Loss:  tensor(0.0117)\n",
            "Step 2351 Loss:  tensor(0.0117)\n",
            "Step 2352 Loss:  tensor(0.0117)\n",
            "Step 2353 Loss:  tensor(0.0117)\n",
            "Step 2354 Loss:  tensor(0.0117)\n",
            "Step 2355 Loss:  tensor(0.0117)\n",
            "Step 2356 Loss:  tensor(0.0117)\n",
            "Step 2357 Loss:  tensor(0.0117)\n",
            "Step 2358 Loss:  tensor(0.0117)\n",
            "Step 2359 Loss:  tensor(0.0117)\n",
            "Step 2360 Loss:  tensor(0.0117)\n",
            "Step 2361 Loss:  tensor(0.0117)\n",
            "Step 2362 Loss:  tensor(0.0117)\n",
            "Step 2363 Loss:  tensor(0.0117)\n",
            "Step 2364 Loss:  tensor(0.0117)\n",
            "Step 2365 Loss:  tensor(0.0117)\n",
            "Step 2366 Loss:  tensor(0.0117)\n",
            "Step 2367 Loss:  tensor(0.0117)\n",
            "Step 2368 Loss:  tensor(0.0117)\n",
            "Step 2369 Loss:  tensor(0.0117)\n",
            "Step 2370 Loss:  tensor(0.0117)\n",
            "Step 2371 Loss:  tensor(0.0117)\n",
            "Step 2372 Loss:  tensor(0.0117)\n",
            "Step 2373 Loss:  tensor(0.0117)\n",
            "Step 2374 Loss:  tensor(0.0116)\n",
            "Step 2375 Loss:  tensor(0.0116)\n",
            "Step 2376 Loss:  tensor(0.0116)\n",
            "Step 2377 Loss:  tensor(0.0116)\n",
            "Step 2378 Loss:  tensor(0.0116)\n",
            "Step 2379 Loss:  tensor(0.0116)\n",
            "Step 2380 Loss:  tensor(0.0116)\n",
            "Step 2381 Loss:  tensor(0.0116)\n",
            "Step 2382 Loss:  tensor(0.0116)\n",
            "Step 2383 Loss:  tensor(0.0116)\n",
            "Step 2384 Loss:  tensor(0.0116)\n",
            "Step 2385 Loss:  tensor(0.0116)\n",
            "Step 2386 Loss:  tensor(0.0116)\n",
            "Step 2387 Loss:  tensor(0.0116)\n",
            "Step 2388 Loss:  tensor(0.0116)\n",
            "Step 2389 Loss:  tensor(0.0116)\n",
            "Step 2390 Loss:  tensor(0.0116)\n",
            "Step 2391 Loss:  tensor(0.0116)\n",
            "Step 2392 Loss:  tensor(0.0116)\n",
            "Step 2393 Loss:  tensor(0.0116)\n",
            "Step 2394 Loss:  tensor(0.0116)\n",
            "Step 2395 Loss:  tensor(0.0116)\n",
            "Step 2396 Loss:  tensor(0.0116)\n",
            "Step 2397 Loss:  tensor(0.0116)\n",
            "Step 2398 Loss:  tensor(0.0116)\n",
            "Step 2399 Loss:  tensor(0.0116)\n",
            "Step 2400 Loss:  tensor(0.0116)\n",
            "Step 2401 Loss:  tensor(0.0116)\n",
            "Step 2402 Loss:  tensor(0.0116)\n",
            "Step 2403 Loss:  tensor(0.0116)\n",
            "Step 2404 Loss:  tensor(0.0115)\n",
            "Step 2405 Loss:  tensor(0.0115)\n",
            "Step 2406 Loss:  tensor(0.0115)\n",
            "Step 2407 Loss:  tensor(0.0115)\n",
            "Step 2408 Loss:  tensor(0.0115)\n",
            "Step 2409 Loss:  tensor(0.0115)\n",
            "Step 2410 Loss:  tensor(0.0115)\n",
            "Step 2411 Loss:  tensor(0.0115)\n",
            "Step 2412 Loss:  tensor(0.0115)\n",
            "Step 2413 Loss:  tensor(0.0115)\n",
            "Step 2414 Loss:  tensor(0.0115)\n",
            "Step 2415 Loss:  tensor(0.0115)\n",
            "Step 2416 Loss:  tensor(0.0115)\n",
            "Step 2417 Loss:  tensor(0.0115)\n",
            "Step 2418 Loss:  tensor(0.0115)\n",
            "Step 2419 Loss:  tensor(0.0115)\n",
            "Step 2420 Loss:  tensor(0.0115)\n",
            "Step 2421 Loss:  tensor(0.0115)\n",
            "Step 2422 Loss:  tensor(0.0115)\n",
            "Step 2423 Loss:  tensor(0.0115)\n",
            "Step 2424 Loss:  tensor(0.0115)\n",
            "Step 2425 Loss:  tensor(0.0115)\n",
            "Step 2426 Loss:  tensor(0.0115)\n",
            "Step 2427 Loss:  tensor(0.0115)\n",
            "Step 2428 Loss:  tensor(0.0115)\n",
            "Step 2429 Loss:  tensor(0.0115)\n",
            "Step 2430 Loss:  tensor(0.0115)\n",
            "Step 2431 Loss:  tensor(0.0115)\n",
            "Step 2432 Loss:  tensor(0.0115)\n",
            "Step 2433 Loss:  tensor(0.0115)\n",
            "Step 2434 Loss:  tensor(0.0115)\n",
            "Step 2435 Loss:  tensor(0.0114)\n",
            "Step 2436 Loss:  tensor(0.0114)\n",
            "Step 2437 Loss:  tensor(0.0114)\n",
            "Step 2438 Loss:  tensor(0.0114)\n",
            "Step 2439 Loss:  tensor(0.0114)\n",
            "Step 2440 Loss:  tensor(0.0114)\n",
            "Step 2441 Loss:  tensor(0.0114)\n",
            "Step 2442 Loss:  tensor(0.0114)\n",
            "Step 2443 Loss:  tensor(0.0114)\n",
            "Step 2444 Loss:  tensor(0.0114)\n",
            "Step 2445 Loss:  tensor(0.0114)\n",
            "Step 2446 Loss:  tensor(0.0114)\n",
            "Step 2447 Loss:  tensor(0.0114)\n",
            "Step 2448 Loss:  tensor(0.0114)\n",
            "Step 2449 Loss:  tensor(0.0114)\n",
            "Step 2450 Loss:  tensor(0.0114)\n",
            "Step 2451 Loss:  tensor(0.0114)\n",
            "Step 2452 Loss:  tensor(0.0114)\n",
            "Step 2453 Loss:  tensor(0.0114)\n",
            "Step 2454 Loss:  tensor(0.0114)\n",
            "Step 2455 Loss:  tensor(0.0114)\n",
            "Step 2456 Loss:  tensor(0.0114)\n",
            "Step 2457 Loss:  tensor(0.0114)\n",
            "Step 2458 Loss:  tensor(0.0114)\n",
            "Step 2459 Loss:  tensor(0.0114)\n",
            "Step 2460 Loss:  tensor(0.0114)\n",
            "Step 2461 Loss:  tensor(0.0114)\n",
            "Step 2462 Loss:  tensor(0.0114)\n",
            "Step 2463 Loss:  tensor(0.0114)\n",
            "Step 2464 Loss:  tensor(0.0114)\n",
            "Step 2465 Loss:  tensor(0.0114)\n",
            "Step 2466 Loss:  tensor(0.0114)\n",
            "Step 2467 Loss:  tensor(0.0113)\n",
            "Step 2468 Loss:  tensor(0.0113)\n",
            "Step 2469 Loss:  tensor(0.0113)\n",
            "Step 2470 Loss:  tensor(0.0113)\n",
            "Step 2471 Loss:  tensor(0.0113)\n",
            "Step 2472 Loss:  tensor(0.0113)\n",
            "Step 2473 Loss:  tensor(0.0113)\n",
            "Step 2474 Loss:  tensor(0.0113)\n",
            "Step 2475 Loss:  tensor(0.0113)\n",
            "Step 2476 Loss:  tensor(0.0113)\n",
            "Step 2477 Loss:  tensor(0.0113)\n",
            "Step 2478 Loss:  tensor(0.0113)\n",
            "Step 2479 Loss:  tensor(0.0113)\n",
            "Step 2480 Loss:  tensor(0.0113)\n",
            "Step 2481 Loss:  tensor(0.0113)\n",
            "Step 2482 Loss:  tensor(0.0113)\n",
            "Step 2483 Loss:  tensor(0.0113)\n",
            "Step 2484 Loss:  tensor(0.0113)\n",
            "Step 2485 Loss:  tensor(0.0113)\n",
            "Step 2486 Loss:  tensor(0.0113)\n",
            "Step 2487 Loss:  tensor(0.0113)\n",
            "Step 2488 Loss:  tensor(0.0113)\n",
            "Step 2489 Loss:  tensor(0.0113)\n",
            "Step 2490 Loss:  tensor(0.0113)\n",
            "Step 2491 Loss:  tensor(0.0113)\n",
            "Step 2492 Loss:  tensor(0.0113)\n",
            "Step 2493 Loss:  tensor(0.0113)\n",
            "Step 2494 Loss:  tensor(0.0113)\n",
            "Step 2495 Loss:  tensor(0.0113)\n",
            "Step 2496 Loss:  tensor(0.0113)\n",
            "Step 2497 Loss:  tensor(0.0113)\n",
            "Step 2498 Loss:  tensor(0.0113)\n",
            "Step 2499 Loss:  tensor(0.0112)\n",
            "Step 2500 Loss:  tensor(0.0112)\n",
            "Step 2501 Loss:  tensor(0.0112)\n",
            "Step 2502 Loss:  tensor(0.0112)\n",
            "Step 2503 Loss:  tensor(0.0112)\n",
            "Step 2504 Loss:  tensor(0.0112)\n",
            "Step 2505 Loss:  tensor(0.0112)\n",
            "Step 2506 Loss:  tensor(0.0112)\n",
            "Step 2507 Loss:  tensor(0.0112)\n",
            "Step 2508 Loss:  tensor(0.0112)\n",
            "Step 2509 Loss:  tensor(0.0112)\n",
            "Step 2510 Loss:  tensor(0.0112)\n",
            "Step 2511 Loss:  tensor(0.0112)\n",
            "Step 2512 Loss:  tensor(0.0112)\n",
            "Step 2513 Loss:  tensor(0.0112)\n",
            "Step 2514 Loss:  tensor(0.0112)\n",
            "Step 2515 Loss:  tensor(0.0112)\n",
            "Step 2516 Loss:  tensor(0.0112)\n",
            "Step 2517 Loss:  tensor(0.0112)\n",
            "Step 2518 Loss:  tensor(0.0112)\n",
            "Step 2519 Loss:  tensor(0.0112)\n",
            "Step 2520 Loss:  tensor(0.0112)\n",
            "Step 2521 Loss:  tensor(0.0112)\n",
            "Step 2522 Loss:  tensor(0.0112)\n",
            "Step 2523 Loss:  tensor(0.0112)\n",
            "Step 2524 Loss:  tensor(0.0112)\n",
            "Step 2525 Loss:  tensor(0.0112)\n",
            "Step 2526 Loss:  tensor(0.0112)\n",
            "Step 2527 Loss:  tensor(0.0112)\n",
            "Step 2528 Loss:  tensor(0.0112)\n",
            "Step 2529 Loss:  tensor(0.0112)\n",
            "Step 2530 Loss:  tensor(0.0112)\n",
            "Step 2531 Loss:  tensor(0.0112)\n",
            "Step 2532 Loss:  tensor(0.0111)\n",
            "Step 2533 Loss:  tensor(0.0111)\n",
            "Step 2534 Loss:  tensor(0.0111)\n",
            "Step 2535 Loss:  tensor(0.0111)\n",
            "Step 2536 Loss:  tensor(0.0111)\n",
            "Step 2537 Loss:  tensor(0.0111)\n",
            "Step 2538 Loss:  tensor(0.0111)\n",
            "Step 2539 Loss:  tensor(0.0111)\n",
            "Step 2540 Loss:  tensor(0.0111)\n",
            "Step 2541 Loss:  tensor(0.0111)\n",
            "Step 2542 Loss:  tensor(0.0111)\n",
            "Step 2543 Loss:  tensor(0.0111)\n",
            "Step 2544 Loss:  tensor(0.0111)\n",
            "Step 2545 Loss:  tensor(0.0111)\n",
            "Step 2546 Loss:  tensor(0.0111)\n",
            "Step 2547 Loss:  tensor(0.0111)\n",
            "Step 2548 Loss:  tensor(0.0111)\n",
            "Step 2549 Loss:  tensor(0.0111)\n",
            "Step 2550 Loss:  tensor(0.0111)\n",
            "Step 2551 Loss:  tensor(0.0111)\n",
            "Step 2552 Loss:  tensor(0.0111)\n",
            "Step 2553 Loss:  tensor(0.0111)\n",
            "Step 2554 Loss:  tensor(0.0111)\n",
            "Step 2555 Loss:  tensor(0.0111)\n",
            "Step 2556 Loss:  tensor(0.0111)\n",
            "Step 2557 Loss:  tensor(0.0111)\n",
            "Step 2558 Loss:  tensor(0.0111)\n",
            "Step 2559 Loss:  tensor(0.0111)\n",
            "Step 2560 Loss:  tensor(0.0111)\n",
            "Step 2561 Loss:  tensor(0.0111)\n",
            "Step 2562 Loss:  tensor(0.0111)\n",
            "Step 2563 Loss:  tensor(0.0111)\n",
            "Step 2564 Loss:  tensor(0.0111)\n",
            "Step 2565 Loss:  tensor(0.0111)\n",
            "Step 2566 Loss:  tensor(0.0110)\n",
            "Step 2567 Loss:  tensor(0.0110)\n",
            "Step 2568 Loss:  tensor(0.0110)\n",
            "Step 2569 Loss:  tensor(0.0110)\n",
            "Step 2570 Loss:  tensor(0.0110)\n",
            "Step 2571 Loss:  tensor(0.0110)\n",
            "Step 2572 Loss:  tensor(0.0110)\n",
            "Step 2573 Loss:  tensor(0.0110)\n",
            "Step 2574 Loss:  tensor(0.0110)\n",
            "Step 2575 Loss:  tensor(0.0110)\n",
            "Step 2576 Loss:  tensor(0.0110)\n",
            "Step 2577 Loss:  tensor(0.0110)\n",
            "Step 2578 Loss:  tensor(0.0110)\n",
            "Step 2579 Loss:  tensor(0.0110)\n",
            "Step 2580 Loss:  tensor(0.0110)\n",
            "Step 2581 Loss:  tensor(0.0110)\n",
            "Step 2582 Loss:  tensor(0.0110)\n",
            "Step 2583 Loss:  tensor(0.0110)\n",
            "Step 2584 Loss:  tensor(0.0110)\n",
            "Step 2585 Loss:  tensor(0.0110)\n",
            "Step 2586 Loss:  tensor(0.0110)\n",
            "Step 2587 Loss:  tensor(0.0110)\n",
            "Step 2588 Loss:  tensor(0.0110)\n",
            "Step 2589 Loss:  tensor(0.0110)\n",
            "Step 2590 Loss:  tensor(0.0110)\n",
            "Step 2591 Loss:  tensor(0.0110)\n",
            "Step 2592 Loss:  tensor(0.0110)\n",
            "Step 2593 Loss:  tensor(0.0110)\n",
            "Step 2594 Loss:  tensor(0.0110)\n",
            "Step 2595 Loss:  tensor(0.0110)\n",
            "Step 2596 Loss:  tensor(0.0110)\n",
            "Step 2597 Loss:  tensor(0.0110)\n",
            "Step 2598 Loss:  tensor(0.0110)\n",
            "Step 2599 Loss:  tensor(0.0110)\n",
            "Step 2600 Loss:  tensor(0.0109)\n",
            "Step 2601 Loss:  tensor(0.0109)\n",
            "Step 2602 Loss:  tensor(0.0109)\n",
            "Step 2603 Loss:  tensor(0.0109)\n",
            "Step 2604 Loss:  tensor(0.0109)\n",
            "Step 2605 Loss:  tensor(0.0109)\n",
            "Step 2606 Loss:  tensor(0.0109)\n",
            "Step 2607 Loss:  tensor(0.0109)\n",
            "Step 2608 Loss:  tensor(0.0109)\n",
            "Step 2609 Loss:  tensor(0.0109)\n",
            "Step 2610 Loss:  tensor(0.0109)\n",
            "Step 2611 Loss:  tensor(0.0109)\n",
            "Step 2612 Loss:  tensor(0.0109)\n",
            "Step 2613 Loss:  tensor(0.0109)\n",
            "Step 2614 Loss:  tensor(0.0109)\n",
            "Step 2615 Loss:  tensor(0.0109)\n",
            "Step 2616 Loss:  tensor(0.0109)\n",
            "Step 2617 Loss:  tensor(0.0109)\n",
            "Step 2618 Loss:  tensor(0.0109)\n",
            "Step 2619 Loss:  tensor(0.0109)\n",
            "Step 2620 Loss:  tensor(0.0109)\n",
            "Step 2621 Loss:  tensor(0.0109)\n",
            "Step 2622 Loss:  tensor(0.0109)\n",
            "Step 2623 Loss:  tensor(0.0109)\n",
            "Step 2624 Loss:  tensor(0.0109)\n",
            "Step 2625 Loss:  tensor(0.0109)\n",
            "Step 2626 Loss:  tensor(0.0109)\n",
            "Step 2627 Loss:  tensor(0.0109)\n",
            "Step 2628 Loss:  tensor(0.0109)\n",
            "Step 2629 Loss:  tensor(0.0109)\n",
            "Step 2630 Loss:  tensor(0.0109)\n",
            "Step 2631 Loss:  tensor(0.0109)\n",
            "Step 2632 Loss:  tensor(0.0109)\n",
            "Step 2633 Loss:  tensor(0.0109)\n",
            "Step 2634 Loss:  tensor(0.0109)\n",
            "Step 2635 Loss:  tensor(0.0109)\n",
            "Step 2636 Loss:  tensor(0.0108)\n",
            "Step 2637 Loss:  tensor(0.0108)\n",
            "Step 2638 Loss:  tensor(0.0108)\n",
            "Step 2639 Loss:  tensor(0.0108)\n",
            "Step 2640 Loss:  tensor(0.0108)\n",
            "Step 2641 Loss:  tensor(0.0108)\n",
            "Step 2642 Loss:  tensor(0.0108)\n",
            "Step 2643 Loss:  tensor(0.0108)\n",
            "Step 2644 Loss:  tensor(0.0108)\n",
            "Step 2645 Loss:  tensor(0.0108)\n",
            "Step 2646 Loss:  tensor(0.0108)\n",
            "Step 2647 Loss:  tensor(0.0108)\n",
            "Step 2648 Loss:  tensor(0.0108)\n",
            "Step 2649 Loss:  tensor(0.0108)\n",
            "Step 2650 Loss:  tensor(0.0108)\n",
            "Step 2651 Loss:  tensor(0.0108)\n",
            "Step 2652 Loss:  tensor(0.0108)\n",
            "Step 2653 Loss:  tensor(0.0108)\n",
            "Step 2654 Loss:  tensor(0.0108)\n",
            "Step 2655 Loss:  tensor(0.0108)\n",
            "Step 2656 Loss:  tensor(0.0108)\n",
            "Step 2657 Loss:  tensor(0.0108)\n",
            "Step 2658 Loss:  tensor(0.0108)\n",
            "Step 2659 Loss:  tensor(0.0108)\n",
            "Step 2660 Loss:  tensor(0.0108)\n",
            "Step 2661 Loss:  tensor(0.0108)\n",
            "Step 2662 Loss:  tensor(0.0108)\n",
            "Step 2663 Loss:  tensor(0.0108)\n",
            "Step 2664 Loss:  tensor(0.0108)\n",
            "Step 2665 Loss:  tensor(0.0108)\n",
            "Step 2666 Loss:  tensor(0.0108)\n",
            "Step 2667 Loss:  tensor(0.0108)\n",
            "Step 2668 Loss:  tensor(0.0108)\n",
            "Step 2669 Loss:  tensor(0.0108)\n",
            "Step 2670 Loss:  tensor(0.0108)\n",
            "Step 2671 Loss:  tensor(0.0108)\n",
            "Step 2672 Loss:  tensor(0.0107)\n",
            "Step 2673 Loss:  tensor(0.0107)\n",
            "Step 2674 Loss:  tensor(0.0107)\n",
            "Step 2675 Loss:  tensor(0.0107)\n",
            "Step 2676 Loss:  tensor(0.0107)\n",
            "Step 2677 Loss:  tensor(0.0107)\n",
            "Step 2678 Loss:  tensor(0.0107)\n",
            "Step 2679 Loss:  tensor(0.0107)\n",
            "Step 2680 Loss:  tensor(0.0107)\n",
            "Step 2681 Loss:  tensor(0.0107)\n",
            "Step 2682 Loss:  tensor(0.0107)\n",
            "Step 2683 Loss:  tensor(0.0107)\n",
            "Step 2684 Loss:  tensor(0.0107)\n",
            "Step 2685 Loss:  tensor(0.0107)\n",
            "Step 2686 Loss:  tensor(0.0107)\n",
            "Step 2687 Loss:  tensor(0.0107)\n",
            "Step 2688 Loss:  tensor(0.0107)\n",
            "Step 2689 Loss:  tensor(0.0107)\n",
            "Step 2690 Loss:  tensor(0.0107)\n",
            "Step 2691 Loss:  tensor(0.0107)\n",
            "Step 2692 Loss:  tensor(0.0107)\n",
            "Step 2693 Loss:  tensor(0.0107)\n",
            "Step 2694 Loss:  tensor(0.0107)\n",
            "Step 2695 Loss:  tensor(0.0107)\n",
            "Step 2696 Loss:  tensor(0.0107)\n",
            "Step 2697 Loss:  tensor(0.0107)\n",
            "Step 2698 Loss:  tensor(0.0107)\n",
            "Step 2699 Loss:  tensor(0.0107)\n",
            "Step 2700 Loss:  tensor(0.0107)\n",
            "Step 2701 Loss:  tensor(0.0107)\n",
            "Step 2702 Loss:  tensor(0.0107)\n",
            "Step 2703 Loss:  tensor(0.0107)\n",
            "Step 2704 Loss:  tensor(0.0107)\n",
            "Step 2705 Loss:  tensor(0.0107)\n",
            "Step 2706 Loss:  tensor(0.0107)\n",
            "Step 2707 Loss:  tensor(0.0107)\n",
            "Step 2708 Loss:  tensor(0.0106)\n",
            "Step 2709 Loss:  tensor(0.0106)\n",
            "Step 2710 Loss:  tensor(0.0106)\n",
            "Step 2711 Loss:  tensor(0.0106)\n",
            "Step 2712 Loss:  tensor(0.0106)\n",
            "Step 2713 Loss:  tensor(0.0106)\n",
            "Step 2714 Loss:  tensor(0.0106)\n",
            "Step 2715 Loss:  tensor(0.0106)\n",
            "Step 2716 Loss:  tensor(0.0106)\n",
            "Step 2717 Loss:  tensor(0.0106)\n",
            "Step 2718 Loss:  tensor(0.0106)\n",
            "Step 2719 Loss:  tensor(0.0106)\n",
            "Step 2720 Loss:  tensor(0.0106)\n",
            "Step 2721 Loss:  tensor(0.0106)\n",
            "Step 2722 Loss:  tensor(0.0106)\n",
            "Step 2723 Loss:  tensor(0.0106)\n",
            "Step 2724 Loss:  tensor(0.0106)\n",
            "Step 2725 Loss:  tensor(0.0106)\n",
            "Step 2726 Loss:  tensor(0.0106)\n",
            "Step 2727 Loss:  tensor(0.0106)\n",
            "Step 2728 Loss:  tensor(0.0106)\n",
            "Step 2729 Loss:  tensor(0.0106)\n",
            "Step 2730 Loss:  tensor(0.0106)\n",
            "Step 2731 Loss:  tensor(0.0106)\n",
            "Step 2732 Loss:  tensor(0.0106)\n",
            "Step 2733 Loss:  tensor(0.0106)\n",
            "Step 2734 Loss:  tensor(0.0106)\n",
            "Step 2735 Loss:  tensor(0.0106)\n",
            "Step 2736 Loss:  tensor(0.0106)\n",
            "Step 2737 Loss:  tensor(0.0106)\n",
            "Step 2738 Loss:  tensor(0.0106)\n",
            "Step 2739 Loss:  tensor(0.0106)\n",
            "Step 2740 Loss:  tensor(0.0106)\n",
            "Step 2741 Loss:  tensor(0.0106)\n",
            "Step 2742 Loss:  tensor(0.0106)\n",
            "Step 2743 Loss:  tensor(0.0106)\n",
            "Step 2744 Loss:  tensor(0.0106)\n",
            "Step 2745 Loss:  tensor(0.0106)\n",
            "Step 2746 Loss:  tensor(0.0105)\n",
            "Step 2747 Loss:  tensor(0.0105)\n",
            "Step 2748 Loss:  tensor(0.0105)\n",
            "Step 2749 Loss:  tensor(0.0105)\n",
            "Step 2750 Loss:  tensor(0.0105)\n",
            "Step 2751 Loss:  tensor(0.0105)\n",
            "Step 2752 Loss:  tensor(0.0105)\n",
            "Step 2753 Loss:  tensor(0.0105)\n",
            "Step 2754 Loss:  tensor(0.0105)\n",
            "Step 2755 Loss:  tensor(0.0105)\n",
            "Step 2756 Loss:  tensor(0.0105)\n",
            "Step 2757 Loss:  tensor(0.0105)\n",
            "Step 2758 Loss:  tensor(0.0105)\n",
            "Step 2759 Loss:  tensor(0.0105)\n",
            "Step 2760 Loss:  tensor(0.0105)\n",
            "Step 2761 Loss:  tensor(0.0105)\n",
            "Step 2762 Loss:  tensor(0.0105)\n",
            "Step 2763 Loss:  tensor(0.0105)\n",
            "Step 2764 Loss:  tensor(0.0105)\n",
            "Step 2765 Loss:  tensor(0.0105)\n",
            "Step 2766 Loss:  tensor(0.0105)\n",
            "Step 2767 Loss:  tensor(0.0105)\n",
            "Step 2768 Loss:  tensor(0.0105)\n",
            "Step 2769 Loss:  tensor(0.0105)\n",
            "Step 2770 Loss:  tensor(0.0105)\n",
            "Step 2771 Loss:  tensor(0.0105)\n",
            "Step 2772 Loss:  tensor(0.0105)\n",
            "Step 2773 Loss:  tensor(0.0105)\n",
            "Step 2774 Loss:  tensor(0.0105)\n",
            "Step 2775 Loss:  tensor(0.0105)\n",
            "Step 2776 Loss:  tensor(0.0105)\n",
            "Step 2777 Loss:  tensor(0.0105)\n",
            "Step 2778 Loss:  tensor(0.0105)\n",
            "Step 2779 Loss:  tensor(0.0105)\n",
            "Step 2780 Loss:  tensor(0.0105)\n",
            "Step 2781 Loss:  tensor(0.0105)\n",
            "Step 2782 Loss:  tensor(0.0105)\n",
            "Step 2783 Loss:  tensor(0.0105)\n",
            "Step 2784 Loss:  tensor(0.0105)\n",
            "Step 2785 Loss:  tensor(0.0104)\n",
            "Step 2786 Loss:  tensor(0.0104)\n",
            "Step 2787 Loss:  tensor(0.0104)\n",
            "Step 2788 Loss:  tensor(0.0104)\n",
            "Step 2789 Loss:  tensor(0.0104)\n",
            "Step 2790 Loss:  tensor(0.0104)\n",
            "Step 2791 Loss:  tensor(0.0104)\n",
            "Step 2792 Loss:  tensor(0.0104)\n",
            "Step 2793 Loss:  tensor(0.0104)\n",
            "Step 2794 Loss:  tensor(0.0104)\n",
            "Step 2795 Loss:  tensor(0.0104)\n",
            "Step 2796 Loss:  tensor(0.0104)\n",
            "Step 2797 Loss:  tensor(0.0104)\n",
            "Step 2798 Loss:  tensor(0.0104)\n",
            "Step 2799 Loss:  tensor(0.0104)\n",
            "Step 2800 Loss:  tensor(0.0104)\n",
            "Step 2801 Loss:  tensor(0.0104)\n",
            "Step 2802 Loss:  tensor(0.0104)\n",
            "Step 2803 Loss:  tensor(0.0104)\n",
            "Step 2804 Loss:  tensor(0.0104)\n",
            "Step 2805 Loss:  tensor(0.0104)\n",
            "Step 2806 Loss:  tensor(0.0104)\n",
            "Step 2807 Loss:  tensor(0.0104)\n",
            "Step 2808 Loss:  tensor(0.0104)\n",
            "Step 2809 Loss:  tensor(0.0104)\n",
            "Step 2810 Loss:  tensor(0.0104)\n",
            "Step 2811 Loss:  tensor(0.0104)\n",
            "Step 2812 Loss:  tensor(0.0104)\n",
            "Step 2813 Loss:  tensor(0.0104)\n",
            "Step 2814 Loss:  tensor(0.0104)\n",
            "Step 2815 Loss:  tensor(0.0104)\n",
            "Step 2816 Loss:  tensor(0.0104)\n",
            "Step 2817 Loss:  tensor(0.0104)\n",
            "Step 2818 Loss:  tensor(0.0104)\n",
            "Step 2819 Loss:  tensor(0.0104)\n",
            "Step 2820 Loss:  tensor(0.0104)\n",
            "Step 2821 Loss:  tensor(0.0104)\n",
            "Step 2822 Loss:  tensor(0.0104)\n",
            "Step 2823 Loss:  tensor(0.0104)\n",
            "Step 2824 Loss:  tensor(0.0103)\n",
            "Step 2825 Loss:  tensor(0.0103)\n",
            "Step 2826 Loss:  tensor(0.0103)\n",
            "Step 2827 Loss:  tensor(0.0103)\n",
            "Step 2828 Loss:  tensor(0.0103)\n",
            "Step 2829 Loss:  tensor(0.0103)\n",
            "Step 2830 Loss:  tensor(0.0103)\n",
            "Step 2831 Loss:  tensor(0.0103)\n",
            "Step 2832 Loss:  tensor(0.0103)\n",
            "Step 2833 Loss:  tensor(0.0103)\n",
            "Step 2834 Loss:  tensor(0.0103)\n",
            "Step 2835 Loss:  tensor(0.0103)\n",
            "Step 2836 Loss:  tensor(0.0103)\n",
            "Step 2837 Loss:  tensor(0.0103)\n",
            "Step 2838 Loss:  tensor(0.0103)\n",
            "Step 2839 Loss:  tensor(0.0103)\n",
            "Step 2840 Loss:  tensor(0.0103)\n",
            "Step 2841 Loss:  tensor(0.0103)\n",
            "Step 2842 Loss:  tensor(0.0103)\n",
            "Step 2843 Loss:  tensor(0.0103)\n",
            "Step 2844 Loss:  tensor(0.0103)\n",
            "Step 2845 Loss:  tensor(0.0103)\n",
            "Step 2846 Loss:  tensor(0.0103)\n",
            "Step 2847 Loss:  tensor(0.0103)\n",
            "Step 2848 Loss:  tensor(0.0103)\n",
            "Step 2849 Loss:  tensor(0.0103)\n",
            "Step 2850 Loss:  tensor(0.0103)\n",
            "Step 2851 Loss:  tensor(0.0103)\n",
            "Step 2852 Loss:  tensor(0.0103)\n",
            "Step 2853 Loss:  tensor(0.0103)\n",
            "Step 2854 Loss:  tensor(0.0103)\n",
            "Step 2855 Loss:  tensor(0.0103)\n",
            "Step 2856 Loss:  tensor(0.0103)\n",
            "Step 2857 Loss:  tensor(0.0103)\n",
            "Step 2858 Loss:  tensor(0.0103)\n",
            "Step 2859 Loss:  tensor(0.0103)\n",
            "Step 2860 Loss:  tensor(0.0103)\n",
            "Step 2861 Loss:  tensor(0.0103)\n",
            "Step 2862 Loss:  tensor(0.0103)\n",
            "Step 2863 Loss:  tensor(0.0103)\n",
            "Step 2864 Loss:  tensor(0.0102)\n",
            "Step 2865 Loss:  tensor(0.0102)\n",
            "Step 2866 Loss:  tensor(0.0102)\n",
            "Step 2867 Loss:  tensor(0.0102)\n",
            "Step 2868 Loss:  tensor(0.0102)\n",
            "Step 2869 Loss:  tensor(0.0102)\n",
            "Step 2870 Loss:  tensor(0.0102)\n",
            "Step 2871 Loss:  tensor(0.0102)\n",
            "Step 2872 Loss:  tensor(0.0102)\n",
            "Step 2873 Loss:  tensor(0.0102)\n",
            "Step 2874 Loss:  tensor(0.0102)\n",
            "Step 2875 Loss:  tensor(0.0102)\n",
            "Step 2876 Loss:  tensor(0.0102)\n",
            "Step 2877 Loss:  tensor(0.0102)\n",
            "Step 2878 Loss:  tensor(0.0102)\n",
            "Step 2879 Loss:  tensor(0.0102)\n",
            "Step 2880 Loss:  tensor(0.0102)\n",
            "Step 2881 Loss:  tensor(0.0102)\n",
            "Step 2882 Loss:  tensor(0.0102)\n",
            "Step 2883 Loss:  tensor(0.0102)\n",
            "Step 2884 Loss:  tensor(0.0102)\n",
            "Step 2885 Loss:  tensor(0.0102)\n",
            "Step 2886 Loss:  tensor(0.0102)\n",
            "Step 2887 Loss:  tensor(0.0102)\n",
            "Step 2888 Loss:  tensor(0.0102)\n",
            "Step 2889 Loss:  tensor(0.0102)\n",
            "Step 2890 Loss:  tensor(0.0102)\n",
            "Step 2891 Loss:  tensor(0.0102)\n",
            "Step 2892 Loss:  tensor(0.0102)\n",
            "Step 2893 Loss:  tensor(0.0102)\n",
            "Step 2894 Loss:  tensor(0.0102)\n",
            "Step 2895 Loss:  tensor(0.0102)\n",
            "Step 2896 Loss:  tensor(0.0102)\n",
            "Step 2897 Loss:  tensor(0.0102)\n",
            "Step 2898 Loss:  tensor(0.0102)\n",
            "Step 2899 Loss:  tensor(0.0102)\n",
            "Step 2900 Loss:  tensor(0.0102)\n",
            "Step 2901 Loss:  tensor(0.0102)\n",
            "Step 2902 Loss:  tensor(0.0102)\n",
            "Step 2903 Loss:  tensor(0.0102)\n",
            "Step 2904 Loss:  tensor(0.0102)\n",
            "Step 2905 Loss:  tensor(0.0101)\n",
            "Step 2906 Loss:  tensor(0.0101)\n",
            "Step 2907 Loss:  tensor(0.0101)\n",
            "Step 2908 Loss:  tensor(0.0101)\n",
            "Step 2909 Loss:  tensor(0.0101)\n",
            "Step 2910 Loss:  tensor(0.0101)\n",
            "Step 2911 Loss:  tensor(0.0101)\n",
            "Step 2912 Loss:  tensor(0.0101)\n",
            "Step 2913 Loss:  tensor(0.0101)\n",
            "Step 2914 Loss:  tensor(0.0101)\n",
            "Step 2915 Loss:  tensor(0.0101)\n",
            "Step 2916 Loss:  tensor(0.0101)\n",
            "Step 2917 Loss:  tensor(0.0101)\n",
            "Step 2918 Loss:  tensor(0.0101)\n",
            "Step 2919 Loss:  tensor(0.0101)\n",
            "Step 2920 Loss:  tensor(0.0101)\n",
            "Step 2921 Loss:  tensor(0.0101)\n",
            "Step 2922 Loss:  tensor(0.0101)\n",
            "Step 2923 Loss:  tensor(0.0101)\n",
            "Step 2924 Loss:  tensor(0.0101)\n",
            "Step 2925 Loss:  tensor(0.0101)\n",
            "Step 2926 Loss:  tensor(0.0101)\n",
            "Step 2927 Loss:  tensor(0.0101)\n",
            "Step 2928 Loss:  tensor(0.0101)\n",
            "Step 2929 Loss:  tensor(0.0101)\n",
            "Step 2930 Loss:  tensor(0.0101)\n",
            "Step 2931 Loss:  tensor(0.0101)\n",
            "Step 2932 Loss:  tensor(0.0101)\n",
            "Step 2933 Loss:  tensor(0.0101)\n",
            "Step 2934 Loss:  tensor(0.0101)\n",
            "Step 2935 Loss:  tensor(0.0101)\n",
            "Step 2936 Loss:  tensor(0.0101)\n",
            "Step 2937 Loss:  tensor(0.0101)\n",
            "Step 2938 Loss:  tensor(0.0101)\n",
            "Step 2939 Loss:  tensor(0.0101)\n",
            "Step 2940 Loss:  tensor(0.0101)\n",
            "Step 2941 Loss:  tensor(0.0101)\n",
            "Step 2942 Loss:  tensor(0.0101)\n",
            "Step 2943 Loss:  tensor(0.0101)\n",
            "Step 2944 Loss:  tensor(0.0101)\n",
            "Step 2945 Loss:  tensor(0.0101)\n",
            "Step 2946 Loss:  tensor(0.0101)\n",
            "Step 2947 Loss:  tensor(0.0100)\n",
            "Step 2948 Loss:  tensor(0.0100)\n",
            "Step 2949 Loss:  tensor(0.0100)\n",
            "Step 2950 Loss:  tensor(0.0100)\n",
            "Step 2951 Loss:  tensor(0.0100)\n",
            "Step 2952 Loss:  tensor(0.0100)\n",
            "Step 2953 Loss:  tensor(0.0100)\n",
            "Step 2954 Loss:  tensor(0.0100)\n",
            "Step 2955 Loss:  tensor(0.0100)\n",
            "Step 2956 Loss:  tensor(0.0100)\n",
            "Step 2957 Loss:  tensor(0.0100)\n",
            "Step 2958 Loss:  tensor(0.0100)\n",
            "Step 2959 Loss:  tensor(0.0100)\n",
            "Step 2960 Loss:  tensor(0.0100)\n",
            "Step 2961 Loss:  tensor(0.0100)\n",
            "Step 2962 Loss:  tensor(0.0100)\n",
            "Step 2963 Loss:  tensor(0.0100)\n",
            "Step 2964 Loss:  tensor(0.0100)\n",
            "Step 2965 Loss:  tensor(0.0100)\n",
            "Step 2966 Loss:  tensor(0.0100)\n",
            "Step 2967 Loss:  tensor(0.0100)\n",
            "Step 2968 Loss:  tensor(0.0100)\n",
            "Step 2969 Loss:  tensor(0.0100)\n",
            "Step 2970 Loss:  tensor(0.0100)\n",
            "Step 2971 Loss:  tensor(0.0100)\n",
            "Step 2972 Loss:  tensor(0.0100)\n",
            "Step 2973 Loss:  tensor(0.0100)\n",
            "Step 2974 Loss:  tensor(0.0100)\n",
            "Step 2975 Loss:  tensor(0.0100)\n",
            "Step 2976 Loss:  tensor(0.0100)\n",
            "Step 2977 Loss:  tensor(0.0100)\n",
            "Step 2978 Loss:  tensor(0.0100)\n",
            "Step 2979 Loss:  tensor(0.0100)\n",
            "Step 2980 Loss:  tensor(0.0100)\n",
            "Step 2981 Loss:  tensor(0.0100)\n",
            "Step 2982 Loss:  tensor(0.0100)\n",
            "Step 2983 Loss:  tensor(0.0100)\n",
            "Step 2984 Loss:  tensor(0.0100)\n",
            "Step 2985 Loss:  tensor(0.0100)\n",
            "Step 2986 Loss:  tensor(0.0100)\n",
            "Step 2987 Loss:  tensor(0.0100)\n",
            "Step 2988 Loss:  tensor(0.0100)\n",
            "Step 2989 Loss:  tensor(0.0100)\n",
            "Step 2990 Loss:  tensor(0.0100)\n",
            "Step 2991 Loss:  tensor(0.0099)\n",
            "Step 2992 Loss:  tensor(0.0099)\n",
            "Step 2993 Loss:  tensor(0.0099)\n",
            "Step 2994 Loss:  tensor(0.0099)\n",
            "Step 2995 Loss:  tensor(0.0099)\n",
            "Step 2996 Loss:  tensor(0.0099)\n",
            "Step 2997 Loss:  tensor(0.0099)\n",
            "Step 2998 Loss:  tensor(0.0099)\n",
            "Step 2999 Loss:  tensor(0.0099)\n",
            "Step 3000 Loss:  tensor(0.0099)\n",
            "Step 3001 Loss:  tensor(0.0099)\n",
            "Step 3002 Loss:  tensor(0.0099)\n",
            "Step 3003 Loss:  tensor(0.0099)\n",
            "Step 3004 Loss:  tensor(0.0099)\n",
            "Step 3005 Loss:  tensor(0.0099)\n",
            "Step 3006 Loss:  tensor(0.0099)\n",
            "Step 3007 Loss:  tensor(0.0099)\n",
            "Step 3008 Loss:  tensor(0.0099)\n",
            "Step 3009 Loss:  tensor(0.0099)\n",
            "Step 3010 Loss:  tensor(0.0099)\n",
            "Step 3011 Loss:  tensor(0.0099)\n",
            "Step 3012 Loss:  tensor(0.0099)\n",
            "Step 3013 Loss:  tensor(0.0099)\n",
            "Step 3014 Loss:  tensor(0.0099)\n",
            "Step 3015 Loss:  tensor(0.0099)\n",
            "Step 3016 Loss:  tensor(0.0099)\n",
            "Step 3017 Loss:  tensor(0.0099)\n",
            "Step 3018 Loss:  tensor(0.0099)\n",
            "Step 3019 Loss:  tensor(0.0099)\n",
            "Step 3020 Loss:  tensor(0.0099)\n",
            "Step 3021 Loss:  tensor(0.0099)\n",
            "Step 3022 Loss:  tensor(0.0099)\n",
            "Step 3023 Loss:  tensor(0.0099)\n",
            "Step 3024 Loss:  tensor(0.0099)\n",
            "Step 3025 Loss:  tensor(0.0099)\n",
            "Step 3026 Loss:  tensor(0.0099)\n",
            "Step 3027 Loss:  tensor(0.0099)\n",
            "Step 3028 Loss:  tensor(0.0099)\n",
            "Step 3029 Loss:  tensor(0.0099)\n",
            "Step 3030 Loss:  tensor(0.0099)\n",
            "Step 3031 Loss:  tensor(0.0099)\n",
            "Step 3032 Loss:  tensor(0.0099)\n",
            "Step 3033 Loss:  tensor(0.0099)\n",
            "Step 3034 Loss:  tensor(0.0099)\n",
            "Step 3035 Loss:  tensor(0.0098)\n",
            "Step 3036 Loss:  tensor(0.0098)\n",
            "Step 3037 Loss:  tensor(0.0098)\n",
            "Step 3038 Loss:  tensor(0.0098)\n",
            "Step 3039 Loss:  tensor(0.0098)\n",
            "Step 3040 Loss:  tensor(0.0098)\n",
            "Step 3041 Loss:  tensor(0.0098)\n",
            "Step 3042 Loss:  tensor(0.0098)\n",
            "Step 3043 Loss:  tensor(0.0098)\n",
            "Step 3044 Loss:  tensor(0.0098)\n",
            "Step 3045 Loss:  tensor(0.0098)\n",
            "Step 3046 Loss:  tensor(0.0098)\n",
            "Step 3047 Loss:  tensor(0.0098)\n",
            "Step 3048 Loss:  tensor(0.0098)\n",
            "Step 3049 Loss:  tensor(0.0098)\n",
            "Step 3050 Loss:  tensor(0.0098)\n",
            "Step 3051 Loss:  tensor(0.0098)\n",
            "Step 3052 Loss:  tensor(0.0098)\n",
            "Step 3053 Loss:  tensor(0.0098)\n",
            "Step 3054 Loss:  tensor(0.0098)\n",
            "Step 3055 Loss:  tensor(0.0098)\n",
            "Step 3056 Loss:  tensor(0.0098)\n",
            "Step 3057 Loss:  tensor(0.0098)\n",
            "Step 3058 Loss:  tensor(0.0098)\n",
            "Step 3059 Loss:  tensor(0.0098)\n",
            "Step 3060 Loss:  tensor(0.0098)\n",
            "Step 3061 Loss:  tensor(0.0098)\n",
            "Step 3062 Loss:  tensor(0.0098)\n",
            "Step 3063 Loss:  tensor(0.0098)\n",
            "Step 3064 Loss:  tensor(0.0098)\n",
            "Step 3065 Loss:  tensor(0.0098)\n",
            "Step 3066 Loss:  tensor(0.0098)\n",
            "Step 3067 Loss:  tensor(0.0098)\n",
            "Step 3068 Loss:  tensor(0.0098)\n",
            "Step 3069 Loss:  tensor(0.0098)\n",
            "Step 3070 Loss:  tensor(0.0098)\n",
            "Step 3071 Loss:  tensor(0.0098)\n",
            "Step 3072 Loss:  tensor(0.0098)\n",
            "Step 3073 Loss:  tensor(0.0098)\n",
            "Step 3074 Loss:  tensor(0.0098)\n",
            "Step 3075 Loss:  tensor(0.0098)\n",
            "Step 3076 Loss:  tensor(0.0098)\n",
            "Step 3077 Loss:  tensor(0.0098)\n",
            "Step 3078 Loss:  tensor(0.0098)\n",
            "Step 3079 Loss:  tensor(0.0098)\n",
            "Step 3080 Loss:  tensor(0.0097)\n",
            "Step 3081 Loss:  tensor(0.0097)\n",
            "Step 3082 Loss:  tensor(0.0097)\n",
            "Step 3083 Loss:  tensor(0.0097)\n",
            "Step 3084 Loss:  tensor(0.0097)\n",
            "Step 3085 Loss:  tensor(0.0097)\n",
            "Step 3086 Loss:  tensor(0.0097)\n",
            "Step 3087 Loss:  tensor(0.0097)\n",
            "Step 3088 Loss:  tensor(0.0097)\n",
            "Step 3089 Loss:  tensor(0.0097)\n",
            "Step 3090 Loss:  tensor(0.0097)\n",
            "Step 3091 Loss:  tensor(0.0097)\n",
            "Step 3092 Loss:  tensor(0.0097)\n",
            "Step 3093 Loss:  tensor(0.0097)\n",
            "Step 3094 Loss:  tensor(0.0097)\n",
            "Step 3095 Loss:  tensor(0.0097)\n",
            "Step 3096 Loss:  tensor(0.0097)\n",
            "Step 3097 Loss:  tensor(0.0097)\n",
            "Step 3098 Loss:  tensor(0.0097)\n",
            "Step 3099 Loss:  tensor(0.0097)\n",
            "Step 3100 Loss:  tensor(0.0097)\n",
            "Step 3101 Loss:  tensor(0.0097)\n",
            "Step 3102 Loss:  tensor(0.0097)\n",
            "Step 3103 Loss:  tensor(0.0097)\n",
            "Step 3104 Loss:  tensor(0.0097)\n",
            "Step 3105 Loss:  tensor(0.0097)\n",
            "Step 3106 Loss:  tensor(0.0097)\n",
            "Step 3107 Loss:  tensor(0.0097)\n",
            "Step 3108 Loss:  tensor(0.0097)\n",
            "Step 3109 Loss:  tensor(0.0097)\n",
            "Step 3110 Loss:  tensor(0.0097)\n",
            "Step 3111 Loss:  tensor(0.0097)\n",
            "Step 3112 Loss:  tensor(0.0097)\n",
            "Step 3113 Loss:  tensor(0.0097)\n",
            "Step 3114 Loss:  tensor(0.0097)\n",
            "Step 3115 Loss:  tensor(0.0097)\n",
            "Step 3116 Loss:  tensor(0.0097)\n",
            "Step 3117 Loss:  tensor(0.0097)\n",
            "Step 3118 Loss:  tensor(0.0097)\n",
            "Step 3119 Loss:  tensor(0.0097)\n",
            "Step 3120 Loss:  tensor(0.0097)\n",
            "Step 3121 Loss:  tensor(0.0097)\n",
            "Step 3122 Loss:  tensor(0.0097)\n",
            "Step 3123 Loss:  tensor(0.0097)\n",
            "Step 3124 Loss:  tensor(0.0097)\n",
            "Step 3125 Loss:  tensor(0.0097)\n",
            "Step 3126 Loss:  tensor(0.0096)\n",
            "Step 3127 Loss:  tensor(0.0096)\n",
            "Step 3128 Loss:  tensor(0.0096)\n",
            "Step 3129 Loss:  tensor(0.0096)\n",
            "Step 3130 Loss:  tensor(0.0096)\n",
            "Step 3131 Loss:  tensor(0.0096)\n",
            "Step 3132 Loss:  tensor(0.0096)\n",
            "Step 3133 Loss:  tensor(0.0096)\n",
            "Step 3134 Loss:  tensor(0.0096)\n",
            "Step 3135 Loss:  tensor(0.0096)\n",
            "Step 3136 Loss:  tensor(0.0096)\n",
            "Step 3137 Loss:  tensor(0.0096)\n",
            "Step 3138 Loss:  tensor(0.0096)\n",
            "Step 3139 Loss:  tensor(0.0096)\n",
            "Step 3140 Loss:  tensor(0.0096)\n",
            "Step 3141 Loss:  tensor(0.0096)\n",
            "Step 3142 Loss:  tensor(0.0096)\n",
            "Step 3143 Loss:  tensor(0.0096)\n",
            "Step 3144 Loss:  tensor(0.0096)\n",
            "Step 3145 Loss:  tensor(0.0096)\n",
            "Step 3146 Loss:  tensor(0.0096)\n",
            "Step 3147 Loss:  tensor(0.0096)\n",
            "Step 3148 Loss:  tensor(0.0096)\n",
            "Step 3149 Loss:  tensor(0.0096)\n",
            "Step 3150 Loss:  tensor(0.0096)\n",
            "Step 3151 Loss:  tensor(0.0096)\n",
            "Step 3152 Loss:  tensor(0.0096)\n",
            "Step 3153 Loss:  tensor(0.0096)\n",
            "Step 3154 Loss:  tensor(0.0096)\n",
            "Step 3155 Loss:  tensor(0.0096)\n",
            "Step 3156 Loss:  tensor(0.0096)\n",
            "Step 3157 Loss:  tensor(0.0096)\n",
            "Step 3158 Loss:  tensor(0.0096)\n",
            "Step 3159 Loss:  tensor(0.0096)\n",
            "Step 3160 Loss:  tensor(0.0096)\n",
            "Step 3161 Loss:  tensor(0.0096)\n",
            "Step 3162 Loss:  tensor(0.0096)\n",
            "Step 3163 Loss:  tensor(0.0096)\n",
            "Step 3164 Loss:  tensor(0.0096)\n",
            "Step 3165 Loss:  tensor(0.0096)\n",
            "Step 3166 Loss:  tensor(0.0096)\n",
            "Step 3167 Loss:  tensor(0.0096)\n",
            "Step 3168 Loss:  tensor(0.0096)\n",
            "Step 3169 Loss:  tensor(0.0096)\n",
            "Step 3170 Loss:  tensor(0.0096)\n",
            "Step 3171 Loss:  tensor(0.0096)\n",
            "Step 3172 Loss:  tensor(0.0096)\n",
            "Step 3173 Loss:  tensor(0.0096)\n",
            "Step 3174 Loss:  tensor(0.0095)\n",
            "Step 3175 Loss:  tensor(0.0095)\n",
            "Step 3176 Loss:  tensor(0.0095)\n",
            "Step 3177 Loss:  tensor(0.0095)\n",
            "Step 3178 Loss:  tensor(0.0095)\n",
            "Step 3179 Loss:  tensor(0.0095)\n",
            "Step 3180 Loss:  tensor(0.0095)\n",
            "Step 3181 Loss:  tensor(0.0095)\n",
            "Step 3182 Loss:  tensor(0.0095)\n",
            "Step 3183 Loss:  tensor(0.0095)\n",
            "Step 3184 Loss:  tensor(0.0095)\n",
            "Step 3185 Loss:  tensor(0.0095)\n",
            "Step 3186 Loss:  tensor(0.0095)\n",
            "Step 3187 Loss:  tensor(0.0095)\n",
            "Step 3188 Loss:  tensor(0.0095)\n",
            "Step 3189 Loss:  tensor(0.0095)\n",
            "Step 3190 Loss:  tensor(0.0095)\n",
            "Step 3191 Loss:  tensor(0.0095)\n",
            "Step 3192 Loss:  tensor(0.0095)\n",
            "Step 3193 Loss:  tensor(0.0095)\n",
            "Step 3194 Loss:  tensor(0.0095)\n",
            "Step 3195 Loss:  tensor(0.0095)\n",
            "Step 3196 Loss:  tensor(0.0095)\n",
            "Step 3197 Loss:  tensor(0.0095)\n",
            "Step 3198 Loss:  tensor(0.0095)\n",
            "Step 3199 Loss:  tensor(0.0095)\n",
            "Step 3200 Loss:  tensor(0.0095)\n",
            "Step 3201 Loss:  tensor(0.0095)\n",
            "Step 3202 Loss:  tensor(0.0095)\n",
            "Step 3203 Loss:  tensor(0.0095)\n",
            "Step 3204 Loss:  tensor(0.0095)\n",
            "Step 3205 Loss:  tensor(0.0095)\n",
            "Step 3206 Loss:  tensor(0.0095)\n",
            "Step 3207 Loss:  tensor(0.0095)\n",
            "Step 3208 Loss:  tensor(0.0095)\n",
            "Step 3209 Loss:  tensor(0.0095)\n",
            "Step 3210 Loss:  tensor(0.0095)\n",
            "Step 3211 Loss:  tensor(0.0095)\n",
            "Step 3212 Loss:  tensor(0.0095)\n",
            "Step 3213 Loss:  tensor(0.0095)\n",
            "Step 3214 Loss:  tensor(0.0095)\n",
            "Step 3215 Loss:  tensor(0.0095)\n",
            "Step 3216 Loss:  tensor(0.0095)\n",
            "Step 3217 Loss:  tensor(0.0095)\n",
            "Step 3218 Loss:  tensor(0.0095)\n",
            "Step 3219 Loss:  tensor(0.0095)\n",
            "Step 3220 Loss:  tensor(0.0095)\n",
            "Step 3221 Loss:  tensor(0.0095)\n",
            "Step 3222 Loss:  tensor(0.0094)\n",
            "Step 3223 Loss:  tensor(0.0094)\n",
            "Step 3224 Loss:  tensor(0.0094)\n",
            "Step 3225 Loss:  tensor(0.0094)\n",
            "Step 3226 Loss:  tensor(0.0094)\n",
            "Step 3227 Loss:  tensor(0.0094)\n",
            "Step 3228 Loss:  tensor(0.0094)\n",
            "Step 3229 Loss:  tensor(0.0094)\n",
            "Step 3230 Loss:  tensor(0.0094)\n",
            "Step 3231 Loss:  tensor(0.0094)\n",
            "Step 3232 Loss:  tensor(0.0094)\n",
            "Step 3233 Loss:  tensor(0.0094)\n",
            "Step 3234 Loss:  tensor(0.0094)\n",
            "Step 3235 Loss:  tensor(0.0094)\n",
            "Step 3236 Loss:  tensor(0.0094)\n",
            "Step 3237 Loss:  tensor(0.0094)\n",
            "Step 3238 Loss:  tensor(0.0094)\n",
            "Step 3239 Loss:  tensor(0.0094)\n",
            "Step 3240 Loss:  tensor(0.0094)\n",
            "Step 3241 Loss:  tensor(0.0094)\n",
            "Step 3242 Loss:  tensor(0.0094)\n",
            "Step 3243 Loss:  tensor(0.0094)\n",
            "Step 3244 Loss:  tensor(0.0094)\n",
            "Step 3245 Loss:  tensor(0.0094)\n",
            "Step 3246 Loss:  tensor(0.0094)\n",
            "Step 3247 Loss:  tensor(0.0094)\n",
            "Step 3248 Loss:  tensor(0.0094)\n",
            "Step 3249 Loss:  tensor(0.0094)\n",
            "Step 3250 Loss:  tensor(0.0094)\n",
            "Step 3251 Loss:  tensor(0.0094)\n",
            "Step 3252 Loss:  tensor(0.0094)\n",
            "Step 3253 Loss:  tensor(0.0094)\n",
            "Step 3254 Loss:  tensor(0.0094)\n",
            "Step 3255 Loss:  tensor(0.0094)\n",
            "Step 3256 Loss:  tensor(0.0094)\n",
            "Step 3257 Loss:  tensor(0.0094)\n",
            "Step 3258 Loss:  tensor(0.0094)\n",
            "Step 3259 Loss:  tensor(0.0094)\n",
            "Step 3260 Loss:  tensor(0.0094)\n",
            "Step 3261 Loss:  tensor(0.0094)\n",
            "Step 3262 Loss:  tensor(0.0094)\n",
            "Step 3263 Loss:  tensor(0.0094)\n",
            "Step 3264 Loss:  tensor(0.0094)\n",
            "Step 3265 Loss:  tensor(0.0094)\n",
            "Step 3266 Loss:  tensor(0.0094)\n",
            "Step 3267 Loss:  tensor(0.0094)\n",
            "Step 3268 Loss:  tensor(0.0094)\n",
            "Step 3269 Loss:  tensor(0.0094)\n",
            "Step 3270 Loss:  tensor(0.0094)\n",
            "Step 3271 Loss:  tensor(0.0094)\n",
            "Step 3272 Loss:  tensor(0.0093)\n",
            "Step 3273 Loss:  tensor(0.0093)\n",
            "Step 3274 Loss:  tensor(0.0093)\n",
            "Step 3275 Loss:  tensor(0.0093)\n",
            "Step 3276 Loss:  tensor(0.0093)\n",
            "Step 3277 Loss:  tensor(0.0093)\n",
            "Step 3278 Loss:  tensor(0.0093)\n",
            "Step 3279 Loss:  tensor(0.0093)\n",
            "Step 3280 Loss:  tensor(0.0093)\n",
            "Step 3281 Loss:  tensor(0.0093)\n",
            "Step 3282 Loss:  tensor(0.0093)\n",
            "Step 3283 Loss:  tensor(0.0093)\n",
            "Step 3284 Loss:  tensor(0.0093)\n",
            "Step 3285 Loss:  tensor(0.0093)\n",
            "Step 3286 Loss:  tensor(0.0093)\n",
            "Step 3287 Loss:  tensor(0.0093)\n",
            "Step 3288 Loss:  tensor(0.0093)\n",
            "Step 3289 Loss:  tensor(0.0093)\n",
            "Step 3290 Loss:  tensor(0.0093)\n",
            "Step 3291 Loss:  tensor(0.0093)\n",
            "Step 3292 Loss:  tensor(0.0093)\n",
            "Step 3293 Loss:  tensor(0.0093)\n",
            "Step 3294 Loss:  tensor(0.0093)\n",
            "Step 3295 Loss:  tensor(0.0093)\n",
            "Step 3296 Loss:  tensor(0.0093)\n",
            "Step 3297 Loss:  tensor(0.0093)\n",
            "Step 3298 Loss:  tensor(0.0093)\n",
            "Step 3299 Loss:  tensor(0.0093)\n",
            "Step 3300 Loss:  tensor(0.0093)\n",
            "Step 3301 Loss:  tensor(0.0093)\n",
            "Step 3302 Loss:  tensor(0.0093)\n",
            "Step 3303 Loss:  tensor(0.0093)\n",
            "Step 3304 Loss:  tensor(0.0093)\n",
            "Step 3305 Loss:  tensor(0.0093)\n",
            "Step 3306 Loss:  tensor(0.0093)\n",
            "Step 3307 Loss:  tensor(0.0093)\n",
            "Step 3308 Loss:  tensor(0.0093)\n",
            "Step 3309 Loss:  tensor(0.0093)\n",
            "Step 3310 Loss:  tensor(0.0093)\n",
            "Step 3311 Loss:  tensor(0.0093)\n",
            "Step 3312 Loss:  tensor(0.0093)\n",
            "Step 3313 Loss:  tensor(0.0093)\n",
            "Step 3314 Loss:  tensor(0.0093)\n",
            "Step 3315 Loss:  tensor(0.0093)\n",
            "Step 3316 Loss:  tensor(0.0093)\n",
            "Step 3317 Loss:  tensor(0.0093)\n",
            "Step 3318 Loss:  tensor(0.0093)\n",
            "Step 3319 Loss:  tensor(0.0093)\n",
            "Step 3320 Loss:  tensor(0.0093)\n",
            "Step 3321 Loss:  tensor(0.0093)\n",
            "Step 3322 Loss:  tensor(0.0093)\n",
            "Step 3323 Loss:  tensor(0.0092)\n",
            "Step 3324 Loss:  tensor(0.0092)\n",
            "Step 3325 Loss:  tensor(0.0092)\n",
            "Step 3326 Loss:  tensor(0.0092)\n",
            "Step 3327 Loss:  tensor(0.0092)\n",
            "Step 3328 Loss:  tensor(0.0092)\n",
            "Step 3329 Loss:  tensor(0.0092)\n",
            "Step 3330 Loss:  tensor(0.0092)\n",
            "Step 3331 Loss:  tensor(0.0092)\n",
            "Step 3332 Loss:  tensor(0.0092)\n",
            "Step 3333 Loss:  tensor(0.0092)\n",
            "Step 3334 Loss:  tensor(0.0092)\n",
            "Step 3335 Loss:  tensor(0.0092)\n",
            "Step 3336 Loss:  tensor(0.0092)\n",
            "Step 3337 Loss:  tensor(0.0092)\n",
            "Step 3338 Loss:  tensor(0.0092)\n",
            "Step 3339 Loss:  tensor(0.0092)\n",
            "Step 3340 Loss:  tensor(0.0092)\n",
            "Step 3341 Loss:  tensor(0.0092)\n",
            "Step 3342 Loss:  tensor(0.0092)\n",
            "Step 3343 Loss:  tensor(0.0092)\n",
            "Step 3344 Loss:  tensor(0.0092)\n",
            "Step 3345 Loss:  tensor(0.0092)\n",
            "Step 3346 Loss:  tensor(0.0092)\n",
            "Step 3347 Loss:  tensor(0.0092)\n",
            "Step 3348 Loss:  tensor(0.0092)\n",
            "Step 3349 Loss:  tensor(0.0092)\n",
            "Step 3350 Loss:  tensor(0.0092)\n",
            "Step 3351 Loss:  tensor(0.0092)\n",
            "Step 3352 Loss:  tensor(0.0092)\n",
            "Step 3353 Loss:  tensor(0.0092)\n",
            "Step 3354 Loss:  tensor(0.0092)\n",
            "Step 3355 Loss:  tensor(0.0092)\n",
            "Step 3356 Loss:  tensor(0.0092)\n",
            "Step 3357 Loss:  tensor(0.0092)\n",
            "Step 3358 Loss:  tensor(0.0092)\n",
            "Step 3359 Loss:  tensor(0.0092)\n",
            "Step 3360 Loss:  tensor(0.0092)\n",
            "Step 3361 Loss:  tensor(0.0092)\n",
            "Step 3362 Loss:  tensor(0.0092)\n",
            "Step 3363 Loss:  tensor(0.0092)\n",
            "Step 3364 Loss:  tensor(0.0092)\n",
            "Step 3365 Loss:  tensor(0.0092)\n",
            "Step 3366 Loss:  tensor(0.0092)\n",
            "Step 3367 Loss:  tensor(0.0092)\n",
            "Step 3368 Loss:  tensor(0.0092)\n",
            "Step 3369 Loss:  tensor(0.0092)\n",
            "Step 3370 Loss:  tensor(0.0092)\n",
            "Step 3371 Loss:  tensor(0.0092)\n",
            "Step 3372 Loss:  tensor(0.0092)\n",
            "Step 3373 Loss:  tensor(0.0092)\n",
            "Step 3374 Loss:  tensor(0.0092)\n",
            "Step 3375 Loss:  tensor(0.0092)\n",
            "Step 3376 Loss:  tensor(0.0091)\n",
            "Step 3377 Loss:  tensor(0.0091)\n",
            "Step 3378 Loss:  tensor(0.0091)\n",
            "Step 3379 Loss:  tensor(0.0091)\n",
            "Step 3380 Loss:  tensor(0.0091)\n",
            "Step 3381 Loss:  tensor(0.0091)\n",
            "Step 3382 Loss:  tensor(0.0091)\n",
            "Step 3383 Loss:  tensor(0.0091)\n",
            "Step 3384 Loss:  tensor(0.0091)\n",
            "Step 3385 Loss:  tensor(0.0091)\n",
            "Step 3386 Loss:  tensor(0.0091)\n",
            "Step 3387 Loss:  tensor(0.0091)\n",
            "Step 3388 Loss:  tensor(0.0091)\n",
            "Step 3389 Loss:  tensor(0.0091)\n",
            "Step 3390 Loss:  tensor(0.0091)\n",
            "Step 3391 Loss:  tensor(0.0091)\n",
            "Step 3392 Loss:  tensor(0.0091)\n",
            "Step 3393 Loss:  tensor(0.0091)\n",
            "Step 3394 Loss:  tensor(0.0091)\n",
            "Step 3395 Loss:  tensor(0.0091)\n",
            "Step 3396 Loss:  tensor(0.0091)\n",
            "Step 3397 Loss:  tensor(0.0091)\n",
            "Step 3398 Loss:  tensor(0.0091)\n",
            "Step 3399 Loss:  tensor(0.0091)\n",
            "Step 3400 Loss:  tensor(0.0091)\n",
            "Step 3401 Loss:  tensor(0.0091)\n",
            "Step 3402 Loss:  tensor(0.0091)\n",
            "Step 3403 Loss:  tensor(0.0091)\n",
            "Step 3404 Loss:  tensor(0.0091)\n",
            "Step 3405 Loss:  tensor(0.0091)\n",
            "Step 3406 Loss:  tensor(0.0091)\n",
            "Step 3407 Loss:  tensor(0.0091)\n",
            "Step 3408 Loss:  tensor(0.0091)\n",
            "Step 3409 Loss:  tensor(0.0091)\n",
            "Step 3410 Loss:  tensor(0.0091)\n",
            "Step 3411 Loss:  tensor(0.0091)\n",
            "Step 3412 Loss:  tensor(0.0091)\n",
            "Step 3413 Loss:  tensor(0.0091)\n",
            "Step 3414 Loss:  tensor(0.0091)\n",
            "Step 3415 Loss:  tensor(0.0091)\n",
            "Step 3416 Loss:  tensor(0.0091)\n",
            "Step 3417 Loss:  tensor(0.0091)\n",
            "Step 3418 Loss:  tensor(0.0091)\n",
            "Step 3419 Loss:  tensor(0.0091)\n",
            "Step 3420 Loss:  tensor(0.0091)\n",
            "Step 3421 Loss:  tensor(0.0091)\n",
            "Step 3422 Loss:  tensor(0.0091)\n",
            "Step 3423 Loss:  tensor(0.0091)\n",
            "Step 3424 Loss:  tensor(0.0091)\n",
            "Step 3425 Loss:  tensor(0.0091)\n",
            "Step 3426 Loss:  tensor(0.0091)\n",
            "Step 3427 Loss:  tensor(0.0091)\n",
            "Step 3428 Loss:  tensor(0.0091)\n",
            "Step 3429 Loss:  tensor(0.0091)\n",
            "Step 3430 Loss:  tensor(0.0090)\n",
            "Step 3431 Loss:  tensor(0.0090)\n",
            "Step 3432 Loss:  tensor(0.0090)\n",
            "Step 3433 Loss:  tensor(0.0090)\n",
            "Step 3434 Loss:  tensor(0.0090)\n",
            "Step 3435 Loss:  tensor(0.0090)\n",
            "Step 3436 Loss:  tensor(0.0090)\n",
            "Step 3437 Loss:  tensor(0.0090)\n",
            "Step 3438 Loss:  tensor(0.0090)\n",
            "Step 3439 Loss:  tensor(0.0090)\n",
            "Step 3440 Loss:  tensor(0.0090)\n",
            "Step 3441 Loss:  tensor(0.0090)\n",
            "Step 3442 Loss:  tensor(0.0090)\n",
            "Step 3443 Loss:  tensor(0.0090)\n",
            "Step 3444 Loss:  tensor(0.0090)\n",
            "Step 3445 Loss:  tensor(0.0090)\n",
            "Step 3446 Loss:  tensor(0.0090)\n",
            "Step 3447 Loss:  tensor(0.0090)\n",
            "Step 3448 Loss:  tensor(0.0090)\n",
            "Step 3449 Loss:  tensor(0.0090)\n",
            "Step 3450 Loss:  tensor(0.0090)\n",
            "Step 3451 Loss:  tensor(0.0090)\n",
            "Step 3452 Loss:  tensor(0.0090)\n",
            "Step 3453 Loss:  tensor(0.0090)\n",
            "Step 3454 Loss:  tensor(0.0090)\n",
            "Step 3455 Loss:  tensor(0.0090)\n",
            "Step 3456 Loss:  tensor(0.0090)\n",
            "Step 3457 Loss:  tensor(0.0090)\n",
            "Step 3458 Loss:  tensor(0.0090)\n",
            "Step 3459 Loss:  tensor(0.0090)\n",
            "Step 3460 Loss:  tensor(0.0090)\n",
            "Step 3461 Loss:  tensor(0.0090)\n",
            "Step 3462 Loss:  tensor(0.0090)\n",
            "Step 3463 Loss:  tensor(0.0090)\n",
            "Step 3464 Loss:  tensor(0.0090)\n",
            "Step 3465 Loss:  tensor(0.0090)\n",
            "Step 3466 Loss:  tensor(0.0090)\n",
            "Step 3467 Loss:  tensor(0.0090)\n",
            "Step 3468 Loss:  tensor(0.0090)\n",
            "Step 3469 Loss:  tensor(0.0090)\n",
            "Step 3470 Loss:  tensor(0.0090)\n",
            "Step 3471 Loss:  tensor(0.0090)\n",
            "Step 3472 Loss:  tensor(0.0090)\n",
            "Step 3473 Loss:  tensor(0.0090)\n",
            "Step 3474 Loss:  tensor(0.0090)\n",
            "Step 3475 Loss:  tensor(0.0090)\n",
            "Step 3476 Loss:  tensor(0.0090)\n",
            "Step 3477 Loss:  tensor(0.0090)\n",
            "Step 3478 Loss:  tensor(0.0090)\n",
            "Step 3479 Loss:  tensor(0.0090)\n",
            "Step 3480 Loss:  tensor(0.0090)\n",
            "Step 3481 Loss:  tensor(0.0090)\n",
            "Step 3482 Loss:  tensor(0.0090)\n",
            "Step 3483 Loss:  tensor(0.0090)\n",
            "Step 3484 Loss:  tensor(0.0090)\n",
            "Step 3485 Loss:  tensor(0.0089)\n",
            "Step 3486 Loss:  tensor(0.0089)\n",
            "Step 3487 Loss:  tensor(0.0089)\n",
            "Step 3488 Loss:  tensor(0.0089)\n",
            "Step 3489 Loss:  tensor(0.0089)\n",
            "Step 3490 Loss:  tensor(0.0089)\n",
            "Step 3491 Loss:  tensor(0.0089)\n",
            "Step 3492 Loss:  tensor(0.0089)\n",
            "Step 3493 Loss:  tensor(0.0089)\n",
            "Step 3494 Loss:  tensor(0.0089)\n",
            "Step 3495 Loss:  tensor(0.0089)\n",
            "Step 3496 Loss:  tensor(0.0089)\n",
            "Step 3497 Loss:  tensor(0.0089)\n",
            "Step 3498 Loss:  tensor(0.0089)\n",
            "Step 3499 Loss:  tensor(0.0089)\n",
            "Step 3500 Loss:  tensor(0.0089)\n",
            "Step 3501 Loss:  tensor(0.0089)\n",
            "Step 3502 Loss:  tensor(0.0089)\n",
            "Step 3503 Loss:  tensor(0.0089)\n",
            "Step 3504 Loss:  tensor(0.0089)\n",
            "Step 3505 Loss:  tensor(0.0089)\n",
            "Step 3506 Loss:  tensor(0.0089)\n",
            "Step 3507 Loss:  tensor(0.0089)\n",
            "Step 3508 Loss:  tensor(0.0089)\n",
            "Step 3509 Loss:  tensor(0.0089)\n",
            "Step 3510 Loss:  tensor(0.0089)\n",
            "Step 3511 Loss:  tensor(0.0089)\n",
            "Step 3512 Loss:  tensor(0.0089)\n",
            "Step 3513 Loss:  tensor(0.0089)\n",
            "Step 3514 Loss:  tensor(0.0089)\n",
            "Step 3515 Loss:  tensor(0.0089)\n",
            "Step 3516 Loss:  tensor(0.0089)\n",
            "Step 3517 Loss:  tensor(0.0089)\n",
            "Step 3518 Loss:  tensor(0.0089)\n",
            "Step 3519 Loss:  tensor(0.0089)\n",
            "Step 3520 Loss:  tensor(0.0089)\n",
            "Step 3521 Loss:  tensor(0.0089)\n",
            "Step 3522 Loss:  tensor(0.0089)\n",
            "Step 3523 Loss:  tensor(0.0089)\n",
            "Step 3524 Loss:  tensor(0.0089)\n",
            "Step 3525 Loss:  tensor(0.0089)\n",
            "Step 3526 Loss:  tensor(0.0089)\n",
            "Step 3527 Loss:  tensor(0.0089)\n",
            "Step 3528 Loss:  tensor(0.0089)\n",
            "Step 3529 Loss:  tensor(0.0089)\n",
            "Step 3530 Loss:  tensor(0.0089)\n",
            "Step 3531 Loss:  tensor(0.0089)\n",
            "Step 3532 Loss:  tensor(0.0089)\n",
            "Step 3533 Loss:  tensor(0.0089)\n",
            "Step 3534 Loss:  tensor(0.0089)\n",
            "Step 3535 Loss:  tensor(0.0089)\n",
            "Step 3536 Loss:  tensor(0.0089)\n",
            "Step 3537 Loss:  tensor(0.0089)\n",
            "Step 3538 Loss:  tensor(0.0089)\n",
            "Step 3539 Loss:  tensor(0.0089)\n",
            "Step 3540 Loss:  tensor(0.0089)\n",
            "Step 3541 Loss:  tensor(0.0088)\n",
            "Step 3542 Loss:  tensor(0.0088)\n",
            "Step 3543 Loss:  tensor(0.0088)\n",
            "Step 3544 Loss:  tensor(0.0088)\n",
            "Step 3545 Loss:  tensor(0.0088)\n",
            "Step 3546 Loss:  tensor(0.0088)\n",
            "Step 3547 Loss:  tensor(0.0088)\n",
            "Step 3548 Loss:  tensor(0.0088)\n",
            "Step 3549 Loss:  tensor(0.0088)\n",
            "Step 3550 Loss:  tensor(0.0088)\n",
            "Step 3551 Loss:  tensor(0.0088)\n",
            "Step 3552 Loss:  tensor(0.0088)\n",
            "Step 3553 Loss:  tensor(0.0088)\n",
            "Step 3554 Loss:  tensor(0.0088)\n",
            "Step 3555 Loss:  tensor(0.0088)\n",
            "Step 3556 Loss:  tensor(0.0088)\n",
            "Step 3557 Loss:  tensor(0.0088)\n",
            "Step 3558 Loss:  tensor(0.0088)\n",
            "Step 3559 Loss:  tensor(0.0088)\n",
            "Step 3560 Loss:  tensor(0.0088)\n",
            "Step 3561 Loss:  tensor(0.0088)\n",
            "Step 3562 Loss:  tensor(0.0088)\n",
            "Step 3563 Loss:  tensor(0.0088)\n",
            "Step 3564 Loss:  tensor(0.0088)\n",
            "Step 3565 Loss:  tensor(0.0088)\n",
            "Step 3566 Loss:  tensor(0.0088)\n",
            "Step 3567 Loss:  tensor(0.0088)\n",
            "Step 3568 Loss:  tensor(0.0088)\n",
            "Step 3569 Loss:  tensor(0.0088)\n",
            "Step 3570 Loss:  tensor(0.0088)\n",
            "Step 3571 Loss:  tensor(0.0088)\n",
            "Step 3572 Loss:  tensor(0.0088)\n",
            "Step 3573 Loss:  tensor(0.0088)\n",
            "Step 3574 Loss:  tensor(0.0088)\n",
            "Step 3575 Loss:  tensor(0.0088)\n",
            "Step 3576 Loss:  tensor(0.0088)\n",
            "Step 3577 Loss:  tensor(0.0088)\n",
            "Step 3578 Loss:  tensor(0.0088)\n",
            "Step 3579 Loss:  tensor(0.0088)\n",
            "Step 3580 Loss:  tensor(0.0088)\n",
            "Step 3581 Loss:  tensor(0.0088)\n",
            "Step 3582 Loss:  tensor(0.0088)\n",
            "Step 3583 Loss:  tensor(0.0088)\n",
            "Step 3584 Loss:  tensor(0.0088)\n",
            "Step 3585 Loss:  tensor(0.0088)\n",
            "Step 3586 Loss:  tensor(0.0088)\n",
            "Step 3587 Loss:  tensor(0.0088)\n",
            "Step 3588 Loss:  tensor(0.0088)\n",
            "Step 3589 Loss:  tensor(0.0088)\n",
            "Step 3590 Loss:  tensor(0.0088)\n",
            "Step 3591 Loss:  tensor(0.0088)\n",
            "Step 3592 Loss:  tensor(0.0088)\n",
            "Step 3593 Loss:  tensor(0.0088)\n",
            "Step 3594 Loss:  tensor(0.0088)\n",
            "Step 3595 Loss:  tensor(0.0088)\n",
            "Step 3596 Loss:  tensor(0.0088)\n",
            "Step 3597 Loss:  tensor(0.0088)\n",
            "Step 3598 Loss:  tensor(0.0088)\n",
            "Step 3599 Loss:  tensor(0.0088)\n",
            "Step 3600 Loss:  tensor(0.0087)\n",
            "Step 3601 Loss:  tensor(0.0087)\n",
            "Step 3602 Loss:  tensor(0.0087)\n",
            "Step 3603 Loss:  tensor(0.0087)\n",
            "Step 3604 Loss:  tensor(0.0087)\n",
            "Step 3605 Loss:  tensor(0.0087)\n",
            "Step 3606 Loss:  tensor(0.0087)\n",
            "Step 3607 Loss:  tensor(0.0087)\n",
            "Step 3608 Loss:  tensor(0.0087)\n",
            "Step 3609 Loss:  tensor(0.0087)\n",
            "Step 3610 Loss:  tensor(0.0087)\n",
            "Step 3611 Loss:  tensor(0.0087)\n",
            "Step 3612 Loss:  tensor(0.0087)\n",
            "Step 3613 Loss:  tensor(0.0087)\n",
            "Step 3614 Loss:  tensor(0.0087)\n",
            "Step 3615 Loss:  tensor(0.0087)\n",
            "Step 3616 Loss:  tensor(0.0087)\n",
            "Step 3617 Loss:  tensor(0.0087)\n",
            "Step 3618 Loss:  tensor(0.0087)\n",
            "Step 3619 Loss:  tensor(0.0087)\n",
            "Step 3620 Loss:  tensor(0.0087)\n",
            "Step 3621 Loss:  tensor(0.0087)\n",
            "Step 3622 Loss:  tensor(0.0087)\n",
            "Step 3623 Loss:  tensor(0.0087)\n",
            "Step 3624 Loss:  tensor(0.0087)\n",
            "Step 3625 Loss:  tensor(0.0087)\n",
            "Step 3626 Loss:  tensor(0.0087)\n",
            "Step 3627 Loss:  tensor(0.0087)\n",
            "Step 3628 Loss:  tensor(0.0087)\n",
            "Step 3629 Loss:  tensor(0.0087)\n",
            "Step 3630 Loss:  tensor(0.0087)\n",
            "Step 3631 Loss:  tensor(0.0087)\n",
            "Step 3632 Loss:  tensor(0.0087)\n",
            "Step 3633 Loss:  tensor(0.0087)\n",
            "Step 3634 Loss:  tensor(0.0087)\n",
            "Step 3635 Loss:  tensor(0.0087)\n",
            "Step 3636 Loss:  tensor(0.0087)\n",
            "Step 3637 Loss:  tensor(0.0087)\n",
            "Step 3638 Loss:  tensor(0.0087)\n",
            "Step 3639 Loss:  tensor(0.0087)\n",
            "Step 3640 Loss:  tensor(0.0087)\n",
            "Step 3641 Loss:  tensor(0.0087)\n",
            "Step 3642 Loss:  tensor(0.0087)\n",
            "Step 3643 Loss:  tensor(0.0087)\n",
            "Step 3644 Loss:  tensor(0.0087)\n",
            "Step 3645 Loss:  tensor(0.0087)\n",
            "Step 3646 Loss:  tensor(0.0087)\n",
            "Step 3647 Loss:  tensor(0.0087)\n",
            "Step 3648 Loss:  tensor(0.0087)\n",
            "Step 3649 Loss:  tensor(0.0087)\n",
            "Step 3650 Loss:  tensor(0.0087)\n",
            "Step 3651 Loss:  tensor(0.0087)\n",
            "Step 3652 Loss:  tensor(0.0087)\n",
            "Step 3653 Loss:  tensor(0.0087)\n",
            "Step 3654 Loss:  tensor(0.0087)\n",
            "Step 3655 Loss:  tensor(0.0087)\n",
            "Step 3656 Loss:  tensor(0.0087)\n",
            "Step 3657 Loss:  tensor(0.0087)\n",
            "Step 3658 Loss:  tensor(0.0087)\n",
            "Step 3659 Loss:  tensor(0.0086)\n",
            "Step 3660 Loss:  tensor(0.0086)\n",
            "Step 3661 Loss:  tensor(0.0086)\n",
            "Step 3662 Loss:  tensor(0.0086)\n",
            "Step 3663 Loss:  tensor(0.0086)\n",
            "Step 3664 Loss:  tensor(0.0086)\n",
            "Step 3665 Loss:  tensor(0.0086)\n",
            "Step 3666 Loss:  tensor(0.0086)\n",
            "Step 3667 Loss:  tensor(0.0086)\n",
            "Step 3668 Loss:  tensor(0.0086)\n",
            "Step 3669 Loss:  tensor(0.0086)\n",
            "Step 3670 Loss:  tensor(0.0086)\n",
            "Step 3671 Loss:  tensor(0.0086)\n",
            "Step 3672 Loss:  tensor(0.0086)\n",
            "Step 3673 Loss:  tensor(0.0086)\n",
            "Step 3674 Loss:  tensor(0.0086)\n",
            "Step 3675 Loss:  tensor(0.0086)\n",
            "Step 3676 Loss:  tensor(0.0086)\n",
            "Step 3677 Loss:  tensor(0.0086)\n",
            "Step 3678 Loss:  tensor(0.0086)\n",
            "Step 3679 Loss:  tensor(0.0086)\n",
            "Step 3680 Loss:  tensor(0.0086)\n",
            "Step 3681 Loss:  tensor(0.0086)\n",
            "Step 3682 Loss:  tensor(0.0086)\n",
            "Step 3683 Loss:  tensor(0.0086)\n",
            "Step 3684 Loss:  tensor(0.0086)\n",
            "Step 3685 Loss:  tensor(0.0086)\n",
            "Step 3686 Loss:  tensor(0.0086)\n",
            "Step 3687 Loss:  tensor(0.0086)\n",
            "Step 3688 Loss:  tensor(0.0086)\n",
            "Step 3689 Loss:  tensor(0.0086)\n",
            "Step 3690 Loss:  tensor(0.0086)\n",
            "Step 3691 Loss:  tensor(0.0086)\n",
            "Step 3692 Loss:  tensor(0.0086)\n",
            "Step 3693 Loss:  tensor(0.0086)\n",
            "Step 3694 Loss:  tensor(0.0086)\n",
            "Step 3695 Loss:  tensor(0.0086)\n",
            "Step 3696 Loss:  tensor(0.0086)\n",
            "Step 3697 Loss:  tensor(0.0086)\n",
            "Step 3698 Loss:  tensor(0.0086)\n",
            "Step 3699 Loss:  tensor(0.0086)\n",
            "Step 3700 Loss:  tensor(0.0086)\n",
            "Step 3701 Loss:  tensor(0.0086)\n",
            "Step 3702 Loss:  tensor(0.0086)\n",
            "Step 3703 Loss:  tensor(0.0086)\n",
            "Step 3704 Loss:  tensor(0.0086)\n",
            "Step 3705 Loss:  tensor(0.0086)\n",
            "Step 3706 Loss:  tensor(0.0086)\n",
            "Step 3707 Loss:  tensor(0.0086)\n",
            "Step 3708 Loss:  tensor(0.0086)\n",
            "Step 3709 Loss:  tensor(0.0086)\n",
            "Step 3710 Loss:  tensor(0.0086)\n",
            "Step 3711 Loss:  tensor(0.0086)\n",
            "Step 3712 Loss:  tensor(0.0086)\n",
            "Step 3713 Loss:  tensor(0.0086)\n",
            "Step 3714 Loss:  tensor(0.0086)\n",
            "Step 3715 Loss:  tensor(0.0086)\n",
            "Step 3716 Loss:  tensor(0.0086)\n",
            "Step 3717 Loss:  tensor(0.0086)\n",
            "Step 3718 Loss:  tensor(0.0086)\n",
            "Step 3719 Loss:  tensor(0.0086)\n",
            "Step 3720 Loss:  tensor(0.0086)\n",
            "Step 3721 Loss:  tensor(0.0085)\n",
            "Step 3722 Loss:  tensor(0.0085)\n",
            "Step 3723 Loss:  tensor(0.0085)\n",
            "Step 3724 Loss:  tensor(0.0085)\n",
            "Step 3725 Loss:  tensor(0.0085)\n",
            "Step 3726 Loss:  tensor(0.0085)\n",
            "Step 3727 Loss:  tensor(0.0085)\n",
            "Step 3728 Loss:  tensor(0.0085)\n",
            "Step 3729 Loss:  tensor(0.0085)\n",
            "Step 3730 Loss:  tensor(0.0085)\n",
            "Step 3731 Loss:  tensor(0.0085)\n",
            "Step 3732 Loss:  tensor(0.0085)\n",
            "Step 3733 Loss:  tensor(0.0085)\n",
            "Step 3734 Loss:  tensor(0.0085)\n",
            "Step 3735 Loss:  tensor(0.0085)\n",
            "Step 3736 Loss:  tensor(0.0085)\n",
            "Step 3737 Loss:  tensor(0.0085)\n",
            "Step 3738 Loss:  tensor(0.0085)\n",
            "Step 3739 Loss:  tensor(0.0085)\n",
            "Step 3740 Loss:  tensor(0.0085)\n",
            "Step 3741 Loss:  tensor(0.0085)\n",
            "Step 3742 Loss:  tensor(0.0085)\n",
            "Step 3743 Loss:  tensor(0.0085)\n",
            "Step 3744 Loss:  tensor(0.0085)\n",
            "Step 3745 Loss:  tensor(0.0085)\n",
            "Step 3746 Loss:  tensor(0.0085)\n",
            "Step 3747 Loss:  tensor(0.0085)\n",
            "Step 3748 Loss:  tensor(0.0085)\n",
            "Step 3749 Loss:  tensor(0.0085)\n",
            "Step 3750 Loss:  tensor(0.0085)\n",
            "Step 3751 Loss:  tensor(0.0085)\n",
            "Step 3752 Loss:  tensor(0.0085)\n",
            "Step 3753 Loss:  tensor(0.0085)\n",
            "Step 3754 Loss:  tensor(0.0085)\n",
            "Step 3755 Loss:  tensor(0.0085)\n",
            "Step 3756 Loss:  tensor(0.0085)\n",
            "Step 3757 Loss:  tensor(0.0085)\n",
            "Step 3758 Loss:  tensor(0.0085)\n",
            "Step 3759 Loss:  tensor(0.0085)\n",
            "Step 3760 Loss:  tensor(0.0085)\n",
            "Step 3761 Loss:  tensor(0.0085)\n",
            "Step 3762 Loss:  tensor(0.0085)\n",
            "Step 3763 Loss:  tensor(0.0085)\n",
            "Step 3764 Loss:  tensor(0.0085)\n",
            "Step 3765 Loss:  tensor(0.0085)\n",
            "Step 3766 Loss:  tensor(0.0085)\n",
            "Step 3767 Loss:  tensor(0.0085)\n",
            "Step 3768 Loss:  tensor(0.0085)\n",
            "Step 3769 Loss:  tensor(0.0085)\n",
            "Step 3770 Loss:  tensor(0.0085)\n",
            "Step 3771 Loss:  tensor(0.0085)\n",
            "Step 3772 Loss:  tensor(0.0085)\n",
            "Step 3773 Loss:  tensor(0.0085)\n",
            "Step 3774 Loss:  tensor(0.0085)\n",
            "Step 3775 Loss:  tensor(0.0085)\n",
            "Step 3776 Loss:  tensor(0.0085)\n",
            "Step 3777 Loss:  tensor(0.0085)\n",
            "Step 3778 Loss:  tensor(0.0085)\n",
            "Step 3779 Loss:  tensor(0.0085)\n",
            "Step 3780 Loss:  tensor(0.0085)\n",
            "Step 3781 Loss:  tensor(0.0085)\n",
            "Step 3782 Loss:  tensor(0.0085)\n",
            "Step 3783 Loss:  tensor(0.0085)\n",
            "Step 3784 Loss:  tensor(0.0084)\n",
            "Step 3785 Loss:  tensor(0.0084)\n",
            "Step 3786 Loss:  tensor(0.0084)\n",
            "Step 3787 Loss:  tensor(0.0084)\n",
            "Step 3788 Loss:  tensor(0.0084)\n",
            "Step 3789 Loss:  tensor(0.0084)\n",
            "Step 3790 Loss:  tensor(0.0084)\n",
            "Step 3791 Loss:  tensor(0.0084)\n",
            "Step 3792 Loss:  tensor(0.0084)\n",
            "Step 3793 Loss:  tensor(0.0084)\n",
            "Step 3794 Loss:  tensor(0.0084)\n",
            "Step 3795 Loss:  tensor(0.0084)\n",
            "Step 3796 Loss:  tensor(0.0084)\n",
            "Step 3797 Loss:  tensor(0.0084)\n",
            "Step 3798 Loss:  tensor(0.0084)\n",
            "Step 3799 Loss:  tensor(0.0084)\n",
            "Step 3800 Loss:  tensor(0.0084)\n",
            "Step 3801 Loss:  tensor(0.0084)\n",
            "Step 3802 Loss:  tensor(0.0084)\n",
            "Step 3803 Loss:  tensor(0.0084)\n",
            "Step 3804 Loss:  tensor(0.0084)\n",
            "Step 3805 Loss:  tensor(0.0084)\n",
            "Step 3806 Loss:  tensor(0.0084)\n",
            "Step 3807 Loss:  tensor(0.0084)\n",
            "Step 3808 Loss:  tensor(0.0084)\n",
            "Step 3809 Loss:  tensor(0.0084)\n",
            "Step 3810 Loss:  tensor(0.0084)\n",
            "Step 3811 Loss:  tensor(0.0084)\n",
            "Step 3812 Loss:  tensor(0.0084)\n",
            "Step 3813 Loss:  tensor(0.0084)\n",
            "Step 3814 Loss:  tensor(0.0084)\n",
            "Step 3815 Loss:  tensor(0.0084)\n",
            "Step 3816 Loss:  tensor(0.0084)\n",
            "Step 3817 Loss:  tensor(0.0084)\n",
            "Step 3818 Loss:  tensor(0.0084)\n",
            "Step 3819 Loss:  tensor(0.0084)\n",
            "Step 3820 Loss:  tensor(0.0084)\n",
            "Step 3821 Loss:  tensor(0.0084)\n",
            "Step 3822 Loss:  tensor(0.0084)\n",
            "Step 3823 Loss:  tensor(0.0084)\n",
            "Step 3824 Loss:  tensor(0.0084)\n",
            "Step 3825 Loss:  tensor(0.0084)\n",
            "Step 3826 Loss:  tensor(0.0084)\n",
            "Step 3827 Loss:  tensor(0.0084)\n",
            "Step 3828 Loss:  tensor(0.0084)\n",
            "Step 3829 Loss:  tensor(0.0084)\n",
            "Step 3830 Loss:  tensor(0.0084)\n",
            "Step 3831 Loss:  tensor(0.0084)\n",
            "Step 3832 Loss:  tensor(0.0084)\n",
            "Step 3833 Loss:  tensor(0.0084)\n",
            "Step 3834 Loss:  tensor(0.0084)\n",
            "Step 3835 Loss:  tensor(0.0084)\n",
            "Step 3836 Loss:  tensor(0.0084)\n",
            "Step 3837 Loss:  tensor(0.0084)\n",
            "Step 3838 Loss:  tensor(0.0084)\n",
            "Step 3839 Loss:  tensor(0.0084)\n",
            "Step 3840 Loss:  tensor(0.0084)\n",
            "Step 3841 Loss:  tensor(0.0084)\n",
            "Step 3842 Loss:  tensor(0.0084)\n",
            "Step 3843 Loss:  tensor(0.0084)\n",
            "Step 3844 Loss:  tensor(0.0084)\n",
            "Step 3845 Loss:  tensor(0.0084)\n",
            "Step 3846 Loss:  tensor(0.0084)\n",
            "Step 3847 Loss:  tensor(0.0084)\n",
            "Step 3848 Loss:  tensor(0.0084)\n",
            "Step 3849 Loss:  tensor(0.0083)\n",
            "Step 3850 Loss:  tensor(0.0083)\n",
            "Step 3851 Loss:  tensor(0.0083)\n",
            "Step 3852 Loss:  tensor(0.0083)\n",
            "Step 3853 Loss:  tensor(0.0083)\n",
            "Step 3854 Loss:  tensor(0.0083)\n",
            "Step 3855 Loss:  tensor(0.0083)\n",
            "Step 3856 Loss:  tensor(0.0083)\n",
            "Step 3857 Loss:  tensor(0.0083)\n",
            "Step 3858 Loss:  tensor(0.0083)\n",
            "Step 3859 Loss:  tensor(0.0083)\n",
            "Step 3860 Loss:  tensor(0.0083)\n",
            "Step 3861 Loss:  tensor(0.0083)\n",
            "Step 3862 Loss:  tensor(0.0083)\n",
            "Step 3863 Loss:  tensor(0.0083)\n",
            "Step 3864 Loss:  tensor(0.0083)\n",
            "Step 3865 Loss:  tensor(0.0083)\n",
            "Step 3866 Loss:  tensor(0.0083)\n",
            "Step 3867 Loss:  tensor(0.0083)\n",
            "Step 3868 Loss:  tensor(0.0083)\n",
            "Step 3869 Loss:  tensor(0.0083)\n",
            "Step 3870 Loss:  tensor(0.0083)\n",
            "Step 3871 Loss:  tensor(0.0083)\n",
            "Step 3872 Loss:  tensor(0.0083)\n",
            "Step 3873 Loss:  tensor(0.0083)\n",
            "Step 3874 Loss:  tensor(0.0083)\n",
            "Step 3875 Loss:  tensor(0.0083)\n",
            "Step 3876 Loss:  tensor(0.0083)\n",
            "Step 3877 Loss:  tensor(0.0083)\n",
            "Step 3878 Loss:  tensor(0.0083)\n",
            "Step 3879 Loss:  tensor(0.0083)\n",
            "Step 3880 Loss:  tensor(0.0083)\n",
            "Step 3881 Loss:  tensor(0.0083)\n",
            "Step 3882 Loss:  tensor(0.0083)\n",
            "Step 3883 Loss:  tensor(0.0083)\n",
            "Step 3884 Loss:  tensor(0.0083)\n",
            "Step 3885 Loss:  tensor(0.0083)\n",
            "Step 3886 Loss:  tensor(0.0083)\n",
            "Step 3887 Loss:  tensor(0.0083)\n",
            "Step 3888 Loss:  tensor(0.0083)\n",
            "Step 3889 Loss:  tensor(0.0083)\n",
            "Step 3890 Loss:  tensor(0.0083)\n",
            "Step 3891 Loss:  tensor(0.0083)\n",
            "Step 3892 Loss:  tensor(0.0083)\n",
            "Step 3893 Loss:  tensor(0.0083)\n",
            "Step 3894 Loss:  tensor(0.0083)\n",
            "Step 3895 Loss:  tensor(0.0083)\n",
            "Step 3896 Loss:  tensor(0.0083)\n",
            "Step 3897 Loss:  tensor(0.0083)\n",
            "Step 3898 Loss:  tensor(0.0083)\n",
            "Step 3899 Loss:  tensor(0.0083)\n",
            "Step 3900 Loss:  tensor(0.0083)\n",
            "Step 3901 Loss:  tensor(0.0083)\n",
            "Step 3902 Loss:  tensor(0.0083)\n",
            "Step 3903 Loss:  tensor(0.0083)\n",
            "Step 3904 Loss:  tensor(0.0083)\n",
            "Step 3905 Loss:  tensor(0.0083)\n",
            "Step 3906 Loss:  tensor(0.0083)\n",
            "Step 3907 Loss:  tensor(0.0083)\n",
            "Step 3908 Loss:  tensor(0.0083)\n",
            "Step 3909 Loss:  tensor(0.0083)\n",
            "Step 3910 Loss:  tensor(0.0083)\n",
            "Step 3911 Loss:  tensor(0.0083)\n",
            "Step 3912 Loss:  tensor(0.0083)\n",
            "Step 3913 Loss:  tensor(0.0083)\n",
            "Step 3914 Loss:  tensor(0.0083)\n",
            "Step 3915 Loss:  tensor(0.0082)\n",
            "Step 3916 Loss:  tensor(0.0082)\n",
            "Step 3917 Loss:  tensor(0.0082)\n",
            "Step 3918 Loss:  tensor(0.0082)\n",
            "Step 3919 Loss:  tensor(0.0082)\n",
            "Step 3920 Loss:  tensor(0.0082)\n",
            "Step 3921 Loss:  tensor(0.0082)\n",
            "Step 3922 Loss:  tensor(0.0082)\n",
            "Step 3923 Loss:  tensor(0.0082)\n",
            "Step 3924 Loss:  tensor(0.0082)\n",
            "Step 3925 Loss:  tensor(0.0082)\n",
            "Step 3926 Loss:  tensor(0.0082)\n",
            "Step 3927 Loss:  tensor(0.0082)\n",
            "Step 3928 Loss:  tensor(0.0082)\n",
            "Step 3929 Loss:  tensor(0.0082)\n",
            "Step 3930 Loss:  tensor(0.0082)\n",
            "Step 3931 Loss:  tensor(0.0082)\n",
            "Step 3932 Loss:  tensor(0.0082)\n",
            "Step 3933 Loss:  tensor(0.0082)\n",
            "Step 3934 Loss:  tensor(0.0082)\n",
            "Step 3935 Loss:  tensor(0.0082)\n",
            "Step 3936 Loss:  tensor(0.0082)\n",
            "Step 3937 Loss:  tensor(0.0082)\n",
            "Step 3938 Loss:  tensor(0.0082)\n",
            "Step 3939 Loss:  tensor(0.0082)\n",
            "Step 3940 Loss:  tensor(0.0082)\n",
            "Step 3941 Loss:  tensor(0.0082)\n",
            "Step 3942 Loss:  tensor(0.0082)\n",
            "Step 3943 Loss:  tensor(0.0082)\n",
            "Step 3944 Loss:  tensor(0.0082)\n",
            "Step 3945 Loss:  tensor(0.0082)\n",
            "Step 3946 Loss:  tensor(0.0082)\n",
            "Step 3947 Loss:  tensor(0.0082)\n",
            "Step 3948 Loss:  tensor(0.0082)\n",
            "Step 3949 Loss:  tensor(0.0082)\n",
            "Step 3950 Loss:  tensor(0.0082)\n",
            "Step 3951 Loss:  tensor(0.0082)\n",
            "Step 3952 Loss:  tensor(0.0082)\n",
            "Step 3953 Loss:  tensor(0.0082)\n",
            "Step 3954 Loss:  tensor(0.0082)\n",
            "Step 3955 Loss:  tensor(0.0082)\n",
            "Step 3956 Loss:  tensor(0.0082)\n",
            "Step 3957 Loss:  tensor(0.0082)\n",
            "Step 3958 Loss:  tensor(0.0082)\n",
            "Step 3959 Loss:  tensor(0.0082)\n",
            "Step 3960 Loss:  tensor(0.0082)\n",
            "Step 3961 Loss:  tensor(0.0082)\n",
            "Step 3962 Loss:  tensor(0.0082)\n",
            "Step 3963 Loss:  tensor(0.0082)\n",
            "Step 3964 Loss:  tensor(0.0082)\n",
            "Step 3965 Loss:  tensor(0.0082)\n",
            "Step 3966 Loss:  tensor(0.0082)\n",
            "Step 3967 Loss:  tensor(0.0082)\n",
            "Step 3968 Loss:  tensor(0.0082)\n",
            "Step 3969 Loss:  tensor(0.0082)\n",
            "Step 3970 Loss:  tensor(0.0082)\n",
            "Step 3971 Loss:  tensor(0.0082)\n",
            "Step 3972 Loss:  tensor(0.0082)\n",
            "Step 3973 Loss:  tensor(0.0082)\n",
            "Step 3974 Loss:  tensor(0.0082)\n",
            "Step 3975 Loss:  tensor(0.0082)\n",
            "Step 3976 Loss:  tensor(0.0082)\n",
            "Step 3977 Loss:  tensor(0.0082)\n",
            "Step 3978 Loss:  tensor(0.0082)\n",
            "Step 3979 Loss:  tensor(0.0082)\n",
            "Step 3980 Loss:  tensor(0.0082)\n",
            "Step 3981 Loss:  tensor(0.0082)\n",
            "Step 3982 Loss:  tensor(0.0082)\n",
            "Step 3983 Loss:  tensor(0.0082)\n",
            "Step 3984 Loss:  tensor(0.0081)\n",
            "Step 3985 Loss:  tensor(0.0081)\n",
            "Step 3986 Loss:  tensor(0.0081)\n",
            "Step 3987 Loss:  tensor(0.0081)\n",
            "Step 3988 Loss:  tensor(0.0081)\n",
            "Step 3989 Loss:  tensor(0.0081)\n",
            "Step 3990 Loss:  tensor(0.0081)\n",
            "Step 3991 Loss:  tensor(0.0081)\n",
            "Step 3992 Loss:  tensor(0.0081)\n",
            "Step 3993 Loss:  tensor(0.0081)\n",
            "Step 3994 Loss:  tensor(0.0081)\n",
            "Step 3995 Loss:  tensor(0.0081)\n",
            "Step 3996 Loss:  tensor(0.0081)\n",
            "Step 3997 Loss:  tensor(0.0081)\n",
            "Step 3998 Loss:  tensor(0.0081)\n",
            "Step 3999 Loss:  tensor(0.0081)\n",
            "Step 4000 Loss:  tensor(0.0081)\n",
            "Step 4001 Loss:  tensor(0.0081)\n",
            "Step 4002 Loss:  tensor(0.0081)\n",
            "Step 4003 Loss:  tensor(0.0081)\n",
            "Step 4004 Loss:  tensor(0.0081)\n",
            "Step 4005 Loss:  tensor(0.0081)\n",
            "Step 4006 Loss:  tensor(0.0081)\n",
            "Step 4007 Loss:  tensor(0.0081)\n",
            "Step 4008 Loss:  tensor(0.0081)\n",
            "Step 4009 Loss:  tensor(0.0081)\n",
            "Step 4010 Loss:  tensor(0.0081)\n",
            "Step 4011 Loss:  tensor(0.0081)\n",
            "Step 4012 Loss:  tensor(0.0081)\n",
            "Step 4013 Loss:  tensor(0.0081)\n",
            "Step 4014 Loss:  tensor(0.0081)\n",
            "Step 4015 Loss:  tensor(0.0081)\n",
            "Step 4016 Loss:  tensor(0.0081)\n",
            "Step 4017 Loss:  tensor(0.0081)\n",
            "Step 4018 Loss:  tensor(0.0081)\n",
            "Step 4019 Loss:  tensor(0.0081)\n",
            "Step 4020 Loss:  tensor(0.0081)\n",
            "Step 4021 Loss:  tensor(0.0081)\n",
            "Step 4022 Loss:  tensor(0.0081)\n",
            "Step 4023 Loss:  tensor(0.0081)\n",
            "Step 4024 Loss:  tensor(0.0081)\n",
            "Step 4025 Loss:  tensor(0.0081)\n",
            "Step 4026 Loss:  tensor(0.0081)\n",
            "Step 4027 Loss:  tensor(0.0081)\n",
            "Step 4028 Loss:  tensor(0.0081)\n",
            "Step 4029 Loss:  tensor(0.0081)\n",
            "Step 4030 Loss:  tensor(0.0081)\n",
            "Step 4031 Loss:  tensor(0.0081)\n",
            "Step 4032 Loss:  tensor(0.0081)\n",
            "Step 4033 Loss:  tensor(0.0081)\n",
            "Step 4034 Loss:  tensor(0.0081)\n",
            "Step 4035 Loss:  tensor(0.0081)\n",
            "Step 4036 Loss:  tensor(0.0081)\n",
            "Step 4037 Loss:  tensor(0.0081)\n",
            "Step 4038 Loss:  tensor(0.0081)\n",
            "Step 4039 Loss:  tensor(0.0081)\n",
            "Step 4040 Loss:  tensor(0.0081)\n",
            "Step 4041 Loss:  tensor(0.0081)\n",
            "Step 4042 Loss:  tensor(0.0081)\n",
            "Step 4043 Loss:  tensor(0.0081)\n",
            "Step 4044 Loss:  tensor(0.0081)\n",
            "Step 4045 Loss:  tensor(0.0081)\n",
            "Step 4046 Loss:  tensor(0.0081)\n",
            "Step 4047 Loss:  tensor(0.0081)\n",
            "Step 4048 Loss:  tensor(0.0081)\n",
            "Step 4049 Loss:  tensor(0.0081)\n",
            "Step 4050 Loss:  tensor(0.0081)\n",
            "Step 4051 Loss:  tensor(0.0081)\n",
            "Step 4052 Loss:  tensor(0.0081)\n",
            "Step 4053 Loss:  tensor(0.0081)\n",
            "Step 4054 Loss:  tensor(0.0080)\n",
            "Step 4055 Loss:  tensor(0.0080)\n",
            "Step 4056 Loss:  tensor(0.0080)\n",
            "Step 4057 Loss:  tensor(0.0080)\n",
            "Step 4058 Loss:  tensor(0.0080)\n",
            "Step 4059 Loss:  tensor(0.0080)\n",
            "Step 4060 Loss:  tensor(0.0080)\n",
            "Step 4061 Loss:  tensor(0.0080)\n",
            "Step 4062 Loss:  tensor(0.0080)\n",
            "Step 4063 Loss:  tensor(0.0080)\n",
            "Step 4064 Loss:  tensor(0.0080)\n",
            "Step 4065 Loss:  tensor(0.0080)\n",
            "Step 4066 Loss:  tensor(0.0080)\n",
            "Step 4067 Loss:  tensor(0.0080)\n",
            "Step 4068 Loss:  tensor(0.0080)\n",
            "Step 4069 Loss:  tensor(0.0080)\n",
            "Step 4070 Loss:  tensor(0.0080)\n",
            "Step 4071 Loss:  tensor(0.0080)\n",
            "Step 4072 Loss:  tensor(0.0080)\n",
            "Step 4073 Loss:  tensor(0.0080)\n",
            "Step 4074 Loss:  tensor(0.0080)\n",
            "Step 4075 Loss:  tensor(0.0080)\n",
            "Step 4076 Loss:  tensor(0.0080)\n",
            "Step 4077 Loss:  tensor(0.0080)\n",
            "Step 4078 Loss:  tensor(0.0080)\n",
            "Step 4079 Loss:  tensor(0.0080)\n",
            "Step 4080 Loss:  tensor(0.0080)\n",
            "Step 4081 Loss:  tensor(0.0080)\n",
            "Step 4082 Loss:  tensor(0.0080)\n",
            "Step 4083 Loss:  tensor(0.0080)\n",
            "Step 4084 Loss:  tensor(0.0080)\n",
            "Step 4085 Loss:  tensor(0.0080)\n",
            "Step 4086 Loss:  tensor(0.0080)\n",
            "Step 4087 Loss:  tensor(0.0080)\n",
            "Step 4088 Loss:  tensor(0.0080)\n",
            "Step 4089 Loss:  tensor(0.0080)\n",
            "Step 4090 Loss:  tensor(0.0080)\n",
            "Step 4091 Loss:  tensor(0.0080)\n",
            "Step 4092 Loss:  tensor(0.0080)\n",
            "Step 4093 Loss:  tensor(0.0080)\n",
            "Step 4094 Loss:  tensor(0.0080)\n",
            "Step 4095 Loss:  tensor(0.0080)\n",
            "Step 4096 Loss:  tensor(0.0080)\n",
            "Step 4097 Loss:  tensor(0.0080)\n",
            "Step 4098 Loss:  tensor(0.0080)\n",
            "Step 4099 Loss:  tensor(0.0080)\n",
            "Step 4100 Loss:  tensor(0.0080)\n",
            "Step 4101 Loss:  tensor(0.0080)\n",
            "Step 4102 Loss:  tensor(0.0080)\n",
            "Step 4103 Loss:  tensor(0.0080)\n",
            "Step 4104 Loss:  tensor(0.0080)\n",
            "Step 4105 Loss:  tensor(0.0080)\n",
            "Step 4106 Loss:  tensor(0.0080)\n",
            "Step 4107 Loss:  tensor(0.0080)\n",
            "Step 4108 Loss:  tensor(0.0080)\n",
            "Step 4109 Loss:  tensor(0.0080)\n",
            "Step 4110 Loss:  tensor(0.0080)\n",
            "Step 4111 Loss:  tensor(0.0080)\n",
            "Step 4112 Loss:  tensor(0.0080)\n",
            "Step 4113 Loss:  tensor(0.0080)\n",
            "Step 4114 Loss:  tensor(0.0080)\n",
            "Step 4115 Loss:  tensor(0.0080)\n",
            "Step 4116 Loss:  tensor(0.0080)\n",
            "Step 4117 Loss:  tensor(0.0080)\n",
            "Step 4118 Loss:  tensor(0.0080)\n",
            "Step 4119 Loss:  tensor(0.0080)\n",
            "Step 4120 Loss:  tensor(0.0080)\n",
            "Step 4121 Loss:  tensor(0.0080)\n",
            "Step 4122 Loss:  tensor(0.0080)\n",
            "Step 4123 Loss:  tensor(0.0080)\n",
            "Step 4124 Loss:  tensor(0.0080)\n",
            "Step 4125 Loss:  tensor(0.0080)\n",
            "Step 4126 Loss:  tensor(0.0080)\n",
            "Step 4127 Loss:  tensor(0.0079)\n",
            "Step 4128 Loss:  tensor(0.0079)\n",
            "Step 4129 Loss:  tensor(0.0079)\n",
            "Step 4130 Loss:  tensor(0.0079)\n",
            "Step 4131 Loss:  tensor(0.0079)\n",
            "Step 4132 Loss:  tensor(0.0079)\n",
            "Step 4133 Loss:  tensor(0.0079)\n",
            "Step 4134 Loss:  tensor(0.0079)\n",
            "Step 4135 Loss:  tensor(0.0079)\n",
            "Step 4136 Loss:  tensor(0.0079)\n",
            "Step 4137 Loss:  tensor(0.0079)\n",
            "Step 4138 Loss:  tensor(0.0079)\n",
            "Step 4139 Loss:  tensor(0.0079)\n",
            "Step 4140 Loss:  tensor(0.0079)\n",
            "Step 4141 Loss:  tensor(0.0079)\n",
            "Step 4142 Loss:  tensor(0.0079)\n",
            "Step 4143 Loss:  tensor(0.0079)\n",
            "Step 4144 Loss:  tensor(0.0079)\n",
            "Step 4145 Loss:  tensor(0.0079)\n",
            "Step 4146 Loss:  tensor(0.0079)\n",
            "Step 4147 Loss:  tensor(0.0079)\n",
            "Step 4148 Loss:  tensor(0.0079)\n",
            "Step 4149 Loss:  tensor(0.0079)\n",
            "Step 4150 Loss:  tensor(0.0079)\n",
            "Step 4151 Loss:  tensor(0.0079)\n",
            "Step 4152 Loss:  tensor(0.0079)\n",
            "Step 4153 Loss:  tensor(0.0079)\n",
            "Step 4154 Loss:  tensor(0.0079)\n",
            "Step 4155 Loss:  tensor(0.0079)\n",
            "Step 4156 Loss:  tensor(0.0079)\n",
            "Step 4157 Loss:  tensor(0.0079)\n",
            "Step 4158 Loss:  tensor(0.0079)\n",
            "Step 4159 Loss:  tensor(0.0079)\n",
            "Step 4160 Loss:  tensor(0.0079)\n",
            "Step 4161 Loss:  tensor(0.0079)\n",
            "Step 4162 Loss:  tensor(0.0079)\n",
            "Step 4163 Loss:  tensor(0.0079)\n",
            "Step 4164 Loss:  tensor(0.0079)\n",
            "Step 4165 Loss:  tensor(0.0079)\n",
            "Step 4166 Loss:  tensor(0.0079)\n",
            "Step 4167 Loss:  tensor(0.0079)\n",
            "Step 4168 Loss:  tensor(0.0079)\n",
            "Step 4169 Loss:  tensor(0.0079)\n",
            "Step 4170 Loss:  tensor(0.0079)\n",
            "Step 4171 Loss:  tensor(0.0079)\n",
            "Step 4172 Loss:  tensor(0.0079)\n",
            "Step 4173 Loss:  tensor(0.0079)\n",
            "Step 4174 Loss:  tensor(0.0079)\n",
            "Step 4175 Loss:  tensor(0.0079)\n",
            "Step 4176 Loss:  tensor(0.0079)\n",
            "Step 4177 Loss:  tensor(0.0079)\n",
            "Step 4178 Loss:  tensor(0.0079)\n",
            "Step 4179 Loss:  tensor(0.0079)\n",
            "Step 4180 Loss:  tensor(0.0079)\n",
            "Step 4181 Loss:  tensor(0.0079)\n",
            "Step 4182 Loss:  tensor(0.0079)\n",
            "Step 4183 Loss:  tensor(0.0079)\n",
            "Step 4184 Loss:  tensor(0.0079)\n",
            "Step 4185 Loss:  tensor(0.0079)\n",
            "Step 4186 Loss:  tensor(0.0079)\n",
            "Step 4187 Loss:  tensor(0.0079)\n",
            "Step 4188 Loss:  tensor(0.0079)\n",
            "Step 4189 Loss:  tensor(0.0079)\n",
            "Step 4190 Loss:  tensor(0.0079)\n",
            "Step 4191 Loss:  tensor(0.0079)\n",
            "Step 4192 Loss:  tensor(0.0079)\n",
            "Step 4193 Loss:  tensor(0.0079)\n",
            "Step 4194 Loss:  tensor(0.0079)\n",
            "Step 4195 Loss:  tensor(0.0079)\n",
            "Step 4196 Loss:  tensor(0.0079)\n",
            "Step 4197 Loss:  tensor(0.0079)\n",
            "Step 4198 Loss:  tensor(0.0079)\n",
            "Step 4199 Loss:  tensor(0.0079)\n",
            "Step 4200 Loss:  tensor(0.0079)\n",
            "Step 4201 Loss:  tensor(0.0079)\n",
            "Step 4202 Loss:  tensor(0.0078)\n",
            "Step 4203 Loss:  tensor(0.0078)\n",
            "Step 4204 Loss:  tensor(0.0078)\n",
            "Step 4205 Loss:  tensor(0.0078)\n",
            "Step 4206 Loss:  tensor(0.0078)\n",
            "Step 4207 Loss:  tensor(0.0078)\n",
            "Step 4208 Loss:  tensor(0.0078)\n",
            "Step 4209 Loss:  tensor(0.0078)\n",
            "Step 4210 Loss:  tensor(0.0078)\n",
            "Step 4211 Loss:  tensor(0.0078)\n",
            "Step 4212 Loss:  tensor(0.0078)\n",
            "Step 4213 Loss:  tensor(0.0078)\n",
            "Step 4214 Loss:  tensor(0.0078)\n",
            "Step 4215 Loss:  tensor(0.0078)\n",
            "Step 4216 Loss:  tensor(0.0078)\n",
            "Step 4217 Loss:  tensor(0.0078)\n",
            "Step 4218 Loss:  tensor(0.0078)\n",
            "Step 4219 Loss:  tensor(0.0078)\n",
            "Step 4220 Loss:  tensor(0.0078)\n",
            "Step 4221 Loss:  tensor(0.0078)\n",
            "Step 4222 Loss:  tensor(0.0078)\n",
            "Step 4223 Loss:  tensor(0.0078)\n",
            "Step 4224 Loss:  tensor(0.0078)\n",
            "Step 4225 Loss:  tensor(0.0078)\n",
            "Step 4226 Loss:  tensor(0.0078)\n",
            "Step 4227 Loss:  tensor(0.0078)\n",
            "Step 4228 Loss:  tensor(0.0078)\n",
            "Step 4229 Loss:  tensor(0.0078)\n",
            "Step 4230 Loss:  tensor(0.0078)\n",
            "Step 4231 Loss:  tensor(0.0078)\n",
            "Step 4232 Loss:  tensor(0.0078)\n",
            "Step 4233 Loss:  tensor(0.0078)\n",
            "Step 4234 Loss:  tensor(0.0078)\n",
            "Step 4235 Loss:  tensor(0.0078)\n",
            "Step 4236 Loss:  tensor(0.0078)\n",
            "Step 4237 Loss:  tensor(0.0078)\n",
            "Step 4238 Loss:  tensor(0.0078)\n",
            "Step 4239 Loss:  tensor(0.0078)\n",
            "Step 4240 Loss:  tensor(0.0078)\n",
            "Step 4241 Loss:  tensor(0.0078)\n",
            "Step 4242 Loss:  tensor(0.0078)\n",
            "Step 4243 Loss:  tensor(0.0078)\n",
            "Step 4244 Loss:  tensor(0.0078)\n",
            "Step 4245 Loss:  tensor(0.0078)\n",
            "Step 4246 Loss:  tensor(0.0078)\n",
            "Step 4247 Loss:  tensor(0.0078)\n",
            "Step 4248 Loss:  tensor(0.0078)\n",
            "Step 4249 Loss:  tensor(0.0078)\n",
            "Step 4250 Loss:  tensor(0.0078)\n",
            "Step 4251 Loss:  tensor(0.0078)\n",
            "Step 4252 Loss:  tensor(0.0078)\n",
            "Step 4253 Loss:  tensor(0.0078)\n",
            "Step 4254 Loss:  tensor(0.0078)\n",
            "Step 4255 Loss:  tensor(0.0078)\n",
            "Step 4256 Loss:  tensor(0.0078)\n",
            "Step 4257 Loss:  tensor(0.0078)\n",
            "Step 4258 Loss:  tensor(0.0078)\n",
            "Step 4259 Loss:  tensor(0.0078)\n",
            "Step 4260 Loss:  tensor(0.0078)\n",
            "Step 4261 Loss:  tensor(0.0078)\n",
            "Step 4262 Loss:  tensor(0.0078)\n",
            "Step 4263 Loss:  tensor(0.0078)\n",
            "Step 4264 Loss:  tensor(0.0078)\n",
            "Step 4265 Loss:  tensor(0.0078)\n",
            "Step 4266 Loss:  tensor(0.0078)\n",
            "Step 4267 Loss:  tensor(0.0078)\n",
            "Step 4268 Loss:  tensor(0.0078)\n",
            "Step 4269 Loss:  tensor(0.0078)\n",
            "Step 4270 Loss:  tensor(0.0078)\n",
            "Step 4271 Loss:  tensor(0.0078)\n",
            "Step 4272 Loss:  tensor(0.0078)\n",
            "Step 4273 Loss:  tensor(0.0078)\n",
            "Step 4274 Loss:  tensor(0.0078)\n",
            "Step 4275 Loss:  tensor(0.0078)\n",
            "Step 4276 Loss:  tensor(0.0078)\n",
            "Step 4277 Loss:  tensor(0.0078)\n",
            "Step 4278 Loss:  tensor(0.0078)\n",
            "Step 4279 Loss:  tensor(0.0077)\n",
            "Step 4280 Loss:  tensor(0.0077)\n",
            "Step 4281 Loss:  tensor(0.0077)\n",
            "Step 4282 Loss:  tensor(0.0077)\n",
            "Step 4283 Loss:  tensor(0.0077)\n",
            "Step 4284 Loss:  tensor(0.0077)\n",
            "Step 4285 Loss:  tensor(0.0077)\n",
            "Step 4286 Loss:  tensor(0.0077)\n",
            "Step 4287 Loss:  tensor(0.0077)\n",
            "Step 4288 Loss:  tensor(0.0077)\n",
            "Step 4289 Loss:  tensor(0.0077)\n",
            "Step 4290 Loss:  tensor(0.0077)\n",
            "Step 4291 Loss:  tensor(0.0077)\n",
            "Step 4292 Loss:  tensor(0.0077)\n",
            "Step 4293 Loss:  tensor(0.0077)\n",
            "Step 4294 Loss:  tensor(0.0077)\n",
            "Step 4295 Loss:  tensor(0.0077)\n",
            "Step 4296 Loss:  tensor(0.0077)\n",
            "Step 4297 Loss:  tensor(0.0077)\n",
            "Step 4298 Loss:  tensor(0.0077)\n",
            "Step 4299 Loss:  tensor(0.0077)\n",
            "Step 4300 Loss:  tensor(0.0077)\n",
            "Step 4301 Loss:  tensor(0.0077)\n",
            "Step 4302 Loss:  tensor(0.0077)\n",
            "Step 4303 Loss:  tensor(0.0077)\n",
            "Step 4304 Loss:  tensor(0.0077)\n",
            "Step 4305 Loss:  tensor(0.0077)\n",
            "Step 4306 Loss:  tensor(0.0077)\n",
            "Step 4307 Loss:  tensor(0.0077)\n",
            "Step 4308 Loss:  tensor(0.0077)\n",
            "Step 4309 Loss:  tensor(0.0077)\n",
            "Step 4310 Loss:  tensor(0.0077)\n",
            "Step 4311 Loss:  tensor(0.0077)\n",
            "Step 4312 Loss:  tensor(0.0077)\n",
            "Step 4313 Loss:  tensor(0.0077)\n",
            "Step 4314 Loss:  tensor(0.0077)\n",
            "Step 4315 Loss:  tensor(0.0077)\n",
            "Step 4316 Loss:  tensor(0.0077)\n",
            "Step 4317 Loss:  tensor(0.0077)\n",
            "Step 4318 Loss:  tensor(0.0077)\n",
            "Step 4319 Loss:  tensor(0.0077)\n",
            "Step 4320 Loss:  tensor(0.0077)\n",
            "Step 4321 Loss:  tensor(0.0077)\n",
            "Step 4322 Loss:  tensor(0.0077)\n",
            "Step 4323 Loss:  tensor(0.0077)\n",
            "Step 4324 Loss:  tensor(0.0077)\n",
            "Step 4325 Loss:  tensor(0.0077)\n",
            "Step 4326 Loss:  tensor(0.0077)\n",
            "Step 4327 Loss:  tensor(0.0077)\n",
            "Step 4328 Loss:  tensor(0.0077)\n",
            "Step 4329 Loss:  tensor(0.0077)\n",
            "Step 4330 Loss:  tensor(0.0077)\n",
            "Step 4331 Loss:  tensor(0.0077)\n",
            "Step 4332 Loss:  tensor(0.0077)\n",
            "Step 4333 Loss:  tensor(0.0077)\n",
            "Step 4334 Loss:  tensor(0.0077)\n",
            "Step 4335 Loss:  tensor(0.0077)\n",
            "Step 4336 Loss:  tensor(0.0077)\n",
            "Step 4337 Loss:  tensor(0.0077)\n",
            "Step 4338 Loss:  tensor(0.0077)\n",
            "Step 4339 Loss:  tensor(0.0077)\n",
            "Step 4340 Loss:  tensor(0.0077)\n",
            "Step 4341 Loss:  tensor(0.0077)\n",
            "Step 4342 Loss:  tensor(0.0077)\n",
            "Step 4343 Loss:  tensor(0.0077)\n",
            "Step 4344 Loss:  tensor(0.0077)\n",
            "Step 4345 Loss:  tensor(0.0077)\n",
            "Step 4346 Loss:  tensor(0.0077)\n",
            "Step 4347 Loss:  tensor(0.0077)\n",
            "Step 4348 Loss:  tensor(0.0077)\n",
            "Step 4349 Loss:  tensor(0.0077)\n",
            "Step 4350 Loss:  tensor(0.0077)\n",
            "Step 4351 Loss:  tensor(0.0077)\n",
            "Step 4352 Loss:  tensor(0.0077)\n",
            "Step 4353 Loss:  tensor(0.0077)\n",
            "Step 4354 Loss:  tensor(0.0077)\n",
            "Step 4355 Loss:  tensor(0.0077)\n",
            "Step 4356 Loss:  tensor(0.0077)\n",
            "Step 4357 Loss:  tensor(0.0077)\n",
            "Step 4358 Loss:  tensor(0.0076)\n",
            "Step 4359 Loss:  tensor(0.0076)\n",
            "Step 4360 Loss:  tensor(0.0076)\n",
            "Step 4361 Loss:  tensor(0.0076)\n",
            "Step 4362 Loss:  tensor(0.0076)\n",
            "Step 4363 Loss:  tensor(0.0076)\n",
            "Step 4364 Loss:  tensor(0.0076)\n",
            "Step 4365 Loss:  tensor(0.0076)\n",
            "Step 4366 Loss:  tensor(0.0076)\n",
            "Step 4367 Loss:  tensor(0.0076)\n",
            "Step 4368 Loss:  tensor(0.0076)\n",
            "Step 4369 Loss:  tensor(0.0076)\n",
            "Step 4370 Loss:  tensor(0.0076)\n",
            "Step 4371 Loss:  tensor(0.0076)\n",
            "Step 4372 Loss:  tensor(0.0076)\n",
            "Step 4373 Loss:  tensor(0.0076)\n",
            "Step 4374 Loss:  tensor(0.0076)\n",
            "Step 4375 Loss:  tensor(0.0076)\n",
            "Step 4376 Loss:  tensor(0.0076)\n",
            "Step 4377 Loss:  tensor(0.0076)\n",
            "Step 4378 Loss:  tensor(0.0076)\n",
            "Step 4379 Loss:  tensor(0.0076)\n",
            "Step 4380 Loss:  tensor(0.0076)\n",
            "Step 4381 Loss:  tensor(0.0076)\n",
            "Step 4382 Loss:  tensor(0.0076)\n",
            "Step 4383 Loss:  tensor(0.0076)\n",
            "Step 4384 Loss:  tensor(0.0076)\n",
            "Step 4385 Loss:  tensor(0.0076)\n",
            "Step 4386 Loss:  tensor(0.0076)\n",
            "Step 4387 Loss:  tensor(0.0076)\n",
            "Step 4388 Loss:  tensor(0.0076)\n",
            "Step 4389 Loss:  tensor(0.0076)\n",
            "Step 4390 Loss:  tensor(0.0076)\n",
            "Step 4391 Loss:  tensor(0.0076)\n",
            "Step 4392 Loss:  tensor(0.0076)\n",
            "Step 4393 Loss:  tensor(0.0076)\n",
            "Step 4394 Loss:  tensor(0.0076)\n",
            "Step 4395 Loss:  tensor(0.0076)\n",
            "Step 4396 Loss:  tensor(0.0076)\n",
            "Step 4397 Loss:  tensor(0.0076)\n",
            "Step 4398 Loss:  tensor(0.0076)\n",
            "Step 4399 Loss:  tensor(0.0076)\n",
            "Step 4400 Loss:  tensor(0.0076)\n",
            "Step 4401 Loss:  tensor(0.0076)\n",
            "Step 4402 Loss:  tensor(0.0076)\n",
            "Step 4403 Loss:  tensor(0.0076)\n",
            "Step 4404 Loss:  tensor(0.0076)\n",
            "Step 4405 Loss:  tensor(0.0076)\n",
            "Step 4406 Loss:  tensor(0.0076)\n",
            "Step 4407 Loss:  tensor(0.0076)\n",
            "Step 4408 Loss:  tensor(0.0076)\n",
            "Step 4409 Loss:  tensor(0.0076)\n",
            "Step 4410 Loss:  tensor(0.0076)\n",
            "Step 4411 Loss:  tensor(0.0076)\n",
            "Step 4412 Loss:  tensor(0.0076)\n",
            "Step 4413 Loss:  tensor(0.0076)\n",
            "Step 4414 Loss:  tensor(0.0076)\n",
            "Step 4415 Loss:  tensor(0.0076)\n",
            "Step 4416 Loss:  tensor(0.0076)\n",
            "Step 4417 Loss:  tensor(0.0076)\n",
            "Step 4418 Loss:  tensor(0.0076)\n",
            "Step 4419 Loss:  tensor(0.0076)\n",
            "Step 4420 Loss:  tensor(0.0076)\n",
            "Step 4421 Loss:  tensor(0.0076)\n",
            "Step 4422 Loss:  tensor(0.0076)\n",
            "Step 4423 Loss:  tensor(0.0076)\n",
            "Step 4424 Loss:  tensor(0.0076)\n",
            "Step 4425 Loss:  tensor(0.0076)\n",
            "Step 4426 Loss:  tensor(0.0076)\n",
            "Step 4427 Loss:  tensor(0.0076)\n",
            "Step 4428 Loss:  tensor(0.0076)\n",
            "Step 4429 Loss:  tensor(0.0076)\n",
            "Step 4430 Loss:  tensor(0.0076)\n",
            "Step 4431 Loss:  tensor(0.0076)\n",
            "Step 4432 Loss:  tensor(0.0076)\n",
            "Step 4433 Loss:  tensor(0.0076)\n",
            "Step 4434 Loss:  tensor(0.0076)\n",
            "Step 4435 Loss:  tensor(0.0076)\n",
            "Step 4436 Loss:  tensor(0.0076)\n",
            "Step 4437 Loss:  tensor(0.0076)\n",
            "Step 4438 Loss:  tensor(0.0076)\n",
            "Step 4439 Loss:  tensor(0.0076)\n",
            "Step 4440 Loss:  tensor(0.0075)\n",
            "Step 4441 Loss:  tensor(0.0075)\n",
            "Step 4442 Loss:  tensor(0.0075)\n",
            "Step 4443 Loss:  tensor(0.0075)\n",
            "Step 4444 Loss:  tensor(0.0075)\n",
            "Step 4445 Loss:  tensor(0.0075)\n",
            "Step 4446 Loss:  tensor(0.0075)\n",
            "Step 4447 Loss:  tensor(0.0075)\n",
            "Step 4448 Loss:  tensor(0.0075)\n",
            "Step 4449 Loss:  tensor(0.0075)\n",
            "Step 4450 Loss:  tensor(0.0075)\n",
            "Step 4451 Loss:  tensor(0.0075)\n",
            "Step 4452 Loss:  tensor(0.0075)\n",
            "Step 4453 Loss:  tensor(0.0075)\n",
            "Step 4454 Loss:  tensor(0.0075)\n",
            "Step 4455 Loss:  tensor(0.0075)\n",
            "Step 4456 Loss:  tensor(0.0075)\n",
            "Step 4457 Loss:  tensor(0.0075)\n",
            "Step 4458 Loss:  tensor(0.0075)\n",
            "Step 4459 Loss:  tensor(0.0075)\n",
            "Step 4460 Loss:  tensor(0.0075)\n",
            "Step 4461 Loss:  tensor(0.0075)\n",
            "Step 4462 Loss:  tensor(0.0075)\n",
            "Step 4463 Loss:  tensor(0.0075)\n",
            "Step 4464 Loss:  tensor(0.0075)\n",
            "Step 4465 Loss:  tensor(0.0075)\n",
            "Step 4466 Loss:  tensor(0.0075)\n",
            "Step 4467 Loss:  tensor(0.0075)\n",
            "Step 4468 Loss:  tensor(0.0075)\n",
            "Step 4469 Loss:  tensor(0.0075)\n",
            "Step 4470 Loss:  tensor(0.0075)\n",
            "Step 4471 Loss:  tensor(0.0075)\n",
            "Step 4472 Loss:  tensor(0.0075)\n",
            "Step 4473 Loss:  tensor(0.0075)\n",
            "Step 4474 Loss:  tensor(0.0075)\n",
            "Step 4475 Loss:  tensor(0.0075)\n",
            "Step 4476 Loss:  tensor(0.0075)\n",
            "Step 4477 Loss:  tensor(0.0075)\n",
            "Step 4478 Loss:  tensor(0.0075)\n",
            "Step 4479 Loss:  tensor(0.0075)\n",
            "Step 4480 Loss:  tensor(0.0075)\n",
            "Step 4481 Loss:  tensor(0.0075)\n",
            "Step 4482 Loss:  tensor(0.0075)\n",
            "Step 4483 Loss:  tensor(0.0075)\n",
            "Step 4484 Loss:  tensor(0.0075)\n",
            "Step 4485 Loss:  tensor(0.0075)\n",
            "Step 4486 Loss:  tensor(0.0075)\n",
            "Step 4487 Loss:  tensor(0.0075)\n",
            "Step 4488 Loss:  tensor(0.0075)\n",
            "Step 4489 Loss:  tensor(0.0075)\n",
            "Step 4490 Loss:  tensor(0.0075)\n",
            "Step 4491 Loss:  tensor(0.0075)\n",
            "Step 4492 Loss:  tensor(0.0075)\n",
            "Step 4493 Loss:  tensor(0.0075)\n",
            "Step 4494 Loss:  tensor(0.0075)\n",
            "Step 4495 Loss:  tensor(0.0075)\n",
            "Step 4496 Loss:  tensor(0.0075)\n",
            "Step 4497 Loss:  tensor(0.0075)\n",
            "Step 4498 Loss:  tensor(0.0075)\n",
            "Step 4499 Loss:  tensor(0.0075)\n",
            "Step 4500 Loss:  tensor(0.0075)\n",
            "Step 4501 Loss:  tensor(0.0075)\n",
            "Step 4502 Loss:  tensor(0.0075)\n",
            "Step 4503 Loss:  tensor(0.0075)\n",
            "Step 4504 Loss:  tensor(0.0075)\n",
            "Step 4505 Loss:  tensor(0.0075)\n",
            "Step 4506 Loss:  tensor(0.0075)\n",
            "Step 4507 Loss:  tensor(0.0075)\n",
            "Step 4508 Loss:  tensor(0.0075)\n",
            "Step 4509 Loss:  tensor(0.0075)\n",
            "Step 4510 Loss:  tensor(0.0075)\n",
            "Step 4511 Loss:  tensor(0.0075)\n",
            "Step 4512 Loss:  tensor(0.0075)\n",
            "Step 4513 Loss:  tensor(0.0075)\n",
            "Step 4514 Loss:  tensor(0.0075)\n",
            "Step 4515 Loss:  tensor(0.0075)\n",
            "Step 4516 Loss:  tensor(0.0075)\n",
            "Step 4517 Loss:  tensor(0.0075)\n",
            "Step 4518 Loss:  tensor(0.0075)\n",
            "Step 4519 Loss:  tensor(0.0075)\n",
            "Step 4520 Loss:  tensor(0.0075)\n",
            "Step 4521 Loss:  tensor(0.0075)\n",
            "Step 4522 Loss:  tensor(0.0075)\n",
            "Step 4523 Loss:  tensor(0.0075)\n",
            "Step 4524 Loss:  tensor(0.0074)\n",
            "Step 4525 Loss:  tensor(0.0074)\n",
            "Step 4526 Loss:  tensor(0.0074)\n",
            "Step 4527 Loss:  tensor(0.0074)\n",
            "Step 4528 Loss:  tensor(0.0074)\n",
            "Step 4529 Loss:  tensor(0.0074)\n",
            "Step 4530 Loss:  tensor(0.0074)\n",
            "Step 4531 Loss:  tensor(0.0074)\n",
            "Step 4532 Loss:  tensor(0.0074)\n",
            "Step 4533 Loss:  tensor(0.0074)\n",
            "Step 4534 Loss:  tensor(0.0074)\n",
            "Step 4535 Loss:  tensor(0.0074)\n",
            "Step 4536 Loss:  tensor(0.0074)\n",
            "Step 4537 Loss:  tensor(0.0074)\n",
            "Step 4538 Loss:  tensor(0.0074)\n",
            "Step 4539 Loss:  tensor(0.0074)\n",
            "Step 4540 Loss:  tensor(0.0074)\n",
            "Step 4541 Loss:  tensor(0.0074)\n",
            "Step 4542 Loss:  tensor(0.0074)\n",
            "Step 4543 Loss:  tensor(0.0074)\n",
            "Step 4544 Loss:  tensor(0.0074)\n",
            "Step 4545 Loss:  tensor(0.0074)\n",
            "Step 4546 Loss:  tensor(0.0074)\n",
            "Step 4547 Loss:  tensor(0.0074)\n",
            "Step 4548 Loss:  tensor(0.0074)\n",
            "Step 4549 Loss:  tensor(0.0074)\n",
            "Step 4550 Loss:  tensor(0.0074)\n",
            "Step 4551 Loss:  tensor(0.0074)\n",
            "Step 4552 Loss:  tensor(0.0074)\n",
            "Step 4553 Loss:  tensor(0.0074)\n",
            "Step 4554 Loss:  tensor(0.0074)\n",
            "Step 4555 Loss:  tensor(0.0074)\n",
            "Step 4556 Loss:  tensor(0.0074)\n",
            "Step 4557 Loss:  tensor(0.0074)\n",
            "Step 4558 Loss:  tensor(0.0074)\n",
            "Step 4559 Loss:  tensor(0.0074)\n",
            "Step 4560 Loss:  tensor(0.0074)\n",
            "Step 4561 Loss:  tensor(0.0074)\n",
            "Step 4562 Loss:  tensor(0.0074)\n",
            "Step 4563 Loss:  tensor(0.0074)\n",
            "Step 4564 Loss:  tensor(0.0074)\n",
            "Step 4565 Loss:  tensor(0.0074)\n",
            "Step 4566 Loss:  tensor(0.0074)\n",
            "Step 4567 Loss:  tensor(0.0074)\n",
            "Step 4568 Loss:  tensor(0.0074)\n",
            "Step 4569 Loss:  tensor(0.0074)\n",
            "Step 4570 Loss:  tensor(0.0074)\n",
            "Step 4571 Loss:  tensor(0.0074)\n",
            "Step 4572 Loss:  tensor(0.0074)\n",
            "Step 4573 Loss:  tensor(0.0074)\n",
            "Step 4574 Loss:  tensor(0.0074)\n",
            "Step 4575 Loss:  tensor(0.0074)\n",
            "Step 4576 Loss:  tensor(0.0074)\n",
            "Step 4577 Loss:  tensor(0.0074)\n",
            "Step 4578 Loss:  tensor(0.0074)\n",
            "Step 4579 Loss:  tensor(0.0074)\n",
            "Step 4580 Loss:  tensor(0.0074)\n",
            "Step 4581 Loss:  tensor(0.0074)\n",
            "Step 4582 Loss:  tensor(0.0074)\n",
            "Step 4583 Loss:  tensor(0.0074)\n",
            "Step 4584 Loss:  tensor(0.0074)\n",
            "Step 4585 Loss:  tensor(0.0074)\n",
            "Step 4586 Loss:  tensor(0.0074)\n",
            "Step 4587 Loss:  tensor(0.0074)\n",
            "Step 4588 Loss:  tensor(0.0074)\n",
            "Step 4589 Loss:  tensor(0.0074)\n",
            "Step 4590 Loss:  tensor(0.0074)\n",
            "Step 4591 Loss:  tensor(0.0074)\n",
            "Step 4592 Loss:  tensor(0.0074)\n",
            "Step 4593 Loss:  tensor(0.0074)\n",
            "Step 4594 Loss:  tensor(0.0074)\n",
            "Step 4595 Loss:  tensor(0.0074)\n",
            "Step 4596 Loss:  tensor(0.0074)\n",
            "Step 4597 Loss:  tensor(0.0074)\n",
            "Step 4598 Loss:  tensor(0.0074)\n",
            "Step 4599 Loss:  tensor(0.0074)\n",
            "Step 4600 Loss:  tensor(0.0074)\n",
            "Step 4601 Loss:  tensor(0.0074)\n",
            "Step 4602 Loss:  tensor(0.0074)\n",
            "Step 4603 Loss:  tensor(0.0074)\n",
            "Step 4604 Loss:  tensor(0.0074)\n",
            "Step 4605 Loss:  tensor(0.0074)\n",
            "Step 4606 Loss:  tensor(0.0074)\n",
            "Step 4607 Loss:  tensor(0.0074)\n",
            "Step 4608 Loss:  tensor(0.0074)\n",
            "Step 4609 Loss:  tensor(0.0074)\n",
            "Step 4610 Loss:  tensor(0.0074)\n",
            "Step 4611 Loss:  tensor(0.0073)\n",
            "Step 4612 Loss:  tensor(0.0073)\n",
            "Step 4613 Loss:  tensor(0.0073)\n",
            "Step 4614 Loss:  tensor(0.0073)\n",
            "Step 4615 Loss:  tensor(0.0073)\n",
            "Step 4616 Loss:  tensor(0.0073)\n",
            "Step 4617 Loss:  tensor(0.0073)\n",
            "Step 4618 Loss:  tensor(0.0073)\n",
            "Step 4619 Loss:  tensor(0.0073)\n",
            "Step 4620 Loss:  tensor(0.0073)\n",
            "Step 4621 Loss:  tensor(0.0073)\n",
            "Step 4622 Loss:  tensor(0.0073)\n",
            "Step 4623 Loss:  tensor(0.0073)\n",
            "Step 4624 Loss:  tensor(0.0073)\n",
            "Step 4625 Loss:  tensor(0.0073)\n",
            "Step 4626 Loss:  tensor(0.0073)\n",
            "Step 4627 Loss:  tensor(0.0073)\n",
            "Step 4628 Loss:  tensor(0.0073)\n",
            "Step 4629 Loss:  tensor(0.0073)\n",
            "Step 4630 Loss:  tensor(0.0073)\n",
            "Step 4631 Loss:  tensor(0.0073)\n",
            "Step 4632 Loss:  tensor(0.0073)\n",
            "Step 4633 Loss:  tensor(0.0073)\n",
            "Step 4634 Loss:  tensor(0.0073)\n",
            "Step 4635 Loss:  tensor(0.0073)\n",
            "Step 4636 Loss:  tensor(0.0073)\n",
            "Step 4637 Loss:  tensor(0.0073)\n",
            "Step 4638 Loss:  tensor(0.0073)\n",
            "Step 4639 Loss:  tensor(0.0073)\n",
            "Step 4640 Loss:  tensor(0.0073)\n",
            "Step 4641 Loss:  tensor(0.0073)\n",
            "Step 4642 Loss:  tensor(0.0073)\n",
            "Step 4643 Loss:  tensor(0.0073)\n",
            "Step 4644 Loss:  tensor(0.0073)\n",
            "Step 4645 Loss:  tensor(0.0073)\n",
            "Step 4646 Loss:  tensor(0.0073)\n",
            "Step 4647 Loss:  tensor(0.0073)\n",
            "Step 4648 Loss:  tensor(0.0073)\n",
            "Step 4649 Loss:  tensor(0.0073)\n",
            "Step 4650 Loss:  tensor(0.0073)\n",
            "Step 4651 Loss:  tensor(0.0073)\n",
            "Step 4652 Loss:  tensor(0.0073)\n",
            "Step 4653 Loss:  tensor(0.0073)\n",
            "Step 4654 Loss:  tensor(0.0073)\n",
            "Step 4655 Loss:  tensor(0.0073)\n",
            "Step 4656 Loss:  tensor(0.0073)\n",
            "Step 4657 Loss:  tensor(0.0073)\n",
            "Step 4658 Loss:  tensor(0.0073)\n",
            "Step 4659 Loss:  tensor(0.0073)\n",
            "Step 4660 Loss:  tensor(0.0073)\n",
            "Step 4661 Loss:  tensor(0.0073)\n",
            "Step 4662 Loss:  tensor(0.0073)\n",
            "Step 4663 Loss:  tensor(0.0073)\n",
            "Step 4664 Loss:  tensor(0.0073)\n",
            "Step 4665 Loss:  tensor(0.0073)\n",
            "Step 4666 Loss:  tensor(0.0073)\n",
            "Step 4667 Loss:  tensor(0.0073)\n",
            "Step 4668 Loss:  tensor(0.0073)\n",
            "Step 4669 Loss:  tensor(0.0073)\n",
            "Step 4670 Loss:  tensor(0.0073)\n",
            "Step 4671 Loss:  tensor(0.0073)\n",
            "Step 4672 Loss:  tensor(0.0073)\n",
            "Step 4673 Loss:  tensor(0.0073)\n",
            "Step 4674 Loss:  tensor(0.0073)\n",
            "Step 4675 Loss:  tensor(0.0073)\n",
            "Step 4676 Loss:  tensor(0.0073)\n",
            "Step 4677 Loss:  tensor(0.0073)\n",
            "Step 4678 Loss:  tensor(0.0073)\n",
            "Step 4679 Loss:  tensor(0.0073)\n",
            "Step 4680 Loss:  tensor(0.0073)\n",
            "Step 4681 Loss:  tensor(0.0073)\n",
            "Step 4682 Loss:  tensor(0.0073)\n",
            "Step 4683 Loss:  tensor(0.0073)\n",
            "Step 4684 Loss:  tensor(0.0073)\n",
            "Step 4685 Loss:  tensor(0.0073)\n",
            "Step 4686 Loss:  tensor(0.0073)\n",
            "Step 4687 Loss:  tensor(0.0073)\n",
            "Step 4688 Loss:  tensor(0.0073)\n",
            "Step 4689 Loss:  tensor(0.0073)\n",
            "Step 4690 Loss:  tensor(0.0073)\n",
            "Step 4691 Loss:  tensor(0.0073)\n",
            "Step 4692 Loss:  tensor(0.0073)\n",
            "Step 4693 Loss:  tensor(0.0073)\n",
            "Step 4694 Loss:  tensor(0.0073)\n",
            "Step 4695 Loss:  tensor(0.0073)\n",
            "Step 4696 Loss:  tensor(0.0073)\n",
            "Step 4697 Loss:  tensor(0.0073)\n",
            "Step 4698 Loss:  tensor(0.0073)\n",
            "Step 4699 Loss:  tensor(0.0073)\n",
            "Step 4700 Loss:  tensor(0.0073)\n",
            "Step 4701 Loss:  tensor(0.0072)\n",
            "Step 4702 Loss:  tensor(0.0072)\n",
            "Step 4703 Loss:  tensor(0.0072)\n",
            "Step 4704 Loss:  tensor(0.0072)\n",
            "Step 4705 Loss:  tensor(0.0072)\n",
            "Step 4706 Loss:  tensor(0.0072)\n",
            "Step 4707 Loss:  tensor(0.0072)\n",
            "Step 4708 Loss:  tensor(0.0072)\n",
            "Step 4709 Loss:  tensor(0.0072)\n",
            "Step 4710 Loss:  tensor(0.0072)\n",
            "Step 4711 Loss:  tensor(0.0072)\n",
            "Step 4712 Loss:  tensor(0.0072)\n",
            "Step 4713 Loss:  tensor(0.0072)\n",
            "Step 4714 Loss:  tensor(0.0072)\n",
            "Step 4715 Loss:  tensor(0.0072)\n",
            "Step 4716 Loss:  tensor(0.0072)\n",
            "Step 4717 Loss:  tensor(0.0072)\n",
            "Step 4718 Loss:  tensor(0.0072)\n",
            "Step 4719 Loss:  tensor(0.0072)\n",
            "Step 4720 Loss:  tensor(0.0072)\n",
            "Step 4721 Loss:  tensor(0.0072)\n",
            "Step 4722 Loss:  tensor(0.0072)\n",
            "Step 4723 Loss:  tensor(0.0072)\n",
            "Step 4724 Loss:  tensor(0.0072)\n",
            "Step 4725 Loss:  tensor(0.0072)\n",
            "Step 4726 Loss:  tensor(0.0072)\n",
            "Step 4727 Loss:  tensor(0.0072)\n",
            "Step 4728 Loss:  tensor(0.0072)\n",
            "Step 4729 Loss:  tensor(0.0072)\n",
            "Step 4730 Loss:  tensor(0.0072)\n",
            "Step 4731 Loss:  tensor(0.0072)\n",
            "Step 4732 Loss:  tensor(0.0072)\n",
            "Step 4733 Loss:  tensor(0.0072)\n",
            "Step 4734 Loss:  tensor(0.0072)\n",
            "Step 4735 Loss:  tensor(0.0072)\n",
            "Step 4736 Loss:  tensor(0.0072)\n",
            "Step 4737 Loss:  tensor(0.0072)\n",
            "Step 4738 Loss:  tensor(0.0072)\n",
            "Step 4739 Loss:  tensor(0.0072)\n",
            "Step 4740 Loss:  tensor(0.0072)\n",
            "Step 4741 Loss:  tensor(0.0072)\n",
            "Step 4742 Loss:  tensor(0.0072)\n",
            "Step 4743 Loss:  tensor(0.0072)\n",
            "Step 4744 Loss:  tensor(0.0072)\n",
            "Step 4745 Loss:  tensor(0.0072)\n",
            "Step 4746 Loss:  tensor(0.0072)\n",
            "Step 4747 Loss:  tensor(0.0072)\n",
            "Step 4748 Loss:  tensor(0.0072)\n",
            "Step 4749 Loss:  tensor(0.0072)\n",
            "Step 4750 Loss:  tensor(0.0072)\n",
            "Step 4751 Loss:  tensor(0.0072)\n",
            "Step 4752 Loss:  tensor(0.0072)\n",
            "Step 4753 Loss:  tensor(0.0072)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbgYRd-l2pEU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "be7fd27f-55db-4214-febd-5410e640c43c"
      },
      "source": [
        "x_1 = torch.linspace(-3, 8, 50)\n",
        "x_2 =  -w[1]/w[2] * x_1 - w[0]/w[2]\n",
        "\n",
        "plt.scatter(X_1[:, 0], X_1[:, 1])\n",
        "plt.scatter(X_2[:, 0], X_2[:, 1])\n",
        "plt.plot(x_1.detach().numpy(), x_2.detach().numpy())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HynaY16m4X4r"
      },
      "source": [
        "### Using autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODDFi2A52tnq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eaea2fe3-5a90-4f5a-c542-62f2c138012c"
      },
      "source": [
        "# w is the parameter to optimize\n",
        "w = torch.normal(mean=0, std=10, size=(X.shape[1],))\n",
        "w.requires_grad = True\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "\n",
        "N_ITERATIONS = 10000\n",
        "\n",
        "\n",
        "prev_loss = torch.FloatTensor([float('inf')])\n",
        "\n",
        "for i in range(N_ITERATIONS):\n",
        "\n",
        "    Y_hat = torch.sigmoid(X @ w)\n",
        "\n",
        "    loss = cross_entropy_loss(Y, Y_hat)\n",
        "    print(f\"Step {i + 1} Loss: \", loss)\n",
        "    if torch.isclose(prev_loss, loss, atol=1e-5):\n",
        "        break\n",
        "    \n",
        "    \n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= LEARNING_RATE * w.grad\n",
        "        w.grad.zero_()\n",
        "    prev_loss = loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Loss:  tensor(1.1025, grad_fn=<MulBackward0>)\n",
            "Step 2 Loss:  tensor(1.0984, grad_fn=<MulBackward0>)\n",
            "Step 3 Loss:  tensor(1.0935, grad_fn=<MulBackward0>)\n",
            "Step 4 Loss:  tensor(1.0897, grad_fn=<MulBackward0>)\n",
            "Step 5 Loss:  tensor(1.0855, grad_fn=<MulBackward0>)\n",
            "Step 6 Loss:  tensor(1.0808, grad_fn=<MulBackward0>)\n",
            "Step 7 Loss:  tensor(1.0770, grad_fn=<MulBackward0>)\n",
            "Step 8 Loss:  tensor(1.0731, grad_fn=<MulBackward0>)\n",
            "Step 9 Loss:  tensor(1.0685, grad_fn=<MulBackward0>)\n",
            "Step 10 Loss:  tensor(1.0648, grad_fn=<MulBackward0>)\n",
            "Step 11 Loss:  tensor(1.0609, grad_fn=<MulBackward0>)\n",
            "Step 12 Loss:  tensor(1.0566, grad_fn=<MulBackward0>)\n",
            "Step 13 Loss:  tensor(1.0527, grad_fn=<MulBackward0>)\n",
            "Step 14 Loss:  tensor(1.0483, grad_fn=<MulBackward0>)\n",
            "Step 15 Loss:  tensor(1.0444, grad_fn=<MulBackward0>)\n",
            "Step 16 Loss:  tensor(1.0406, grad_fn=<MulBackward0>)\n",
            "Step 17 Loss:  tensor(1.0365, grad_fn=<MulBackward0>)\n",
            "Step 18 Loss:  tensor(1.0328, grad_fn=<MulBackward0>)\n",
            "Step 19 Loss:  tensor(1.0286, grad_fn=<MulBackward0>)\n",
            "Step 20 Loss:  tensor(1.0248, grad_fn=<MulBackward0>)\n",
            "Step 21 Loss:  tensor(1.0208, grad_fn=<MulBackward0>)\n",
            "Step 22 Loss:  tensor(1.0167, grad_fn=<MulBackward0>)\n",
            "Step 23 Loss:  tensor(1.0130, grad_fn=<MulBackward0>)\n",
            "Step 24 Loss:  tensor(1.0091, grad_fn=<MulBackward0>)\n",
            "Step 25 Loss:  tensor(1.0053, grad_fn=<MulBackward0>)\n",
            "Step 26 Loss:  tensor(1.0016, grad_fn=<MulBackward0>)\n",
            "Step 27 Loss:  tensor(0.9978, grad_fn=<MulBackward0>)\n",
            "Step 28 Loss:  tensor(0.9940, grad_fn=<MulBackward0>)\n",
            "Step 29 Loss:  tensor(0.9904, grad_fn=<MulBackward0>)\n",
            "Step 30 Loss:  tensor(0.9866, grad_fn=<MulBackward0>)\n",
            "Step 31 Loss:  tensor(0.9828, grad_fn=<MulBackward0>)\n",
            "Step 32 Loss:  tensor(0.9791, grad_fn=<MulBackward0>)\n",
            "Step 33 Loss:  tensor(0.9754, grad_fn=<MulBackward0>)\n",
            "Step 34 Loss:  tensor(0.9717, grad_fn=<MulBackward0>)\n",
            "Step 35 Loss:  tensor(0.9683, grad_fn=<MulBackward0>)\n",
            "Step 36 Loss:  tensor(0.9646, grad_fn=<MulBackward0>)\n",
            "Step 37 Loss:  tensor(0.9610, grad_fn=<MulBackward0>)\n",
            "Step 38 Loss:  tensor(0.9573, grad_fn=<MulBackward0>)\n",
            "Step 39 Loss:  tensor(0.9538, grad_fn=<MulBackward0>)\n",
            "Step 40 Loss:  tensor(0.9503, grad_fn=<MulBackward0>)\n",
            "Step 41 Loss:  tensor(0.9468, grad_fn=<MulBackward0>)\n",
            "Step 42 Loss:  tensor(0.9433, grad_fn=<MulBackward0>)\n",
            "Step 43 Loss:  tensor(0.9398, grad_fn=<MulBackward0>)\n",
            "Step 44 Loss:  tensor(0.9363, grad_fn=<MulBackward0>)\n",
            "Step 45 Loss:  tensor(0.9329, grad_fn=<MulBackward0>)\n",
            "Step 46 Loss:  tensor(0.9295, grad_fn=<MulBackward0>)\n",
            "Step 47 Loss:  tensor(0.9261, grad_fn=<MulBackward0>)\n",
            "Step 48 Loss:  tensor(0.9227, grad_fn=<MulBackward0>)\n",
            "Step 49 Loss:  tensor(0.9193, grad_fn=<MulBackward0>)\n",
            "Step 50 Loss:  tensor(0.9160, grad_fn=<MulBackward0>)\n",
            "Step 51 Loss:  tensor(0.9127, grad_fn=<MulBackward0>)\n",
            "Step 52 Loss:  tensor(0.9093, grad_fn=<MulBackward0>)\n",
            "Step 53 Loss:  tensor(0.9060, grad_fn=<MulBackward0>)\n",
            "Step 54 Loss:  tensor(0.9028, grad_fn=<MulBackward0>)\n",
            "Step 55 Loss:  tensor(0.8995, grad_fn=<MulBackward0>)\n",
            "Step 56 Loss:  tensor(0.8964, grad_fn=<MulBackward0>)\n",
            "Step 57 Loss:  tensor(0.8931, grad_fn=<MulBackward0>)\n",
            "Step 58 Loss:  tensor(0.8899, grad_fn=<MulBackward0>)\n",
            "Step 59 Loss:  tensor(0.8867, grad_fn=<MulBackward0>)\n",
            "Step 60 Loss:  tensor(0.8836, grad_fn=<MulBackward0>)\n",
            "Step 61 Loss:  tensor(0.8677, grad_fn=<MulBackward0>)\n",
            "Step 62 Loss:  tensor(0.8643, grad_fn=<MulBackward0>)\n",
            "Step 63 Loss:  tensor(0.8611, grad_fn=<MulBackward0>)\n",
            "Step 64 Loss:  tensor(0.8578, grad_fn=<MulBackward0>)\n",
            "Step 65 Loss:  tensor(0.8546, grad_fn=<MulBackward0>)\n",
            "Step 66 Loss:  tensor(0.8514, grad_fn=<MulBackward0>)\n",
            "Step 67 Loss:  tensor(0.8482, grad_fn=<MulBackward0>)\n",
            "Step 68 Loss:  tensor(0.8450, grad_fn=<MulBackward0>)\n",
            "Step 69 Loss:  tensor(0.8419, grad_fn=<MulBackward0>)\n",
            "Step 70 Loss:  tensor(0.8387, grad_fn=<MulBackward0>)\n",
            "Step 71 Loss:  tensor(0.8356, grad_fn=<MulBackward0>)\n",
            "Step 72 Loss:  tensor(0.8325, grad_fn=<MulBackward0>)\n",
            "Step 73 Loss:  tensor(0.8295, grad_fn=<MulBackward0>)\n",
            "Step 74 Loss:  tensor(0.8264, grad_fn=<MulBackward0>)\n",
            "Step 75 Loss:  tensor(0.8233, grad_fn=<MulBackward0>)\n",
            "Step 76 Loss:  tensor(0.8203, grad_fn=<MulBackward0>)\n",
            "Step 77 Loss:  tensor(0.8173, grad_fn=<MulBackward0>)\n",
            "Step 78 Loss:  tensor(0.8143, grad_fn=<MulBackward0>)\n",
            "Step 79 Loss:  tensor(0.8113, grad_fn=<MulBackward0>)\n",
            "Step 80 Loss:  tensor(0.8084, grad_fn=<MulBackward0>)\n",
            "Step 81 Loss:  tensor(0.8055, grad_fn=<MulBackward0>)\n",
            "Step 82 Loss:  tensor(0.8026, grad_fn=<MulBackward0>)\n",
            "Step 83 Loss:  tensor(0.7997, grad_fn=<MulBackward0>)\n",
            "Step 84 Loss:  tensor(0.7968, grad_fn=<MulBackward0>)\n",
            "Step 85 Loss:  tensor(0.7940, grad_fn=<MulBackward0>)\n",
            "Step 86 Loss:  tensor(0.7912, grad_fn=<MulBackward0>)\n",
            "Step 87 Loss:  tensor(0.7883, grad_fn=<MulBackward0>)\n",
            "Step 88 Loss:  tensor(0.7855, grad_fn=<MulBackward0>)\n",
            "Step 89 Loss:  tensor(0.7795, grad_fn=<MulBackward0>)\n",
            "Step 90 Loss:  tensor(0.7767, grad_fn=<MulBackward0>)\n",
            "Step 91 Loss:  tensor(0.7740, grad_fn=<MulBackward0>)\n",
            "Step 92 Loss:  tensor(0.7712, grad_fn=<MulBackward0>)\n",
            "Step 93 Loss:  tensor(0.7685, grad_fn=<MulBackward0>)\n",
            "Step 94 Loss:  tensor(0.7658, grad_fn=<MulBackward0>)\n",
            "Step 95 Loss:  tensor(0.7631, grad_fn=<MulBackward0>)\n",
            "Step 96 Loss:  tensor(0.7605, grad_fn=<MulBackward0>)\n",
            "Step 97 Loss:  tensor(0.7579, grad_fn=<MulBackward0>)\n",
            "Step 98 Loss:  tensor(0.7552, grad_fn=<MulBackward0>)\n",
            "Step 99 Loss:  tensor(0.7526, grad_fn=<MulBackward0>)\n",
            "Step 100 Loss:  tensor(0.7500, grad_fn=<MulBackward0>)\n",
            "Step 101 Loss:  tensor(0.7347, grad_fn=<MulBackward0>)\n",
            "Step 102 Loss:  tensor(0.7300, grad_fn=<MulBackward0>)\n",
            "Step 103 Loss:  tensor(0.7273, grad_fn=<MulBackward0>)\n",
            "Step 104 Loss:  tensor(0.7246, grad_fn=<MulBackward0>)\n",
            "Step 105 Loss:  tensor(0.7219, grad_fn=<MulBackward0>)\n",
            "Step 106 Loss:  tensor(0.7193, grad_fn=<MulBackward0>)\n",
            "Step 107 Loss:  tensor(0.7166, grad_fn=<MulBackward0>)\n",
            "Step 108 Loss:  tensor(0.7140, grad_fn=<MulBackward0>)\n",
            "Step 109 Loss:  tensor(0.7114, grad_fn=<MulBackward0>)\n",
            "Step 110 Loss:  tensor(0.7088, grad_fn=<MulBackward0>)\n",
            "Step 111 Loss:  tensor(0.7049, grad_fn=<MulBackward0>)\n",
            "Step 112 Loss:  tensor(0.7023, grad_fn=<MulBackward0>)\n",
            "Step 113 Loss:  tensor(0.6998, grad_fn=<MulBackward0>)\n",
            "Step 114 Loss:  tensor(0.6973, grad_fn=<MulBackward0>)\n",
            "Step 115 Loss:  tensor(0.6948, grad_fn=<MulBackward0>)\n",
            "Step 116 Loss:  tensor(0.6923, grad_fn=<MulBackward0>)\n",
            "Step 117 Loss:  tensor(0.6887, grad_fn=<MulBackward0>)\n",
            "Step 118 Loss:  tensor(0.6863, grad_fn=<MulBackward0>)\n",
            "Step 119 Loss:  tensor(0.6838, grad_fn=<MulBackward0>)\n",
            "Step 120 Loss:  tensor(0.6814, grad_fn=<MulBackward0>)\n",
            "Step 121 Loss:  tensor(0.6790, grad_fn=<MulBackward0>)\n",
            "Step 122 Loss:  tensor(0.6766, grad_fn=<MulBackward0>)\n",
            "Step 123 Loss:  tensor(0.6733, grad_fn=<MulBackward0>)\n",
            "Step 124 Loss:  tensor(0.6710, grad_fn=<MulBackward0>)\n",
            "Step 125 Loss:  tensor(0.6686, grad_fn=<MulBackward0>)\n",
            "Step 126 Loss:  tensor(0.6663, grad_fn=<MulBackward0>)\n",
            "Step 127 Loss:  tensor(0.6632, grad_fn=<MulBackward0>)\n",
            "Step 128 Loss:  tensor(0.6609, grad_fn=<MulBackward0>)\n",
            "Step 129 Loss:  tensor(0.6553, grad_fn=<MulBackward0>)\n",
            "Step 130 Loss:  tensor(0.6531, grad_fn=<MulBackward0>)\n",
            "Step 131 Loss:  tensor(0.6501, grad_fn=<MulBackward0>)\n",
            "Step 132 Loss:  tensor(0.6479, grad_fn=<MulBackward0>)\n",
            "Step 133 Loss:  tensor(0.6457, grad_fn=<MulBackward0>)\n",
            "Step 134 Loss:  tensor(0.6428, grad_fn=<MulBackward0>)\n",
            "Step 135 Loss:  tensor(0.6406, grad_fn=<MulBackward0>)\n",
            "Step 136 Loss:  tensor(0.6384, grad_fn=<MulBackward0>)\n",
            "Step 137 Loss:  tensor(0.6357, grad_fn=<MulBackward0>)\n",
            "Step 138 Loss:  tensor(0.6336, grad_fn=<MulBackward0>)\n",
            "Step 139 Loss:  tensor(0.6314, grad_fn=<MulBackward0>)\n",
            "Step 140 Loss:  tensor(0.6288, grad_fn=<MulBackward0>)\n",
            "Step 141 Loss:  tensor(0.6267, grad_fn=<MulBackward0>)\n",
            "Step 142 Loss:  tensor(0.6226, grad_fn=<MulBackward0>)\n",
            "Step 143 Loss:  tensor(0.6200, grad_fn=<MulBackward0>)\n",
            "Step 144 Loss:  tensor(0.6179, grad_fn=<MulBackward0>)\n",
            "Step 145 Loss:  tensor(0.6155, grad_fn=<MulBackward0>)\n",
            "Step 146 Loss:  tensor(0.6134, grad_fn=<MulBackward0>)\n",
            "Step 147 Loss:  tensor(0.6110, grad_fn=<MulBackward0>)\n",
            "Step 148 Loss:  tensor(0.6089, grad_fn=<MulBackward0>)\n",
            "Step 149 Loss:  tensor(0.6065, grad_fn=<MulBackward0>)\n",
            "Step 150 Loss:  tensor(0.6045, grad_fn=<MulBackward0>)\n",
            "Step 151 Loss:  tensor(0.6022, grad_fn=<MulBackward0>)\n",
            "Step 152 Loss:  tensor(0.5988, grad_fn=<MulBackward0>)\n",
            "Step 153 Loss:  tensor(0.5965, grad_fn=<MulBackward0>)\n",
            "Step 154 Loss:  tensor(0.5945, grad_fn=<MulBackward0>)\n",
            "Step 155 Loss:  tensor(0.5923, grad_fn=<MulBackward0>)\n",
            "Step 156 Loss:  tensor(0.5903, grad_fn=<MulBackward0>)\n",
            "Step 157 Loss:  tensor(0.5881, grad_fn=<MulBackward0>)\n",
            "Step 158 Loss:  tensor(0.5859, grad_fn=<MulBackward0>)\n",
            "Step 159 Loss:  tensor(0.5829, grad_fn=<MulBackward0>)\n",
            "Step 160 Loss:  tensor(0.5807, grad_fn=<MulBackward0>)\n",
            "Step 161 Loss:  tensor(0.5786, grad_fn=<MulBackward0>)\n",
            "Step 162 Loss:  tensor(0.5765, grad_fn=<MulBackward0>)\n",
            "Step 163 Loss:  tensor(0.5746, grad_fn=<MulBackward0>)\n",
            "Step 164 Loss:  tensor(0.5716, grad_fn=<MulBackward0>)\n",
            "Step 165 Loss:  tensor(0.5696, grad_fn=<MulBackward0>)\n",
            "Step 166 Loss:  tensor(0.5675, grad_fn=<MulBackward0>)\n",
            "Step 167 Loss:  tensor(0.5655, grad_fn=<MulBackward0>)\n",
            "Step 168 Loss:  tensor(0.5635, grad_fn=<MulBackward0>)\n",
            "Step 169 Loss:  tensor(0.5607, grad_fn=<MulBackward0>)\n",
            "Step 170 Loss:  tensor(0.5588, grad_fn=<MulBackward0>)\n",
            "Step 171 Loss:  tensor(0.5568, grad_fn=<MulBackward0>)\n",
            "Step 172 Loss:  tensor(0.5549, grad_fn=<MulBackward0>)\n",
            "Step 173 Loss:  tensor(0.5523, grad_fn=<MulBackward0>)\n",
            "Step 174 Loss:  tensor(0.5503, grad_fn=<MulBackward0>)\n",
            "Step 175 Loss:  tensor(0.5484, grad_fn=<MulBackward0>)\n",
            "Step 176 Loss:  tensor(0.5466, grad_fn=<MulBackward0>)\n",
            "Step 177 Loss:  tensor(0.5441, grad_fn=<MulBackward0>)\n",
            "Step 178 Loss:  tensor(0.5422, grad_fn=<MulBackward0>)\n",
            "Step 179 Loss:  tensor(0.5402, grad_fn=<MulBackward0>)\n",
            "Step 180 Loss:  tensor(0.5379, grad_fn=<MulBackward0>)\n",
            "Step 181 Loss:  tensor(0.5361, grad_fn=<MulBackward0>)\n",
            "Step 182 Loss:  tensor(0.5341, grad_fn=<MulBackward0>)\n",
            "Step 183 Loss:  tensor(0.5319, grad_fn=<MulBackward0>)\n",
            "Step 184 Loss:  tensor(0.5300, grad_fn=<MulBackward0>)\n",
            "Step 185 Loss:  tensor(0.5282, grad_fn=<MulBackward0>)\n",
            "Step 186 Loss:  tensor(0.5259, grad_fn=<MulBackward0>)\n",
            "Step 187 Loss:  tensor(0.5241, grad_fn=<MulBackward0>)\n",
            "Step 188 Loss:  tensor(0.5223, grad_fn=<MulBackward0>)\n",
            "Step 189 Loss:  tensor(0.5201, grad_fn=<MulBackward0>)\n",
            "Step 190 Loss:  tensor(0.5184, grad_fn=<MulBackward0>)\n",
            "Step 191 Loss:  tensor(0.5162, grad_fn=<MulBackward0>)\n",
            "Step 192 Loss:  tensor(0.5144, grad_fn=<MulBackward0>)\n",
            "Step 193 Loss:  tensor(0.5123, grad_fn=<MulBackward0>)\n",
            "Step 194 Loss:  tensor(0.5106, grad_fn=<MulBackward0>)\n",
            "Step 195 Loss:  tensor(0.5085, grad_fn=<MulBackward0>)\n",
            "Step 196 Loss:  tensor(0.5068, grad_fn=<MulBackward0>)\n",
            "Step 197 Loss:  tensor(0.5048, grad_fn=<MulBackward0>)\n",
            "Step 198 Loss:  tensor(0.5030, grad_fn=<MulBackward0>)\n",
            "Step 199 Loss:  tensor(0.5010, grad_fn=<MulBackward0>)\n",
            "Step 200 Loss:  tensor(0.4991, grad_fn=<MulBackward0>)\n",
            "Step 201 Loss:  tensor(0.4974, grad_fn=<MulBackward0>)\n",
            "Step 202 Loss:  tensor(0.4954, grad_fn=<MulBackward0>)\n",
            "Step 203 Loss:  tensor(0.4937, grad_fn=<MulBackward0>)\n",
            "Step 204 Loss:  tensor(0.4918, grad_fn=<MulBackward0>)\n",
            "Step 205 Loss:  tensor(0.4899, grad_fn=<MulBackward0>)\n",
            "Step 206 Loss:  tensor(0.4880, grad_fn=<MulBackward0>)\n",
            "Step 207 Loss:  tensor(0.4864, grad_fn=<MulBackward0>)\n",
            "Step 208 Loss:  tensor(0.4846, grad_fn=<MulBackward0>)\n",
            "Step 209 Loss:  tensor(0.4827, grad_fn=<MulBackward0>)\n",
            "Step 210 Loss:  tensor(0.4808, grad_fn=<MulBackward0>)\n",
            "Step 211 Loss:  tensor(0.4792, grad_fn=<MulBackward0>)\n",
            "Step 212 Loss:  tensor(0.4774, grad_fn=<MulBackward0>)\n",
            "Step 213 Loss:  tensor(0.4757, grad_fn=<MulBackward0>)\n",
            "Step 214 Loss:  tensor(0.4739, grad_fn=<MulBackward0>)\n",
            "Step 215 Loss:  tensor(0.4721, grad_fn=<MulBackward0>)\n",
            "Step 216 Loss:  tensor(0.4704, grad_fn=<MulBackward0>)\n",
            "Step 217 Loss:  tensor(0.4686, grad_fn=<MulBackward0>)\n",
            "Step 218 Loss:  tensor(0.4669, grad_fn=<MulBackward0>)\n",
            "Step 219 Loss:  tensor(0.4652, grad_fn=<MulBackward0>)\n",
            "Step 220 Loss:  tensor(0.4635, grad_fn=<MulBackward0>)\n",
            "Step 221 Loss:  tensor(0.4618, grad_fn=<MulBackward0>)\n",
            "Step 222 Loss:  tensor(0.4601, grad_fn=<MulBackward0>)\n",
            "Step 223 Loss:  tensor(0.4583, grad_fn=<MulBackward0>)\n",
            "Step 224 Loss:  tensor(0.4567, grad_fn=<MulBackward0>)\n",
            "Step 225 Loss:  tensor(0.4550, grad_fn=<MulBackward0>)\n",
            "Step 226 Loss:  tensor(0.4534, grad_fn=<MulBackward0>)\n",
            "Step 227 Loss:  tensor(0.4516, grad_fn=<MulBackward0>)\n",
            "Step 228 Loss:  tensor(0.4500, grad_fn=<MulBackward0>)\n",
            "Step 229 Loss:  tensor(0.4484, grad_fn=<MulBackward0>)\n",
            "Step 230 Loss:  tensor(0.4467, grad_fn=<MulBackward0>)\n",
            "Step 231 Loss:  tensor(0.4451, grad_fn=<MulBackward0>)\n",
            "Step 232 Loss:  tensor(0.4434, grad_fn=<MulBackward0>)\n",
            "Step 233 Loss:  tensor(0.4418, grad_fn=<MulBackward0>)\n",
            "Step 234 Loss:  tensor(0.4402, grad_fn=<MulBackward0>)\n",
            "Step 235 Loss:  tensor(0.4386, grad_fn=<MulBackward0>)\n",
            "Step 236 Loss:  tensor(0.4369, grad_fn=<MulBackward0>)\n",
            "Step 237 Loss:  tensor(0.4354, grad_fn=<MulBackward0>)\n",
            "Step 238 Loss:  tensor(0.4338, grad_fn=<MulBackward0>)\n",
            "Step 239 Loss:  tensor(0.4322, grad_fn=<MulBackward0>)\n",
            "Step 240 Loss:  tensor(0.4306, grad_fn=<MulBackward0>)\n",
            "Step 241 Loss:  tensor(0.4290, grad_fn=<MulBackward0>)\n",
            "Step 242 Loss:  tensor(0.4275, grad_fn=<MulBackward0>)\n",
            "Step 243 Loss:  tensor(0.4259, grad_fn=<MulBackward0>)\n",
            "Step 244 Loss:  tensor(0.4243, grad_fn=<MulBackward0>)\n",
            "Step 245 Loss:  tensor(0.4227, grad_fn=<MulBackward0>)\n",
            "Step 246 Loss:  tensor(0.4212, grad_fn=<MulBackward0>)\n",
            "Step 247 Loss:  tensor(0.4197, grad_fn=<MulBackward0>)\n",
            "Step 248 Loss:  tensor(0.4181, grad_fn=<MulBackward0>)\n",
            "Step 249 Loss:  tensor(0.4166, grad_fn=<MulBackward0>)\n",
            "Step 250 Loss:  tensor(0.4151, grad_fn=<MulBackward0>)\n",
            "Step 251 Loss:  tensor(0.4135, grad_fn=<MulBackward0>)\n",
            "Step 252 Loss:  tensor(0.4121, grad_fn=<MulBackward0>)\n",
            "Step 253 Loss:  tensor(0.4105, grad_fn=<MulBackward0>)\n",
            "Step 254 Loss:  tensor(0.4090, grad_fn=<MulBackward0>)\n",
            "Step 255 Loss:  tensor(0.4075, grad_fn=<MulBackward0>)\n",
            "Step 256 Loss:  tensor(0.4061, grad_fn=<MulBackward0>)\n",
            "Step 257 Loss:  tensor(0.4046, grad_fn=<MulBackward0>)\n",
            "Step 258 Loss:  tensor(0.4031, grad_fn=<MulBackward0>)\n",
            "Step 259 Loss:  tensor(0.4016, grad_fn=<MulBackward0>)\n",
            "Step 260 Loss:  tensor(0.4001, grad_fn=<MulBackward0>)\n",
            "Step 261 Loss:  tensor(0.3987, grad_fn=<MulBackward0>)\n",
            "Step 262 Loss:  tensor(0.3972, grad_fn=<MulBackward0>)\n",
            "Step 263 Loss:  tensor(0.3958, grad_fn=<MulBackward0>)\n",
            "Step 264 Loss:  tensor(0.3943, grad_fn=<MulBackward0>)\n",
            "Step 265 Loss:  tensor(0.3929, grad_fn=<MulBackward0>)\n",
            "Step 266 Loss:  tensor(0.3914, grad_fn=<MulBackward0>)\n",
            "Step 267 Loss:  tensor(0.3900, grad_fn=<MulBackward0>)\n",
            "Step 268 Loss:  tensor(0.3886, grad_fn=<MulBackward0>)\n",
            "Step 269 Loss:  tensor(0.3872, grad_fn=<MulBackward0>)\n",
            "Step 270 Loss:  tensor(0.3857, grad_fn=<MulBackward0>)\n",
            "Step 271 Loss:  tensor(0.3843, grad_fn=<MulBackward0>)\n",
            "Step 272 Loss:  tensor(0.3829, grad_fn=<MulBackward0>)\n",
            "Step 273 Loss:  tensor(0.3815, grad_fn=<MulBackward0>)\n",
            "Step 274 Loss:  tensor(0.3801, grad_fn=<MulBackward0>)\n",
            "Step 275 Loss:  tensor(0.3787, grad_fn=<MulBackward0>)\n",
            "Step 276 Loss:  tensor(0.3774, grad_fn=<MulBackward0>)\n",
            "Step 277 Loss:  tensor(0.3760, grad_fn=<MulBackward0>)\n",
            "Step 278 Loss:  tensor(0.3746, grad_fn=<MulBackward0>)\n",
            "Step 279 Loss:  tensor(0.3732, grad_fn=<MulBackward0>)\n",
            "Step 280 Loss:  tensor(0.3719, grad_fn=<MulBackward0>)\n",
            "Step 281 Loss:  tensor(0.3705, grad_fn=<MulBackward0>)\n",
            "Step 282 Loss:  tensor(0.3692, grad_fn=<MulBackward0>)\n",
            "Step 283 Loss:  tensor(0.3678, grad_fn=<MulBackward0>)\n",
            "Step 284 Loss:  tensor(0.3665, grad_fn=<MulBackward0>)\n",
            "Step 285 Loss:  tensor(0.3651, grad_fn=<MulBackward0>)\n",
            "Step 286 Loss:  tensor(0.3638, grad_fn=<MulBackward0>)\n",
            "Step 287 Loss:  tensor(0.3625, grad_fn=<MulBackward0>)\n",
            "Step 288 Loss:  tensor(0.3611, grad_fn=<MulBackward0>)\n",
            "Step 289 Loss:  tensor(0.3598, grad_fn=<MulBackward0>)\n",
            "Step 290 Loss:  tensor(0.3585, grad_fn=<MulBackward0>)\n",
            "Step 291 Loss:  tensor(0.3572, grad_fn=<MulBackward0>)\n",
            "Step 292 Loss:  tensor(0.3559, grad_fn=<MulBackward0>)\n",
            "Step 293 Loss:  tensor(0.3546, grad_fn=<MulBackward0>)\n",
            "Step 294 Loss:  tensor(0.3533, grad_fn=<MulBackward0>)\n",
            "Step 295 Loss:  tensor(0.3520, grad_fn=<MulBackward0>)\n",
            "Step 296 Loss:  tensor(0.3507, grad_fn=<MulBackward0>)\n",
            "Step 297 Loss:  tensor(0.3494, grad_fn=<MulBackward0>)\n",
            "Step 298 Loss:  tensor(0.3482, grad_fn=<MulBackward0>)\n",
            "Step 299 Loss:  tensor(0.3469, grad_fn=<MulBackward0>)\n",
            "Step 300 Loss:  tensor(0.3456, grad_fn=<MulBackward0>)\n",
            "Step 301 Loss:  tensor(0.3444, grad_fn=<MulBackward0>)\n",
            "Step 302 Loss:  tensor(0.3431, grad_fn=<MulBackward0>)\n",
            "Step 303 Loss:  tensor(0.3418, grad_fn=<MulBackward0>)\n",
            "Step 304 Loss:  tensor(0.3406, grad_fn=<MulBackward0>)\n",
            "Step 305 Loss:  tensor(0.3394, grad_fn=<MulBackward0>)\n",
            "Step 306 Loss:  tensor(0.3381, grad_fn=<MulBackward0>)\n",
            "Step 307 Loss:  tensor(0.3369, grad_fn=<MulBackward0>)\n",
            "Step 308 Loss:  tensor(0.3357, grad_fn=<MulBackward0>)\n",
            "Step 309 Loss:  tensor(0.3344, grad_fn=<MulBackward0>)\n",
            "Step 310 Loss:  tensor(0.3332, grad_fn=<MulBackward0>)\n",
            "Step 311 Loss:  tensor(0.3320, grad_fn=<MulBackward0>)\n",
            "Step 312 Loss:  tensor(0.3308, grad_fn=<MulBackward0>)\n",
            "Step 313 Loss:  tensor(0.3296, grad_fn=<MulBackward0>)\n",
            "Step 314 Loss:  tensor(0.3284, grad_fn=<MulBackward0>)\n",
            "Step 315 Loss:  tensor(0.3272, grad_fn=<MulBackward0>)\n",
            "Step 316 Loss:  tensor(0.3260, grad_fn=<MulBackward0>)\n",
            "Step 317 Loss:  tensor(0.3120, grad_fn=<MulBackward0>)\n",
            "Step 318 Loss:  tensor(0.3107, grad_fn=<MulBackward0>)\n",
            "Step 319 Loss:  tensor(0.3093, grad_fn=<MulBackward0>)\n",
            "Step 320 Loss:  tensor(0.3080, grad_fn=<MulBackward0>)\n",
            "Step 321 Loss:  tensor(0.3067, grad_fn=<MulBackward0>)\n",
            "Step 322 Loss:  tensor(0.3054, grad_fn=<MulBackward0>)\n",
            "Step 323 Loss:  tensor(0.3041, grad_fn=<MulBackward0>)\n",
            "Step 324 Loss:  tensor(0.3028, grad_fn=<MulBackward0>)\n",
            "Step 325 Loss:  tensor(0.3015, grad_fn=<MulBackward0>)\n",
            "Step 326 Loss:  tensor(0.3002, grad_fn=<MulBackward0>)\n",
            "Step 327 Loss:  tensor(0.2989, grad_fn=<MulBackward0>)\n",
            "Step 328 Loss:  tensor(0.2976, grad_fn=<MulBackward0>)\n",
            "Step 329 Loss:  tensor(0.2963, grad_fn=<MulBackward0>)\n",
            "Step 330 Loss:  tensor(0.2951, grad_fn=<MulBackward0>)\n",
            "Step 331 Loss:  tensor(0.2938, grad_fn=<MulBackward0>)\n",
            "Step 332 Loss:  tensor(0.2925, grad_fn=<MulBackward0>)\n",
            "Step 333 Loss:  tensor(0.2913, grad_fn=<MulBackward0>)\n",
            "Step 334 Loss:  tensor(0.2900, grad_fn=<MulBackward0>)\n",
            "Step 335 Loss:  tensor(0.2888, grad_fn=<MulBackward0>)\n",
            "Step 336 Loss:  tensor(0.2875, grad_fn=<MulBackward0>)\n",
            "Step 337 Loss:  tensor(0.2863, grad_fn=<MulBackward0>)\n",
            "Step 338 Loss:  tensor(0.2851, grad_fn=<MulBackward0>)\n",
            "Step 339 Loss:  tensor(0.2839, grad_fn=<MulBackward0>)\n",
            "Step 340 Loss:  tensor(0.2826, grad_fn=<MulBackward0>)\n",
            "Step 341 Loss:  tensor(0.2814, grad_fn=<MulBackward0>)\n",
            "Step 342 Loss:  tensor(0.2802, grad_fn=<MulBackward0>)\n",
            "Step 343 Loss:  tensor(0.2790, grad_fn=<MulBackward0>)\n",
            "Step 344 Loss:  tensor(0.2778, grad_fn=<MulBackward0>)\n",
            "Step 345 Loss:  tensor(0.2766, grad_fn=<MulBackward0>)\n",
            "Step 346 Loss:  tensor(0.2755, grad_fn=<MulBackward0>)\n",
            "Step 347 Loss:  tensor(0.2710, grad_fn=<MulBackward0>)\n",
            "Step 348 Loss:  tensor(0.2698, grad_fn=<MulBackward0>)\n",
            "Step 349 Loss:  tensor(0.2686, grad_fn=<MulBackward0>)\n",
            "Step 350 Loss:  tensor(0.2675, grad_fn=<MulBackward0>)\n",
            "Step 351 Loss:  tensor(0.2663, grad_fn=<MulBackward0>)\n",
            "Step 352 Loss:  tensor(0.2651, grad_fn=<MulBackward0>)\n",
            "Step 353 Loss:  tensor(0.2640, grad_fn=<MulBackward0>)\n",
            "Step 354 Loss:  tensor(0.2628, grad_fn=<MulBackward0>)\n",
            "Step 355 Loss:  tensor(0.2617, grad_fn=<MulBackward0>)\n",
            "Step 356 Loss:  tensor(0.2606, grad_fn=<MulBackward0>)\n",
            "Step 357 Loss:  tensor(0.2594, grad_fn=<MulBackward0>)\n",
            "Step 358 Loss:  tensor(0.2583, grad_fn=<MulBackward0>)\n",
            "Step 359 Loss:  tensor(0.2572, grad_fn=<MulBackward0>)\n",
            "Step 360 Loss:  tensor(0.2561, grad_fn=<MulBackward0>)\n",
            "Step 361 Loss:  tensor(0.2530, grad_fn=<MulBackward0>)\n",
            "Step 362 Loss:  tensor(0.2519, grad_fn=<MulBackward0>)\n",
            "Step 363 Loss:  tensor(0.2508, grad_fn=<MulBackward0>)\n",
            "Step 364 Loss:  tensor(0.2497, grad_fn=<MulBackward0>)\n",
            "Step 365 Loss:  tensor(0.2486, grad_fn=<MulBackward0>)\n",
            "Step 366 Loss:  tensor(0.2475, grad_fn=<MulBackward0>)\n",
            "Step 367 Loss:  tensor(0.2464, grad_fn=<MulBackward0>)\n",
            "Step 368 Loss:  tensor(0.2453, grad_fn=<MulBackward0>)\n",
            "Step 369 Loss:  tensor(0.2443, grad_fn=<MulBackward0>)\n",
            "Step 370 Loss:  tensor(0.2432, grad_fn=<MulBackward0>)\n",
            "Step 371 Loss:  tensor(0.2407, grad_fn=<MulBackward0>)\n",
            "Step 372 Loss:  tensor(0.2397, grad_fn=<MulBackward0>)\n",
            "Step 373 Loss:  tensor(0.2386, grad_fn=<MulBackward0>)\n",
            "Step 374 Loss:  tensor(0.2376, grad_fn=<MulBackward0>)\n",
            "Step 375 Loss:  tensor(0.2365, grad_fn=<MulBackward0>)\n",
            "Step 376 Loss:  tensor(0.2355, grad_fn=<MulBackward0>)\n",
            "Step 377 Loss:  tensor(0.2345, grad_fn=<MulBackward0>)\n",
            "Step 378 Loss:  tensor(0.2324, grad_fn=<MulBackward0>)\n",
            "Step 379 Loss:  tensor(0.2313, grad_fn=<MulBackward0>)\n",
            "Step 380 Loss:  tensor(0.2303, grad_fn=<MulBackward0>)\n",
            "Step 381 Loss:  tensor(0.2293, grad_fn=<MulBackward0>)\n",
            "Step 382 Loss:  tensor(0.2283, grad_fn=<MulBackward0>)\n",
            "Step 383 Loss:  tensor(0.2273, grad_fn=<MulBackward0>)\n",
            "Step 384 Loss:  tensor(0.2254, grad_fn=<MulBackward0>)\n",
            "Step 385 Loss:  tensor(0.2244, grad_fn=<MulBackward0>)\n",
            "Step 386 Loss:  tensor(0.2235, grad_fn=<MulBackward0>)\n",
            "Step 387 Loss:  tensor(0.2225, grad_fn=<MulBackward0>)\n",
            "Step 388 Loss:  tensor(0.2215, grad_fn=<MulBackward0>)\n",
            "Step 389 Loss:  tensor(0.2198, grad_fn=<MulBackward0>)\n",
            "Step 390 Loss:  tensor(0.2188, grad_fn=<MulBackward0>)\n",
            "Step 391 Loss:  tensor(0.2179, grad_fn=<MulBackward0>)\n",
            "Step 392 Loss:  tensor(0.2169, grad_fn=<MulBackward0>)\n",
            "Step 393 Loss:  tensor(0.2153, grad_fn=<MulBackward0>)\n",
            "Step 394 Loss:  tensor(0.2144, grad_fn=<MulBackward0>)\n",
            "Step 395 Loss:  tensor(0.2134, grad_fn=<MulBackward0>)\n",
            "Step 396 Loss:  tensor(0.2125, grad_fn=<MulBackward0>)\n",
            "Step 397 Loss:  tensor(0.2110, grad_fn=<MulBackward0>)\n",
            "Step 398 Loss:  tensor(0.2101, grad_fn=<MulBackward0>)\n",
            "Step 399 Loss:  tensor(0.2092, grad_fn=<MulBackward0>)\n",
            "Step 400 Loss:  tensor(0.2078, grad_fn=<MulBackward0>)\n",
            "Step 401 Loss:  tensor(0.2068, grad_fn=<MulBackward0>)\n",
            "Step 402 Loss:  tensor(0.2060, grad_fn=<MulBackward0>)\n",
            "Step 403 Loss:  tensor(0.2046, grad_fn=<MulBackward0>)\n",
            "Step 404 Loss:  tensor(0.2037, grad_fn=<MulBackward0>)\n",
            "Step 405 Loss:  tensor(0.2028, grad_fn=<MulBackward0>)\n",
            "Step 406 Loss:  tensor(0.2015, grad_fn=<MulBackward0>)\n",
            "Step 407 Loss:  tensor(0.2006, grad_fn=<MulBackward0>)\n",
            "Step 408 Loss:  tensor(0.1998, grad_fn=<MulBackward0>)\n",
            "Step 409 Loss:  tensor(0.1985, grad_fn=<MulBackward0>)\n",
            "Step 410 Loss:  tensor(0.1977, grad_fn=<MulBackward0>)\n",
            "Step 411 Loss:  tensor(0.1965, grad_fn=<MulBackward0>)\n",
            "Step 412 Loss:  tensor(0.1956, grad_fn=<MulBackward0>)\n",
            "Step 413 Loss:  tensor(0.1948, grad_fn=<MulBackward0>)\n",
            "Step 414 Loss:  tensor(0.1936, grad_fn=<MulBackward0>)\n",
            "Step 415 Loss:  tensor(0.1928, grad_fn=<MulBackward0>)\n",
            "Step 416 Loss:  tensor(0.1916, grad_fn=<MulBackward0>)\n",
            "Step 417 Loss:  tensor(0.1908, grad_fn=<MulBackward0>)\n",
            "Step 418 Loss:  tensor(0.1897, grad_fn=<MulBackward0>)\n",
            "Step 419 Loss:  tensor(0.1889, grad_fn=<MulBackward0>)\n",
            "Step 420 Loss:  tensor(0.1878, grad_fn=<MulBackward0>)\n",
            "Step 421 Loss:  tensor(0.1867, grad_fn=<MulBackward0>)\n",
            "Step 422 Loss:  tensor(0.1860, grad_fn=<MulBackward0>)\n",
            "Step 423 Loss:  tensor(0.1849, grad_fn=<MulBackward0>)\n",
            "Step 424 Loss:  tensor(0.1841, grad_fn=<MulBackward0>)\n",
            "Step 425 Loss:  tensor(0.1831, grad_fn=<MulBackward0>)\n",
            "Step 426 Loss:  tensor(0.1821, grad_fn=<MulBackward0>)\n",
            "Step 427 Loss:  tensor(0.1813, grad_fn=<MulBackward0>)\n",
            "Step 428 Loss:  tensor(0.1804, grad_fn=<MulBackward0>)\n",
            "Step 429 Loss:  tensor(0.1794, grad_fn=<MulBackward0>)\n",
            "Step 430 Loss:  tensor(0.1787, grad_fn=<MulBackward0>)\n",
            "Step 431 Loss:  tensor(0.1777, grad_fn=<MulBackward0>)\n",
            "Step 432 Loss:  tensor(0.1768, grad_fn=<MulBackward0>)\n",
            "Step 433 Loss:  tensor(0.1758, grad_fn=<MulBackward0>)\n",
            "Step 434 Loss:  tensor(0.1749, grad_fn=<MulBackward0>)\n",
            "Step 435 Loss:  tensor(0.1742, grad_fn=<MulBackward0>)\n",
            "Step 436 Loss:  tensor(0.1733, grad_fn=<MulBackward0>)\n",
            "Step 437 Loss:  tensor(0.1724, grad_fn=<MulBackward0>)\n",
            "Step 438 Loss:  tensor(0.1716, grad_fn=<MulBackward0>)\n",
            "Step 439 Loss:  tensor(0.1707, grad_fn=<MulBackward0>)\n",
            "Step 440 Loss:  tensor(0.1699, grad_fn=<MulBackward0>)\n",
            "Step 441 Loss:  tensor(0.1690, grad_fn=<MulBackward0>)\n",
            "Step 442 Loss:  tensor(0.1682, grad_fn=<MulBackward0>)\n",
            "Step 443 Loss:  tensor(0.1674, grad_fn=<MulBackward0>)\n",
            "Step 444 Loss:  tensor(0.1665, grad_fn=<MulBackward0>)\n",
            "Step 445 Loss:  tensor(0.1657, grad_fn=<MulBackward0>)\n",
            "Step 446 Loss:  tensor(0.1649, grad_fn=<MulBackward0>)\n",
            "Step 447 Loss:  tensor(0.1641, grad_fn=<MulBackward0>)\n",
            "Step 448 Loss:  tensor(0.1634, grad_fn=<MulBackward0>)\n",
            "Step 449 Loss:  tensor(0.1625, grad_fn=<MulBackward0>)\n",
            "Step 450 Loss:  tensor(0.1617, grad_fn=<MulBackward0>)\n",
            "Step 451 Loss:  tensor(0.1609, grad_fn=<MulBackward0>)\n",
            "Step 452 Loss:  tensor(0.1602, grad_fn=<MulBackward0>)\n",
            "Step 453 Loss:  tensor(0.1593, grad_fn=<MulBackward0>)\n",
            "Step 454 Loss:  tensor(0.1586, grad_fn=<MulBackward0>)\n",
            "Step 455 Loss:  tensor(0.1579, grad_fn=<MulBackward0>)\n",
            "Step 456 Loss:  tensor(0.1571, grad_fn=<MulBackward0>)\n",
            "Step 457 Loss:  tensor(0.1563, grad_fn=<MulBackward0>)\n",
            "Step 458 Loss:  tensor(0.1555, grad_fn=<MulBackward0>)\n",
            "Step 459 Loss:  tensor(0.1549, grad_fn=<MulBackward0>)\n",
            "Step 460 Loss:  tensor(0.1541, grad_fn=<MulBackward0>)\n",
            "Step 461 Loss:  tensor(0.1534, grad_fn=<MulBackward0>)\n",
            "Step 462 Loss:  tensor(0.1526, grad_fn=<MulBackward0>)\n",
            "Step 463 Loss:  tensor(0.1519, grad_fn=<MulBackward0>)\n",
            "Step 464 Loss:  tensor(0.1512, grad_fn=<MulBackward0>)\n",
            "Step 465 Loss:  tensor(0.1505, grad_fn=<MulBackward0>)\n",
            "Step 466 Loss:  tensor(0.1497, grad_fn=<MulBackward0>)\n",
            "Step 467 Loss:  tensor(0.1490, grad_fn=<MulBackward0>)\n",
            "Step 468 Loss:  tensor(0.1483, grad_fn=<MulBackward0>)\n",
            "Step 469 Loss:  tensor(0.1476, grad_fn=<MulBackward0>)\n",
            "Step 470 Loss:  tensor(0.1469, grad_fn=<MulBackward0>)\n",
            "Step 471 Loss:  tensor(0.1462, grad_fn=<MulBackward0>)\n",
            "Step 472 Loss:  tensor(0.1456, grad_fn=<MulBackward0>)\n",
            "Step 473 Loss:  tensor(0.1449, grad_fn=<MulBackward0>)\n",
            "Step 474 Loss:  tensor(0.1442, grad_fn=<MulBackward0>)\n",
            "Step 475 Loss:  tensor(0.1436, grad_fn=<MulBackward0>)\n",
            "Step 476 Loss:  tensor(0.1429, grad_fn=<MulBackward0>)\n",
            "Step 477 Loss:  tensor(0.1422, grad_fn=<MulBackward0>)\n",
            "Step 478 Loss:  tensor(0.1416, grad_fn=<MulBackward0>)\n",
            "Step 479 Loss:  tensor(0.1409, grad_fn=<MulBackward0>)\n",
            "Step 480 Loss:  tensor(0.1403, grad_fn=<MulBackward0>)\n",
            "Step 481 Loss:  tensor(0.1396, grad_fn=<MulBackward0>)\n",
            "Step 482 Loss:  tensor(0.1390, grad_fn=<MulBackward0>)\n",
            "Step 483 Loss:  tensor(0.1383, grad_fn=<MulBackward0>)\n",
            "Step 484 Loss:  tensor(0.1377, grad_fn=<MulBackward0>)\n",
            "Step 485 Loss:  tensor(0.1371, grad_fn=<MulBackward0>)\n",
            "Step 486 Loss:  tensor(0.1365, grad_fn=<MulBackward0>)\n",
            "Step 487 Loss:  tensor(0.1359, grad_fn=<MulBackward0>)\n",
            "Step 488 Loss:  tensor(0.1352, grad_fn=<MulBackward0>)\n",
            "Step 489 Loss:  tensor(0.1346, grad_fn=<MulBackward0>)\n",
            "Step 490 Loss:  tensor(0.1340, grad_fn=<MulBackward0>)\n",
            "Step 491 Loss:  tensor(0.1334, grad_fn=<MulBackward0>)\n",
            "Step 492 Loss:  tensor(0.1328, grad_fn=<MulBackward0>)\n",
            "Step 493 Loss:  tensor(0.1322, grad_fn=<MulBackward0>)\n",
            "Step 494 Loss:  tensor(0.1316, grad_fn=<MulBackward0>)\n",
            "Step 495 Loss:  tensor(0.1310, grad_fn=<MulBackward0>)\n",
            "Step 496 Loss:  tensor(0.1305, grad_fn=<MulBackward0>)\n",
            "Step 497 Loss:  tensor(0.1299, grad_fn=<MulBackward0>)\n",
            "Step 498 Loss:  tensor(0.1293, grad_fn=<MulBackward0>)\n",
            "Step 499 Loss:  tensor(0.1288, grad_fn=<MulBackward0>)\n",
            "Step 500 Loss:  tensor(0.1282, grad_fn=<MulBackward0>)\n",
            "Step 501 Loss:  tensor(0.1276, grad_fn=<MulBackward0>)\n",
            "Step 502 Loss:  tensor(0.1271, grad_fn=<MulBackward0>)\n",
            "Step 503 Loss:  tensor(0.1265, grad_fn=<MulBackward0>)\n",
            "Step 504 Loss:  tensor(0.1260, grad_fn=<MulBackward0>)\n",
            "Step 505 Loss:  tensor(0.1254, grad_fn=<MulBackward0>)\n",
            "Step 506 Loss:  tensor(0.1249, grad_fn=<MulBackward0>)\n",
            "Step 507 Loss:  tensor(0.1243, grad_fn=<MulBackward0>)\n",
            "Step 508 Loss:  tensor(0.1238, grad_fn=<MulBackward0>)\n",
            "Step 509 Loss:  tensor(0.1233, grad_fn=<MulBackward0>)\n",
            "Step 510 Loss:  tensor(0.1227, grad_fn=<MulBackward0>)\n",
            "Step 511 Loss:  tensor(0.1222, grad_fn=<MulBackward0>)\n",
            "Step 512 Loss:  tensor(0.1217, grad_fn=<MulBackward0>)\n",
            "Step 513 Loss:  tensor(0.1211, grad_fn=<MulBackward0>)\n",
            "Step 514 Loss:  tensor(0.1206, grad_fn=<MulBackward0>)\n",
            "Step 515 Loss:  tensor(0.1201, grad_fn=<MulBackward0>)\n",
            "Step 516 Loss:  tensor(0.1196, grad_fn=<MulBackward0>)\n",
            "Step 517 Loss:  tensor(0.1191, grad_fn=<MulBackward0>)\n",
            "Step 518 Loss:  tensor(0.1186, grad_fn=<MulBackward0>)\n",
            "Step 519 Loss:  tensor(0.1181, grad_fn=<MulBackward0>)\n",
            "Step 520 Loss:  tensor(0.1176, grad_fn=<MulBackward0>)\n",
            "Step 521 Loss:  tensor(0.1171, grad_fn=<MulBackward0>)\n",
            "Step 522 Loss:  tensor(0.1166, grad_fn=<MulBackward0>)\n",
            "Step 523 Loss:  tensor(0.1161, grad_fn=<MulBackward0>)\n",
            "Step 524 Loss:  tensor(0.1156, grad_fn=<MulBackward0>)\n",
            "Step 525 Loss:  tensor(0.1151, grad_fn=<MulBackward0>)\n",
            "Step 526 Loss:  tensor(0.1147, grad_fn=<MulBackward0>)\n",
            "Step 527 Loss:  tensor(0.1142, grad_fn=<MulBackward0>)\n",
            "Step 528 Loss:  tensor(0.1137, grad_fn=<MulBackward0>)\n",
            "Step 529 Loss:  tensor(0.1132, grad_fn=<MulBackward0>)\n",
            "Step 530 Loss:  tensor(0.1128, grad_fn=<MulBackward0>)\n",
            "Step 531 Loss:  tensor(0.1123, grad_fn=<MulBackward0>)\n",
            "Step 532 Loss:  tensor(0.1118, grad_fn=<MulBackward0>)\n",
            "Step 533 Loss:  tensor(0.1114, grad_fn=<MulBackward0>)\n",
            "Step 534 Loss:  tensor(0.1109, grad_fn=<MulBackward0>)\n",
            "Step 535 Loss:  tensor(0.1105, grad_fn=<MulBackward0>)\n",
            "Step 536 Loss:  tensor(0.1100, grad_fn=<MulBackward0>)\n",
            "Step 537 Loss:  tensor(0.1096, grad_fn=<MulBackward0>)\n",
            "Step 538 Loss:  tensor(0.1091, grad_fn=<MulBackward0>)\n",
            "Step 539 Loss:  tensor(0.1087, grad_fn=<MulBackward0>)\n",
            "Step 540 Loss:  tensor(0.1083, grad_fn=<MulBackward0>)\n",
            "Step 541 Loss:  tensor(0.1078, grad_fn=<MulBackward0>)\n",
            "Step 542 Loss:  tensor(0.1074, grad_fn=<MulBackward0>)\n",
            "Step 543 Loss:  tensor(0.1070, grad_fn=<MulBackward0>)\n",
            "Step 544 Loss:  tensor(0.1065, grad_fn=<MulBackward0>)\n",
            "Step 545 Loss:  tensor(0.1061, grad_fn=<MulBackward0>)\n",
            "Step 546 Loss:  tensor(0.1057, grad_fn=<MulBackward0>)\n",
            "Step 547 Loss:  tensor(0.1052, grad_fn=<MulBackward0>)\n",
            "Step 548 Loss:  tensor(0.1048, grad_fn=<MulBackward0>)\n",
            "Step 549 Loss:  tensor(0.1044, grad_fn=<MulBackward0>)\n",
            "Step 550 Loss:  tensor(0.1040, grad_fn=<MulBackward0>)\n",
            "Step 551 Loss:  tensor(0.1036, grad_fn=<MulBackward0>)\n",
            "Step 552 Loss:  tensor(0.1032, grad_fn=<MulBackward0>)\n",
            "Step 553 Loss:  tensor(0.1028, grad_fn=<MulBackward0>)\n",
            "Step 554 Loss:  tensor(0.1024, grad_fn=<MulBackward0>)\n",
            "Step 555 Loss:  tensor(0.1020, grad_fn=<MulBackward0>)\n",
            "Step 556 Loss:  tensor(0.1016, grad_fn=<MulBackward0>)\n",
            "Step 557 Loss:  tensor(0.1012, grad_fn=<MulBackward0>)\n",
            "Step 558 Loss:  tensor(0.1008, grad_fn=<MulBackward0>)\n",
            "Step 559 Loss:  tensor(0.1004, grad_fn=<MulBackward0>)\n",
            "Step 560 Loss:  tensor(0.1000, grad_fn=<MulBackward0>)\n",
            "Step 561 Loss:  tensor(0.0996, grad_fn=<MulBackward0>)\n",
            "Step 562 Loss:  tensor(0.0992, grad_fn=<MulBackward0>)\n",
            "Step 563 Loss:  tensor(0.0988, grad_fn=<MulBackward0>)\n",
            "Step 564 Loss:  tensor(0.0984, grad_fn=<MulBackward0>)\n",
            "Step 565 Loss:  tensor(0.0981, grad_fn=<MulBackward0>)\n",
            "Step 566 Loss:  tensor(0.0977, grad_fn=<MulBackward0>)\n",
            "Step 567 Loss:  tensor(0.0973, grad_fn=<MulBackward0>)\n",
            "Step 568 Loss:  tensor(0.0969, grad_fn=<MulBackward0>)\n",
            "Step 569 Loss:  tensor(0.0965, grad_fn=<MulBackward0>)\n",
            "Step 570 Loss:  tensor(0.0962, grad_fn=<MulBackward0>)\n",
            "Step 571 Loss:  tensor(0.0958, grad_fn=<MulBackward0>)\n",
            "Step 572 Loss:  tensor(0.0954, grad_fn=<MulBackward0>)\n",
            "Step 573 Loss:  tensor(0.0951, grad_fn=<MulBackward0>)\n",
            "Step 574 Loss:  tensor(0.0947, grad_fn=<MulBackward0>)\n",
            "Step 575 Loss:  tensor(0.0943, grad_fn=<MulBackward0>)\n",
            "Step 576 Loss:  tensor(0.0940, grad_fn=<MulBackward0>)\n",
            "Step 577 Loss:  tensor(0.0936, grad_fn=<MulBackward0>)\n",
            "Step 578 Loss:  tensor(0.0933, grad_fn=<MulBackward0>)\n",
            "Step 579 Loss:  tensor(0.0929, grad_fn=<MulBackward0>)\n",
            "Step 580 Loss:  tensor(0.0926, grad_fn=<MulBackward0>)\n",
            "Step 581 Loss:  tensor(0.0922, grad_fn=<MulBackward0>)\n",
            "Step 582 Loss:  tensor(0.0919, grad_fn=<MulBackward0>)\n",
            "Step 583 Loss:  tensor(0.0915, grad_fn=<MulBackward0>)\n",
            "Step 584 Loss:  tensor(0.0912, grad_fn=<MulBackward0>)\n",
            "Step 585 Loss:  tensor(0.0908, grad_fn=<MulBackward0>)\n",
            "Step 586 Loss:  tensor(0.0905, grad_fn=<MulBackward0>)\n",
            "Step 587 Loss:  tensor(0.0902, grad_fn=<MulBackward0>)\n",
            "Step 588 Loss:  tensor(0.0898, grad_fn=<MulBackward0>)\n",
            "Step 589 Loss:  tensor(0.0895, grad_fn=<MulBackward0>)\n",
            "Step 590 Loss:  tensor(0.0891, grad_fn=<MulBackward0>)\n",
            "Step 591 Loss:  tensor(0.0888, grad_fn=<MulBackward0>)\n",
            "Step 592 Loss:  tensor(0.0885, grad_fn=<MulBackward0>)\n",
            "Step 593 Loss:  tensor(0.0882, grad_fn=<MulBackward0>)\n",
            "Step 594 Loss:  tensor(0.0878, grad_fn=<MulBackward0>)\n",
            "Step 595 Loss:  tensor(0.0875, grad_fn=<MulBackward0>)\n",
            "Step 596 Loss:  tensor(0.0872, grad_fn=<MulBackward0>)\n",
            "Step 597 Loss:  tensor(0.0869, grad_fn=<MulBackward0>)\n",
            "Step 598 Loss:  tensor(0.0865, grad_fn=<MulBackward0>)\n",
            "Step 599 Loss:  tensor(0.0862, grad_fn=<MulBackward0>)\n",
            "Step 600 Loss:  tensor(0.0859, grad_fn=<MulBackward0>)\n",
            "Step 601 Loss:  tensor(0.0856, grad_fn=<MulBackward0>)\n",
            "Step 602 Loss:  tensor(0.0853, grad_fn=<MulBackward0>)\n",
            "Step 603 Loss:  tensor(0.0850, grad_fn=<MulBackward0>)\n",
            "Step 604 Loss:  tensor(0.0846, grad_fn=<MulBackward0>)\n",
            "Step 605 Loss:  tensor(0.0843, grad_fn=<MulBackward0>)\n",
            "Step 606 Loss:  tensor(0.0840, grad_fn=<MulBackward0>)\n",
            "Step 607 Loss:  tensor(0.0837, grad_fn=<MulBackward0>)\n",
            "Step 608 Loss:  tensor(0.0834, grad_fn=<MulBackward0>)\n",
            "Step 609 Loss:  tensor(0.0831, grad_fn=<MulBackward0>)\n",
            "Step 610 Loss:  tensor(0.0828, grad_fn=<MulBackward0>)\n",
            "Step 611 Loss:  tensor(0.0825, grad_fn=<MulBackward0>)\n",
            "Step 612 Loss:  tensor(0.0822, grad_fn=<MulBackward0>)\n",
            "Step 613 Loss:  tensor(0.0819, grad_fn=<MulBackward0>)\n",
            "Step 614 Loss:  tensor(0.0816, grad_fn=<MulBackward0>)\n",
            "Step 615 Loss:  tensor(0.0813, grad_fn=<MulBackward0>)\n",
            "Step 616 Loss:  tensor(0.0810, grad_fn=<MulBackward0>)\n",
            "Step 617 Loss:  tensor(0.0807, grad_fn=<MulBackward0>)\n",
            "Step 618 Loss:  tensor(0.0804, grad_fn=<MulBackward0>)\n",
            "Step 619 Loss:  tensor(0.0801, grad_fn=<MulBackward0>)\n",
            "Step 620 Loss:  tensor(0.0798, grad_fn=<MulBackward0>)\n",
            "Step 621 Loss:  tensor(0.0796, grad_fn=<MulBackward0>)\n",
            "Step 622 Loss:  tensor(0.0793, grad_fn=<MulBackward0>)\n",
            "Step 623 Loss:  tensor(0.0790, grad_fn=<MulBackward0>)\n",
            "Step 624 Loss:  tensor(0.0787, grad_fn=<MulBackward0>)\n",
            "Step 625 Loss:  tensor(0.0784, grad_fn=<MulBackward0>)\n",
            "Step 626 Loss:  tensor(0.0781, grad_fn=<MulBackward0>)\n",
            "Step 627 Loss:  tensor(0.0778, grad_fn=<MulBackward0>)\n",
            "Step 628 Loss:  tensor(0.0776, grad_fn=<MulBackward0>)\n",
            "Step 629 Loss:  tensor(0.0773, grad_fn=<MulBackward0>)\n",
            "Step 630 Loss:  tensor(0.0770, grad_fn=<MulBackward0>)\n",
            "Step 631 Loss:  tensor(0.0767, grad_fn=<MulBackward0>)\n",
            "Step 632 Loss:  tensor(0.0765, grad_fn=<MulBackward0>)\n",
            "Step 633 Loss:  tensor(0.0762, grad_fn=<MulBackward0>)\n",
            "Step 634 Loss:  tensor(0.0759, grad_fn=<MulBackward0>)\n",
            "Step 635 Loss:  tensor(0.0756, grad_fn=<MulBackward0>)\n",
            "Step 636 Loss:  tensor(0.0754, grad_fn=<MulBackward0>)\n",
            "Step 637 Loss:  tensor(0.0751, grad_fn=<MulBackward0>)\n",
            "Step 638 Loss:  tensor(0.0748, grad_fn=<MulBackward0>)\n",
            "Step 639 Loss:  tensor(0.0746, grad_fn=<MulBackward0>)\n",
            "Step 640 Loss:  tensor(0.0743, grad_fn=<MulBackward0>)\n",
            "Step 641 Loss:  tensor(0.0740, grad_fn=<MulBackward0>)\n",
            "Step 642 Loss:  tensor(0.0738, grad_fn=<MulBackward0>)\n",
            "Step 643 Loss:  tensor(0.0735, grad_fn=<MulBackward0>)\n",
            "Step 644 Loss:  tensor(0.0733, grad_fn=<MulBackward0>)\n",
            "Step 645 Loss:  tensor(0.0730, grad_fn=<MulBackward0>)\n",
            "Step 646 Loss:  tensor(0.0727, grad_fn=<MulBackward0>)\n",
            "Step 647 Loss:  tensor(0.0725, grad_fn=<MulBackward0>)\n",
            "Step 648 Loss:  tensor(0.0722, grad_fn=<MulBackward0>)\n",
            "Step 649 Loss:  tensor(0.0720, grad_fn=<MulBackward0>)\n",
            "Step 650 Loss:  tensor(0.0717, grad_fn=<MulBackward0>)\n",
            "Step 651 Loss:  tensor(0.0715, grad_fn=<MulBackward0>)\n",
            "Step 652 Loss:  tensor(0.0712, grad_fn=<MulBackward0>)\n",
            "Step 653 Loss:  tensor(0.0710, grad_fn=<MulBackward0>)\n",
            "Step 654 Loss:  tensor(0.0707, grad_fn=<MulBackward0>)\n",
            "Step 655 Loss:  tensor(0.0705, grad_fn=<MulBackward0>)\n",
            "Step 656 Loss:  tensor(0.0702, grad_fn=<MulBackward0>)\n",
            "Step 657 Loss:  tensor(0.0700, grad_fn=<MulBackward0>)\n",
            "Step 658 Loss:  tensor(0.0697, grad_fn=<MulBackward0>)\n",
            "Step 659 Loss:  tensor(0.0695, grad_fn=<MulBackward0>)\n",
            "Step 660 Loss:  tensor(0.0692, grad_fn=<MulBackward0>)\n",
            "Step 661 Loss:  tensor(0.0690, grad_fn=<MulBackward0>)\n",
            "Step 662 Loss:  tensor(0.0687, grad_fn=<MulBackward0>)\n",
            "Step 663 Loss:  tensor(0.0685, grad_fn=<MulBackward0>)\n",
            "Step 664 Loss:  tensor(0.0683, grad_fn=<MulBackward0>)\n",
            "Step 665 Loss:  tensor(0.0680, grad_fn=<MulBackward0>)\n",
            "Step 666 Loss:  tensor(0.0678, grad_fn=<MulBackward0>)\n",
            "Step 667 Loss:  tensor(0.0675, grad_fn=<MulBackward0>)\n",
            "Step 668 Loss:  tensor(0.0673, grad_fn=<MulBackward0>)\n",
            "Step 669 Loss:  tensor(0.0671, grad_fn=<MulBackward0>)\n",
            "Step 670 Loss:  tensor(0.0668, grad_fn=<MulBackward0>)\n",
            "Step 671 Loss:  tensor(0.0666, grad_fn=<MulBackward0>)\n",
            "Step 672 Loss:  tensor(0.0664, grad_fn=<MulBackward0>)\n",
            "Step 673 Loss:  tensor(0.0661, grad_fn=<MulBackward0>)\n",
            "Step 674 Loss:  tensor(0.0659, grad_fn=<MulBackward0>)\n",
            "Step 675 Loss:  tensor(0.0657, grad_fn=<MulBackward0>)\n",
            "Step 676 Loss:  tensor(0.0654, grad_fn=<MulBackward0>)\n",
            "Step 677 Loss:  tensor(0.0652, grad_fn=<MulBackward0>)\n",
            "Step 678 Loss:  tensor(0.0650, grad_fn=<MulBackward0>)\n",
            "Step 679 Loss:  tensor(0.0648, grad_fn=<MulBackward0>)\n",
            "Step 680 Loss:  tensor(0.0645, grad_fn=<MulBackward0>)\n",
            "Step 681 Loss:  tensor(0.0643, grad_fn=<MulBackward0>)\n",
            "Step 682 Loss:  tensor(0.0641, grad_fn=<MulBackward0>)\n",
            "Step 683 Loss:  tensor(0.0639, grad_fn=<MulBackward0>)\n",
            "Step 684 Loss:  tensor(0.0636, grad_fn=<MulBackward0>)\n",
            "Step 685 Loss:  tensor(0.0634, grad_fn=<MulBackward0>)\n",
            "Step 686 Loss:  tensor(0.0632, grad_fn=<MulBackward0>)\n",
            "Step 687 Loss:  tensor(0.0630, grad_fn=<MulBackward0>)\n",
            "Step 688 Loss:  tensor(0.0628, grad_fn=<MulBackward0>)\n",
            "Step 689 Loss:  tensor(0.0626, grad_fn=<MulBackward0>)\n",
            "Step 690 Loss:  tensor(0.0623, grad_fn=<MulBackward0>)\n",
            "Step 691 Loss:  tensor(0.0621, grad_fn=<MulBackward0>)\n",
            "Step 692 Loss:  tensor(0.0619, grad_fn=<MulBackward0>)\n",
            "Step 693 Loss:  tensor(0.0617, grad_fn=<MulBackward0>)\n",
            "Step 694 Loss:  tensor(0.0615, grad_fn=<MulBackward0>)\n",
            "Step 695 Loss:  tensor(0.0613, grad_fn=<MulBackward0>)\n",
            "Step 696 Loss:  tensor(0.0611, grad_fn=<MulBackward0>)\n",
            "Step 697 Loss:  tensor(0.0608, grad_fn=<MulBackward0>)\n",
            "Step 698 Loss:  tensor(0.0606, grad_fn=<MulBackward0>)\n",
            "Step 699 Loss:  tensor(0.0604, grad_fn=<MulBackward0>)\n",
            "Step 700 Loss:  tensor(0.0602, grad_fn=<MulBackward0>)\n",
            "Step 701 Loss:  tensor(0.0600, grad_fn=<MulBackward0>)\n",
            "Step 702 Loss:  tensor(0.0598, grad_fn=<MulBackward0>)\n",
            "Step 703 Loss:  tensor(0.0596, grad_fn=<MulBackward0>)\n",
            "Step 704 Loss:  tensor(0.0594, grad_fn=<MulBackward0>)\n",
            "Step 705 Loss:  tensor(0.0592, grad_fn=<MulBackward0>)\n",
            "Step 706 Loss:  tensor(0.0590, grad_fn=<MulBackward0>)\n",
            "Step 707 Loss:  tensor(0.0588, grad_fn=<MulBackward0>)\n",
            "Step 708 Loss:  tensor(0.0586, grad_fn=<MulBackward0>)\n",
            "Step 709 Loss:  tensor(0.0584, grad_fn=<MulBackward0>)\n",
            "Step 710 Loss:  tensor(0.0582, grad_fn=<MulBackward0>)\n",
            "Step 711 Loss:  tensor(0.0580, grad_fn=<MulBackward0>)\n",
            "Step 712 Loss:  tensor(0.0578, grad_fn=<MulBackward0>)\n",
            "Step 713 Loss:  tensor(0.0576, grad_fn=<MulBackward0>)\n",
            "Step 714 Loss:  tensor(0.0574, grad_fn=<MulBackward0>)\n",
            "Step 715 Loss:  tensor(0.0572, grad_fn=<MulBackward0>)\n",
            "Step 716 Loss:  tensor(0.0570, grad_fn=<MulBackward0>)\n",
            "Step 717 Loss:  tensor(0.0568, grad_fn=<MulBackward0>)\n",
            "Step 718 Loss:  tensor(0.0566, grad_fn=<MulBackward0>)\n",
            "Step 719 Loss:  tensor(0.0564, grad_fn=<MulBackward0>)\n",
            "Step 720 Loss:  tensor(0.0562, grad_fn=<MulBackward0>)\n",
            "Step 721 Loss:  tensor(0.0560, grad_fn=<MulBackward0>)\n",
            "Step 722 Loss:  tensor(0.0558, grad_fn=<MulBackward0>)\n",
            "Step 723 Loss:  tensor(0.0556, grad_fn=<MulBackward0>)\n",
            "Step 724 Loss:  tensor(0.0555, grad_fn=<MulBackward0>)\n",
            "Step 725 Loss:  tensor(0.0553, grad_fn=<MulBackward0>)\n",
            "Step 726 Loss:  tensor(0.0551, grad_fn=<MulBackward0>)\n",
            "Step 727 Loss:  tensor(0.0549, grad_fn=<MulBackward0>)\n",
            "Step 728 Loss:  tensor(0.0547, grad_fn=<MulBackward0>)\n",
            "Step 729 Loss:  tensor(0.0545, grad_fn=<MulBackward0>)\n",
            "Step 730 Loss:  tensor(0.0543, grad_fn=<MulBackward0>)\n",
            "Step 731 Loss:  tensor(0.0541, grad_fn=<MulBackward0>)\n",
            "Step 732 Loss:  tensor(0.0540, grad_fn=<MulBackward0>)\n",
            "Step 733 Loss:  tensor(0.0538, grad_fn=<MulBackward0>)\n",
            "Step 734 Loss:  tensor(0.0536, grad_fn=<MulBackward0>)\n",
            "Step 735 Loss:  tensor(0.0534, grad_fn=<MulBackward0>)\n",
            "Step 736 Loss:  tensor(0.0532, grad_fn=<MulBackward0>)\n",
            "Step 737 Loss:  tensor(0.0531, grad_fn=<MulBackward0>)\n",
            "Step 738 Loss:  tensor(0.0529, grad_fn=<MulBackward0>)\n",
            "Step 739 Loss:  tensor(0.0527, grad_fn=<MulBackward0>)\n",
            "Step 740 Loss:  tensor(0.0525, grad_fn=<MulBackward0>)\n",
            "Step 741 Loss:  tensor(0.0523, grad_fn=<MulBackward0>)\n",
            "Step 742 Loss:  tensor(0.0522, grad_fn=<MulBackward0>)\n",
            "Step 743 Loss:  tensor(0.0520, grad_fn=<MulBackward0>)\n",
            "Step 744 Loss:  tensor(0.0518, grad_fn=<MulBackward0>)\n",
            "Step 745 Loss:  tensor(0.0516, grad_fn=<MulBackward0>)\n",
            "Step 746 Loss:  tensor(0.0515, grad_fn=<MulBackward0>)\n",
            "Step 747 Loss:  tensor(0.0513, grad_fn=<MulBackward0>)\n",
            "Step 748 Loss:  tensor(0.0511, grad_fn=<MulBackward0>)\n",
            "Step 749 Loss:  tensor(0.0509, grad_fn=<MulBackward0>)\n",
            "Step 750 Loss:  tensor(0.0508, grad_fn=<MulBackward0>)\n",
            "Step 751 Loss:  tensor(0.0506, grad_fn=<MulBackward0>)\n",
            "Step 752 Loss:  tensor(0.0504, grad_fn=<MulBackward0>)\n",
            "Step 753 Loss:  tensor(0.0503, grad_fn=<MulBackward0>)\n",
            "Step 754 Loss:  tensor(0.0501, grad_fn=<MulBackward0>)\n",
            "Step 755 Loss:  tensor(0.0499, grad_fn=<MulBackward0>)\n",
            "Step 756 Loss:  tensor(0.0498, grad_fn=<MulBackward0>)\n",
            "Step 757 Loss:  tensor(0.0496, grad_fn=<MulBackward0>)\n",
            "Step 758 Loss:  tensor(0.0494, grad_fn=<MulBackward0>)\n",
            "Step 759 Loss:  tensor(0.0493, grad_fn=<MulBackward0>)\n",
            "Step 760 Loss:  tensor(0.0491, grad_fn=<MulBackward0>)\n",
            "Step 761 Loss:  tensor(0.0489, grad_fn=<MulBackward0>)\n",
            "Step 762 Loss:  tensor(0.0488, grad_fn=<MulBackward0>)\n",
            "Step 763 Loss:  tensor(0.0486, grad_fn=<MulBackward0>)\n",
            "Step 764 Loss:  tensor(0.0484, grad_fn=<MulBackward0>)\n",
            "Step 765 Loss:  tensor(0.0483, grad_fn=<MulBackward0>)\n",
            "Step 766 Loss:  tensor(0.0481, grad_fn=<MulBackward0>)\n",
            "Step 767 Loss:  tensor(0.0480, grad_fn=<MulBackward0>)\n",
            "Step 768 Loss:  tensor(0.0478, grad_fn=<MulBackward0>)\n",
            "Step 769 Loss:  tensor(0.0476, grad_fn=<MulBackward0>)\n",
            "Step 770 Loss:  tensor(0.0475, grad_fn=<MulBackward0>)\n",
            "Step 771 Loss:  tensor(0.0473, grad_fn=<MulBackward0>)\n",
            "Step 772 Loss:  tensor(0.0472, grad_fn=<MulBackward0>)\n",
            "Step 773 Loss:  tensor(0.0470, grad_fn=<MulBackward0>)\n",
            "Step 774 Loss:  tensor(0.0469, grad_fn=<MulBackward0>)\n",
            "Step 775 Loss:  tensor(0.0467, grad_fn=<MulBackward0>)\n",
            "Step 776 Loss:  tensor(0.0465, grad_fn=<MulBackward0>)\n",
            "Step 777 Loss:  tensor(0.0464, grad_fn=<MulBackward0>)\n",
            "Step 778 Loss:  tensor(0.0462, grad_fn=<MulBackward0>)\n",
            "Step 779 Loss:  tensor(0.0461, grad_fn=<MulBackward0>)\n",
            "Step 780 Loss:  tensor(0.0459, grad_fn=<MulBackward0>)\n",
            "Step 781 Loss:  tensor(0.0458, grad_fn=<MulBackward0>)\n",
            "Step 782 Loss:  tensor(0.0456, grad_fn=<MulBackward0>)\n",
            "Step 783 Loss:  tensor(0.0455, grad_fn=<MulBackward0>)\n",
            "Step 784 Loss:  tensor(0.0453, grad_fn=<MulBackward0>)\n",
            "Step 785 Loss:  tensor(0.0452, grad_fn=<MulBackward0>)\n",
            "Step 786 Loss:  tensor(0.0450, grad_fn=<MulBackward0>)\n",
            "Step 787 Loss:  tensor(0.0449, grad_fn=<MulBackward0>)\n",
            "Step 788 Loss:  tensor(0.0447, grad_fn=<MulBackward0>)\n",
            "Step 789 Loss:  tensor(0.0446, grad_fn=<MulBackward0>)\n",
            "Step 790 Loss:  tensor(0.0444, grad_fn=<MulBackward0>)\n",
            "Step 791 Loss:  tensor(0.0443, grad_fn=<MulBackward0>)\n",
            "Step 792 Loss:  tensor(0.0441, grad_fn=<MulBackward0>)\n",
            "Step 793 Loss:  tensor(0.0440, grad_fn=<MulBackward0>)\n",
            "Step 794 Loss:  tensor(0.0439, grad_fn=<MulBackward0>)\n",
            "Step 795 Loss:  tensor(0.0437, grad_fn=<MulBackward0>)\n",
            "Step 796 Loss:  tensor(0.0436, grad_fn=<MulBackward0>)\n",
            "Step 797 Loss:  tensor(0.0434, grad_fn=<MulBackward0>)\n",
            "Step 798 Loss:  tensor(0.0433, grad_fn=<MulBackward0>)\n",
            "Step 799 Loss:  tensor(0.0431, grad_fn=<MulBackward0>)\n",
            "Step 800 Loss:  tensor(0.0430, grad_fn=<MulBackward0>)\n",
            "Step 801 Loss:  tensor(0.0429, grad_fn=<MulBackward0>)\n",
            "Step 802 Loss:  tensor(0.0427, grad_fn=<MulBackward0>)\n",
            "Step 803 Loss:  tensor(0.0426, grad_fn=<MulBackward0>)\n",
            "Step 804 Loss:  tensor(0.0425, grad_fn=<MulBackward0>)\n",
            "Step 805 Loss:  tensor(0.0423, grad_fn=<MulBackward0>)\n",
            "Step 806 Loss:  tensor(0.0422, grad_fn=<MulBackward0>)\n",
            "Step 807 Loss:  tensor(0.0420, grad_fn=<MulBackward0>)\n",
            "Step 808 Loss:  tensor(0.0419, grad_fn=<MulBackward0>)\n",
            "Step 809 Loss:  tensor(0.0418, grad_fn=<MulBackward0>)\n",
            "Step 810 Loss:  tensor(0.0416, grad_fn=<MulBackward0>)\n",
            "Step 811 Loss:  tensor(0.0415, grad_fn=<MulBackward0>)\n",
            "Step 812 Loss:  tensor(0.0414, grad_fn=<MulBackward0>)\n",
            "Step 813 Loss:  tensor(0.0412, grad_fn=<MulBackward0>)\n",
            "Step 814 Loss:  tensor(0.0411, grad_fn=<MulBackward0>)\n",
            "Step 815 Loss:  tensor(0.0410, grad_fn=<MulBackward0>)\n",
            "Step 816 Loss:  tensor(0.0408, grad_fn=<MulBackward0>)\n",
            "Step 817 Loss:  tensor(0.0407, grad_fn=<MulBackward0>)\n",
            "Step 818 Loss:  tensor(0.0406, grad_fn=<MulBackward0>)\n",
            "Step 819 Loss:  tensor(0.0404, grad_fn=<MulBackward0>)\n",
            "Step 820 Loss:  tensor(0.0403, grad_fn=<MulBackward0>)\n",
            "Step 821 Loss:  tensor(0.0402, grad_fn=<MulBackward0>)\n",
            "Step 822 Loss:  tensor(0.0401, grad_fn=<MulBackward0>)\n",
            "Step 823 Loss:  tensor(0.0399, grad_fn=<MulBackward0>)\n",
            "Step 824 Loss:  tensor(0.0398, grad_fn=<MulBackward0>)\n",
            "Step 825 Loss:  tensor(0.0397, grad_fn=<MulBackward0>)\n",
            "Step 826 Loss:  tensor(0.0395, grad_fn=<MulBackward0>)\n",
            "Step 827 Loss:  tensor(0.0394, grad_fn=<MulBackward0>)\n",
            "Step 828 Loss:  tensor(0.0393, grad_fn=<MulBackward0>)\n",
            "Step 829 Loss:  tensor(0.0392, grad_fn=<MulBackward0>)\n",
            "Step 830 Loss:  tensor(0.0390, grad_fn=<MulBackward0>)\n",
            "Step 831 Loss:  tensor(0.0389, grad_fn=<MulBackward0>)\n",
            "Step 832 Loss:  tensor(0.0388, grad_fn=<MulBackward0>)\n",
            "Step 833 Loss:  tensor(0.0387, grad_fn=<MulBackward0>)\n",
            "Step 834 Loss:  tensor(0.0386, grad_fn=<MulBackward0>)\n",
            "Step 835 Loss:  tensor(0.0384, grad_fn=<MulBackward0>)\n",
            "Step 836 Loss:  tensor(0.0383, grad_fn=<MulBackward0>)\n",
            "Step 837 Loss:  tensor(0.0382, grad_fn=<MulBackward0>)\n",
            "Step 838 Loss:  tensor(0.0381, grad_fn=<MulBackward0>)\n",
            "Step 839 Loss:  tensor(0.0380, grad_fn=<MulBackward0>)\n",
            "Step 840 Loss:  tensor(0.0378, grad_fn=<MulBackward0>)\n",
            "Step 841 Loss:  tensor(0.0377, grad_fn=<MulBackward0>)\n",
            "Step 842 Loss:  tensor(0.0376, grad_fn=<MulBackward0>)\n",
            "Step 843 Loss:  tensor(0.0375, grad_fn=<MulBackward0>)\n",
            "Step 844 Loss:  tensor(0.0374, grad_fn=<MulBackward0>)\n",
            "Step 845 Loss:  tensor(0.0372, grad_fn=<MulBackward0>)\n",
            "Step 846 Loss:  tensor(0.0371, grad_fn=<MulBackward0>)\n",
            "Step 847 Loss:  tensor(0.0370, grad_fn=<MulBackward0>)\n",
            "Step 848 Loss:  tensor(0.0369, grad_fn=<MulBackward0>)\n",
            "Step 849 Loss:  tensor(0.0368, grad_fn=<MulBackward0>)\n",
            "Step 850 Loss:  tensor(0.0367, grad_fn=<MulBackward0>)\n",
            "Step 851 Loss:  tensor(0.0365, grad_fn=<MulBackward0>)\n",
            "Step 852 Loss:  tensor(0.0364, grad_fn=<MulBackward0>)\n",
            "Step 853 Loss:  tensor(0.0363, grad_fn=<MulBackward0>)\n",
            "Step 854 Loss:  tensor(0.0362, grad_fn=<MulBackward0>)\n",
            "Step 855 Loss:  tensor(0.0361, grad_fn=<MulBackward0>)\n",
            "Step 856 Loss:  tensor(0.0360, grad_fn=<MulBackward0>)\n",
            "Step 857 Loss:  tensor(0.0359, grad_fn=<MulBackward0>)\n",
            "Step 858 Loss:  tensor(0.0358, grad_fn=<MulBackward0>)\n",
            "Step 859 Loss:  tensor(0.0357, grad_fn=<MulBackward0>)\n",
            "Step 860 Loss:  tensor(0.0355, grad_fn=<MulBackward0>)\n",
            "Step 861 Loss:  tensor(0.0354, grad_fn=<MulBackward0>)\n",
            "Step 862 Loss:  tensor(0.0353, grad_fn=<MulBackward0>)\n",
            "Step 863 Loss:  tensor(0.0352, grad_fn=<MulBackward0>)\n",
            "Step 864 Loss:  tensor(0.0351, grad_fn=<MulBackward0>)\n",
            "Step 865 Loss:  tensor(0.0350, grad_fn=<MulBackward0>)\n",
            "Step 866 Loss:  tensor(0.0349, grad_fn=<MulBackward0>)\n",
            "Step 867 Loss:  tensor(0.0348, grad_fn=<MulBackward0>)\n",
            "Step 868 Loss:  tensor(0.0347, grad_fn=<MulBackward0>)\n",
            "Step 869 Loss:  tensor(0.0346, grad_fn=<MulBackward0>)\n",
            "Step 870 Loss:  tensor(0.0345, grad_fn=<MulBackward0>)\n",
            "Step 871 Loss:  tensor(0.0344, grad_fn=<MulBackward0>)\n",
            "Step 872 Loss:  tensor(0.0343, grad_fn=<MulBackward0>)\n",
            "Step 873 Loss:  tensor(0.0341, grad_fn=<MulBackward0>)\n",
            "Step 874 Loss:  tensor(0.0340, grad_fn=<MulBackward0>)\n",
            "Step 875 Loss:  tensor(0.0339, grad_fn=<MulBackward0>)\n",
            "Step 876 Loss:  tensor(0.0338, grad_fn=<MulBackward0>)\n",
            "Step 877 Loss:  tensor(0.0337, grad_fn=<MulBackward0>)\n",
            "Step 878 Loss:  tensor(0.0336, grad_fn=<MulBackward0>)\n",
            "Step 879 Loss:  tensor(0.0335, grad_fn=<MulBackward0>)\n",
            "Step 880 Loss:  tensor(0.0334, grad_fn=<MulBackward0>)\n",
            "Step 881 Loss:  tensor(0.0333, grad_fn=<MulBackward0>)\n",
            "Step 882 Loss:  tensor(0.0332, grad_fn=<MulBackward0>)\n",
            "Step 883 Loss:  tensor(0.0331, grad_fn=<MulBackward0>)\n",
            "Step 884 Loss:  tensor(0.0330, grad_fn=<MulBackward0>)\n",
            "Step 885 Loss:  tensor(0.0329, grad_fn=<MulBackward0>)\n",
            "Step 886 Loss:  tensor(0.0328, grad_fn=<MulBackward0>)\n",
            "Step 887 Loss:  tensor(0.0327, grad_fn=<MulBackward0>)\n",
            "Step 888 Loss:  tensor(0.0326, grad_fn=<MulBackward0>)\n",
            "Step 889 Loss:  tensor(0.0325, grad_fn=<MulBackward0>)\n",
            "Step 890 Loss:  tensor(0.0324, grad_fn=<MulBackward0>)\n",
            "Step 891 Loss:  tensor(0.0323, grad_fn=<MulBackward0>)\n",
            "Step 892 Loss:  tensor(0.0322, grad_fn=<MulBackward0>)\n",
            "Step 893 Loss:  tensor(0.0321, grad_fn=<MulBackward0>)\n",
            "Step 894 Loss:  tensor(0.0320, grad_fn=<MulBackward0>)\n",
            "Step 895 Loss:  tensor(0.0319, grad_fn=<MulBackward0>)\n",
            "Step 896 Loss:  tensor(0.0319, grad_fn=<MulBackward0>)\n",
            "Step 897 Loss:  tensor(0.0318, grad_fn=<MulBackward0>)\n",
            "Step 898 Loss:  tensor(0.0317, grad_fn=<MulBackward0>)\n",
            "Step 899 Loss:  tensor(0.0316, grad_fn=<MulBackward0>)\n",
            "Step 900 Loss:  tensor(0.0315, grad_fn=<MulBackward0>)\n",
            "Step 901 Loss:  tensor(0.0314, grad_fn=<MulBackward0>)\n",
            "Step 902 Loss:  tensor(0.0313, grad_fn=<MulBackward0>)\n",
            "Step 903 Loss:  tensor(0.0312, grad_fn=<MulBackward0>)\n",
            "Step 904 Loss:  tensor(0.0311, grad_fn=<MulBackward0>)\n",
            "Step 905 Loss:  tensor(0.0310, grad_fn=<MulBackward0>)\n",
            "Step 906 Loss:  tensor(0.0309, grad_fn=<MulBackward0>)\n",
            "Step 907 Loss:  tensor(0.0308, grad_fn=<MulBackward0>)\n",
            "Step 908 Loss:  tensor(0.0307, grad_fn=<MulBackward0>)\n",
            "Step 909 Loss:  tensor(0.0306, grad_fn=<MulBackward0>)\n",
            "Step 910 Loss:  tensor(0.0306, grad_fn=<MulBackward0>)\n",
            "Step 911 Loss:  tensor(0.0305, grad_fn=<MulBackward0>)\n",
            "Step 912 Loss:  tensor(0.0304, grad_fn=<MulBackward0>)\n",
            "Step 913 Loss:  tensor(0.0303, grad_fn=<MulBackward0>)\n",
            "Step 914 Loss:  tensor(0.0302, grad_fn=<MulBackward0>)\n",
            "Step 915 Loss:  tensor(0.0301, grad_fn=<MulBackward0>)\n",
            "Step 916 Loss:  tensor(0.0300, grad_fn=<MulBackward0>)\n",
            "Step 917 Loss:  tensor(0.0299, grad_fn=<MulBackward0>)\n",
            "Step 918 Loss:  tensor(0.0298, grad_fn=<MulBackward0>)\n",
            "Step 919 Loss:  tensor(0.0298, grad_fn=<MulBackward0>)\n",
            "Step 920 Loss:  tensor(0.0297, grad_fn=<MulBackward0>)\n",
            "Step 921 Loss:  tensor(0.0296, grad_fn=<MulBackward0>)\n",
            "Step 922 Loss:  tensor(0.0295, grad_fn=<MulBackward0>)\n",
            "Step 923 Loss:  tensor(0.0294, grad_fn=<MulBackward0>)\n",
            "Step 924 Loss:  tensor(0.0293, grad_fn=<MulBackward0>)\n",
            "Step 925 Loss:  tensor(0.0292, grad_fn=<MulBackward0>)\n",
            "Step 926 Loss:  tensor(0.0291, grad_fn=<MulBackward0>)\n",
            "Step 927 Loss:  tensor(0.0291, grad_fn=<MulBackward0>)\n",
            "Step 928 Loss:  tensor(0.0290, grad_fn=<MulBackward0>)\n",
            "Step 929 Loss:  tensor(0.0289, grad_fn=<MulBackward0>)\n",
            "Step 930 Loss:  tensor(0.0288, grad_fn=<MulBackward0>)\n",
            "Step 931 Loss:  tensor(0.0287, grad_fn=<MulBackward0>)\n",
            "Step 932 Loss:  tensor(0.0286, grad_fn=<MulBackward0>)\n",
            "Step 933 Loss:  tensor(0.0286, grad_fn=<MulBackward0>)\n",
            "Step 934 Loss:  tensor(0.0285, grad_fn=<MulBackward0>)\n",
            "Step 935 Loss:  tensor(0.0284, grad_fn=<MulBackward0>)\n",
            "Step 936 Loss:  tensor(0.0283, grad_fn=<MulBackward0>)\n",
            "Step 937 Loss:  tensor(0.0282, grad_fn=<MulBackward0>)\n",
            "Step 938 Loss:  tensor(0.0281, grad_fn=<MulBackward0>)\n",
            "Step 939 Loss:  tensor(0.0281, grad_fn=<MulBackward0>)\n",
            "Step 940 Loss:  tensor(0.0280, grad_fn=<MulBackward0>)\n",
            "Step 941 Loss:  tensor(0.0279, grad_fn=<MulBackward0>)\n",
            "Step 942 Loss:  tensor(0.0278, grad_fn=<MulBackward0>)\n",
            "Step 943 Loss:  tensor(0.0277, grad_fn=<MulBackward0>)\n",
            "Step 944 Loss:  tensor(0.0277, grad_fn=<MulBackward0>)\n",
            "Step 945 Loss:  tensor(0.0276, grad_fn=<MulBackward0>)\n",
            "Step 946 Loss:  tensor(0.0275, grad_fn=<MulBackward0>)\n",
            "Step 947 Loss:  tensor(0.0274, grad_fn=<MulBackward0>)\n",
            "Step 948 Loss:  tensor(0.0273, grad_fn=<MulBackward0>)\n",
            "Step 949 Loss:  tensor(0.0273, grad_fn=<MulBackward0>)\n",
            "Step 950 Loss:  tensor(0.0272, grad_fn=<MulBackward0>)\n",
            "Step 951 Loss:  tensor(0.0271, grad_fn=<MulBackward0>)\n",
            "Step 952 Loss:  tensor(0.0270, grad_fn=<MulBackward0>)\n",
            "Step 953 Loss:  tensor(0.0270, grad_fn=<MulBackward0>)\n",
            "Step 954 Loss:  tensor(0.0269, grad_fn=<MulBackward0>)\n",
            "Step 955 Loss:  tensor(0.0268, grad_fn=<MulBackward0>)\n",
            "Step 956 Loss:  tensor(0.0267, grad_fn=<MulBackward0>)\n",
            "Step 957 Loss:  tensor(0.0267, grad_fn=<MulBackward0>)\n",
            "Step 958 Loss:  tensor(0.0266, grad_fn=<MulBackward0>)\n",
            "Step 959 Loss:  tensor(0.0265, grad_fn=<MulBackward0>)\n",
            "Step 960 Loss:  tensor(0.0264, grad_fn=<MulBackward0>)\n",
            "Step 961 Loss:  tensor(0.0264, grad_fn=<MulBackward0>)\n",
            "Step 962 Loss:  tensor(0.0263, grad_fn=<MulBackward0>)\n",
            "Step 963 Loss:  tensor(0.0262, grad_fn=<MulBackward0>)\n",
            "Step 964 Loss:  tensor(0.0261, grad_fn=<MulBackward0>)\n",
            "Step 965 Loss:  tensor(0.0261, grad_fn=<MulBackward0>)\n",
            "Step 966 Loss:  tensor(0.0260, grad_fn=<MulBackward0>)\n",
            "Step 967 Loss:  tensor(0.0259, grad_fn=<MulBackward0>)\n",
            "Step 968 Loss:  tensor(0.0258, grad_fn=<MulBackward0>)\n",
            "Step 969 Loss:  tensor(0.0258, grad_fn=<MulBackward0>)\n",
            "Step 970 Loss:  tensor(0.0257, grad_fn=<MulBackward0>)\n",
            "Step 971 Loss:  tensor(0.0256, grad_fn=<MulBackward0>)\n",
            "Step 972 Loss:  tensor(0.0255, grad_fn=<MulBackward0>)\n",
            "Step 973 Loss:  tensor(0.0255, grad_fn=<MulBackward0>)\n",
            "Step 974 Loss:  tensor(0.0254, grad_fn=<MulBackward0>)\n",
            "Step 975 Loss:  tensor(0.0253, grad_fn=<MulBackward0>)\n",
            "Step 976 Loss:  tensor(0.0253, grad_fn=<MulBackward0>)\n",
            "Step 977 Loss:  tensor(0.0252, grad_fn=<MulBackward0>)\n",
            "Step 978 Loss:  tensor(0.0251, grad_fn=<MulBackward0>)\n",
            "Step 979 Loss:  tensor(0.0250, grad_fn=<MulBackward0>)\n",
            "Step 980 Loss:  tensor(0.0250, grad_fn=<MulBackward0>)\n",
            "Step 981 Loss:  tensor(0.0249, grad_fn=<MulBackward0>)\n",
            "Step 982 Loss:  tensor(0.0248, grad_fn=<MulBackward0>)\n",
            "Step 983 Loss:  tensor(0.0248, grad_fn=<MulBackward0>)\n",
            "Step 984 Loss:  tensor(0.0247, grad_fn=<MulBackward0>)\n",
            "Step 985 Loss:  tensor(0.0246, grad_fn=<MulBackward0>)\n",
            "Step 986 Loss:  tensor(0.0246, grad_fn=<MulBackward0>)\n",
            "Step 987 Loss:  tensor(0.0245, grad_fn=<MulBackward0>)\n",
            "Step 988 Loss:  tensor(0.0244, grad_fn=<MulBackward0>)\n",
            "Step 989 Loss:  tensor(0.0243, grad_fn=<MulBackward0>)\n",
            "Step 990 Loss:  tensor(0.0243, grad_fn=<MulBackward0>)\n",
            "Step 991 Loss:  tensor(0.0242, grad_fn=<MulBackward0>)\n",
            "Step 992 Loss:  tensor(0.0241, grad_fn=<MulBackward0>)\n",
            "Step 993 Loss:  tensor(0.0241, grad_fn=<MulBackward0>)\n",
            "Step 994 Loss:  tensor(0.0240, grad_fn=<MulBackward0>)\n",
            "Step 995 Loss:  tensor(0.0239, grad_fn=<MulBackward0>)\n",
            "Step 996 Loss:  tensor(0.0239, grad_fn=<MulBackward0>)\n",
            "Step 997 Loss:  tensor(0.0238, grad_fn=<MulBackward0>)\n",
            "Step 998 Loss:  tensor(0.0237, grad_fn=<MulBackward0>)\n",
            "Step 999 Loss:  tensor(0.0237, grad_fn=<MulBackward0>)\n",
            "Step 1000 Loss:  tensor(0.0236, grad_fn=<MulBackward0>)\n",
            "Step 1001 Loss:  tensor(0.0235, grad_fn=<MulBackward0>)\n",
            "Step 1002 Loss:  tensor(0.0235, grad_fn=<MulBackward0>)\n",
            "Step 1003 Loss:  tensor(0.0234, grad_fn=<MulBackward0>)\n",
            "Step 1004 Loss:  tensor(0.0234, grad_fn=<MulBackward0>)\n",
            "Step 1005 Loss:  tensor(0.0233, grad_fn=<MulBackward0>)\n",
            "Step 1006 Loss:  tensor(0.0232, grad_fn=<MulBackward0>)\n",
            "Step 1007 Loss:  tensor(0.0232, grad_fn=<MulBackward0>)\n",
            "Step 1008 Loss:  tensor(0.0231, grad_fn=<MulBackward0>)\n",
            "Step 1009 Loss:  tensor(0.0230, grad_fn=<MulBackward0>)\n",
            "Step 1010 Loss:  tensor(0.0230, grad_fn=<MulBackward0>)\n",
            "Step 1011 Loss:  tensor(0.0229, grad_fn=<MulBackward0>)\n",
            "Step 1012 Loss:  tensor(0.0228, grad_fn=<MulBackward0>)\n",
            "Step 1013 Loss:  tensor(0.0228, grad_fn=<MulBackward0>)\n",
            "Step 1014 Loss:  tensor(0.0227, grad_fn=<MulBackward0>)\n",
            "Step 1015 Loss:  tensor(0.0227, grad_fn=<MulBackward0>)\n",
            "Step 1016 Loss:  tensor(0.0226, grad_fn=<MulBackward0>)\n",
            "Step 1017 Loss:  tensor(0.0225, grad_fn=<MulBackward0>)\n",
            "Step 1018 Loss:  tensor(0.0225, grad_fn=<MulBackward0>)\n",
            "Step 1019 Loss:  tensor(0.0224, grad_fn=<MulBackward0>)\n",
            "Step 1020 Loss:  tensor(0.0223, grad_fn=<MulBackward0>)\n",
            "Step 1021 Loss:  tensor(0.0223, grad_fn=<MulBackward0>)\n",
            "Step 1022 Loss:  tensor(0.0222, grad_fn=<MulBackward0>)\n",
            "Step 1023 Loss:  tensor(0.0222, grad_fn=<MulBackward0>)\n",
            "Step 1024 Loss:  tensor(0.0221, grad_fn=<MulBackward0>)\n",
            "Step 1025 Loss:  tensor(0.0220, grad_fn=<MulBackward0>)\n",
            "Step 1026 Loss:  tensor(0.0220, grad_fn=<MulBackward0>)\n",
            "Step 1027 Loss:  tensor(0.0219, grad_fn=<MulBackward0>)\n",
            "Step 1028 Loss:  tensor(0.0219, grad_fn=<MulBackward0>)\n",
            "Step 1029 Loss:  tensor(0.0218, grad_fn=<MulBackward0>)\n",
            "Step 1030 Loss:  tensor(0.0217, grad_fn=<MulBackward0>)\n",
            "Step 1031 Loss:  tensor(0.0217, grad_fn=<MulBackward0>)\n",
            "Step 1032 Loss:  tensor(0.0216, grad_fn=<MulBackward0>)\n",
            "Step 1033 Loss:  tensor(0.0216, grad_fn=<MulBackward0>)\n",
            "Step 1034 Loss:  tensor(0.0215, grad_fn=<MulBackward0>)\n",
            "Step 1035 Loss:  tensor(0.0214, grad_fn=<MulBackward0>)\n",
            "Step 1036 Loss:  tensor(0.0214, grad_fn=<MulBackward0>)\n",
            "Step 1037 Loss:  tensor(0.0213, grad_fn=<MulBackward0>)\n",
            "Step 1038 Loss:  tensor(0.0213, grad_fn=<MulBackward0>)\n",
            "Step 1039 Loss:  tensor(0.0212, grad_fn=<MulBackward0>)\n",
            "Step 1040 Loss:  tensor(0.0211, grad_fn=<MulBackward0>)\n",
            "Step 1041 Loss:  tensor(0.0211, grad_fn=<MulBackward0>)\n",
            "Step 1042 Loss:  tensor(0.0210, grad_fn=<MulBackward0>)\n",
            "Step 1043 Loss:  tensor(0.0210, grad_fn=<MulBackward0>)\n",
            "Step 1044 Loss:  tensor(0.0209, grad_fn=<MulBackward0>)\n",
            "Step 1045 Loss:  tensor(0.0209, grad_fn=<MulBackward0>)\n",
            "Step 1046 Loss:  tensor(0.0208, grad_fn=<MulBackward0>)\n",
            "Step 1047 Loss:  tensor(0.0207, grad_fn=<MulBackward0>)\n",
            "Step 1048 Loss:  tensor(0.0207, grad_fn=<MulBackward0>)\n",
            "Step 1049 Loss:  tensor(0.0206, grad_fn=<MulBackward0>)\n",
            "Step 1050 Loss:  tensor(0.0206, grad_fn=<MulBackward0>)\n",
            "Step 1051 Loss:  tensor(0.0205, grad_fn=<MulBackward0>)\n",
            "Step 1052 Loss:  tensor(0.0205, grad_fn=<MulBackward0>)\n",
            "Step 1053 Loss:  tensor(0.0204, grad_fn=<MulBackward0>)\n",
            "Step 1054 Loss:  tensor(0.0203, grad_fn=<MulBackward0>)\n",
            "Step 1055 Loss:  tensor(0.0203, grad_fn=<MulBackward0>)\n",
            "Step 1056 Loss:  tensor(0.0202, grad_fn=<MulBackward0>)\n",
            "Step 1057 Loss:  tensor(0.0202, grad_fn=<MulBackward0>)\n",
            "Step 1058 Loss:  tensor(0.0201, grad_fn=<MulBackward0>)\n",
            "Step 1059 Loss:  tensor(0.0201, grad_fn=<MulBackward0>)\n",
            "Step 1060 Loss:  tensor(0.0200, grad_fn=<MulBackward0>)\n",
            "Step 1061 Loss:  tensor(0.0200, grad_fn=<MulBackward0>)\n",
            "Step 1062 Loss:  tensor(0.0199, grad_fn=<MulBackward0>)\n",
            "Step 1063 Loss:  tensor(0.0199, grad_fn=<MulBackward0>)\n",
            "Step 1064 Loss:  tensor(0.0198, grad_fn=<MulBackward0>)\n",
            "Step 1065 Loss:  tensor(0.0197, grad_fn=<MulBackward0>)\n",
            "Step 1066 Loss:  tensor(0.0197, grad_fn=<MulBackward0>)\n",
            "Step 1067 Loss:  tensor(0.0196, grad_fn=<MulBackward0>)\n",
            "Step 1068 Loss:  tensor(0.0196, grad_fn=<MulBackward0>)\n",
            "Step 1069 Loss:  tensor(0.0195, grad_fn=<MulBackward0>)\n",
            "Step 1070 Loss:  tensor(0.0195, grad_fn=<MulBackward0>)\n",
            "Step 1071 Loss:  tensor(0.0194, grad_fn=<MulBackward0>)\n",
            "Step 1072 Loss:  tensor(0.0194, grad_fn=<MulBackward0>)\n",
            "Step 1073 Loss:  tensor(0.0193, grad_fn=<MulBackward0>)\n",
            "Step 1074 Loss:  tensor(0.0193, grad_fn=<MulBackward0>)\n",
            "Step 1075 Loss:  tensor(0.0192, grad_fn=<MulBackward0>)\n",
            "Step 1076 Loss:  tensor(0.0192, grad_fn=<MulBackward0>)\n",
            "Step 1077 Loss:  tensor(0.0191, grad_fn=<MulBackward0>)\n",
            "Step 1078 Loss:  tensor(0.0191, grad_fn=<MulBackward0>)\n",
            "Step 1079 Loss:  tensor(0.0190, grad_fn=<MulBackward0>)\n",
            "Step 1080 Loss:  tensor(0.0190, grad_fn=<MulBackward0>)\n",
            "Step 1081 Loss:  tensor(0.0189, grad_fn=<MulBackward0>)\n",
            "Step 1082 Loss:  tensor(0.0189, grad_fn=<MulBackward0>)\n",
            "Step 1083 Loss:  tensor(0.0188, grad_fn=<MulBackward0>)\n",
            "Step 1084 Loss:  tensor(0.0187, grad_fn=<MulBackward0>)\n",
            "Step 1085 Loss:  tensor(0.0187, grad_fn=<MulBackward0>)\n",
            "Step 1086 Loss:  tensor(0.0186, grad_fn=<MulBackward0>)\n",
            "Step 1087 Loss:  tensor(0.0186, grad_fn=<MulBackward0>)\n",
            "Step 1088 Loss:  tensor(0.0185, grad_fn=<MulBackward0>)\n",
            "Step 1089 Loss:  tensor(0.0185, grad_fn=<MulBackward0>)\n",
            "Step 1090 Loss:  tensor(0.0184, grad_fn=<MulBackward0>)\n",
            "Step 1091 Loss:  tensor(0.0184, grad_fn=<MulBackward0>)\n",
            "Step 1092 Loss:  tensor(0.0183, grad_fn=<MulBackward0>)\n",
            "Step 1093 Loss:  tensor(0.0183, grad_fn=<MulBackward0>)\n",
            "Step 1094 Loss:  tensor(0.0182, grad_fn=<MulBackward0>)\n",
            "Step 1095 Loss:  tensor(0.0182, grad_fn=<MulBackward0>)\n",
            "Step 1096 Loss:  tensor(0.0181, grad_fn=<MulBackward0>)\n",
            "Step 1097 Loss:  tensor(0.0181, grad_fn=<MulBackward0>)\n",
            "Step 1098 Loss:  tensor(0.0180, grad_fn=<MulBackward0>)\n",
            "Step 1099 Loss:  tensor(0.0180, grad_fn=<MulBackward0>)\n",
            "Step 1100 Loss:  tensor(0.0180, grad_fn=<MulBackward0>)\n",
            "Step 1101 Loss:  tensor(0.0179, grad_fn=<MulBackward0>)\n",
            "Step 1102 Loss:  tensor(0.0179, grad_fn=<MulBackward0>)\n",
            "Step 1103 Loss:  tensor(0.0178, grad_fn=<MulBackward0>)\n",
            "Step 1104 Loss:  tensor(0.0178, grad_fn=<MulBackward0>)\n",
            "Step 1105 Loss:  tensor(0.0177, grad_fn=<MulBackward0>)\n",
            "Step 1106 Loss:  tensor(0.0177, grad_fn=<MulBackward0>)\n",
            "Step 1107 Loss:  tensor(0.0176, grad_fn=<MulBackward0>)\n",
            "Step 1108 Loss:  tensor(0.0176, grad_fn=<MulBackward0>)\n",
            "Step 1109 Loss:  tensor(0.0175, grad_fn=<MulBackward0>)\n",
            "Step 1110 Loss:  tensor(0.0175, grad_fn=<MulBackward0>)\n",
            "Step 1111 Loss:  tensor(0.0174, grad_fn=<MulBackward0>)\n",
            "Step 1112 Loss:  tensor(0.0174, grad_fn=<MulBackward0>)\n",
            "Step 1113 Loss:  tensor(0.0173, grad_fn=<MulBackward0>)\n",
            "Step 1114 Loss:  tensor(0.0173, grad_fn=<MulBackward0>)\n",
            "Step 1115 Loss:  tensor(0.0172, grad_fn=<MulBackward0>)\n",
            "Step 1116 Loss:  tensor(0.0172, grad_fn=<MulBackward0>)\n",
            "Step 1117 Loss:  tensor(0.0171, grad_fn=<MulBackward0>)\n",
            "Step 1118 Loss:  tensor(0.0171, grad_fn=<MulBackward0>)\n",
            "Step 1119 Loss:  tensor(0.0170, grad_fn=<MulBackward0>)\n",
            "Step 1120 Loss:  tensor(0.0170, grad_fn=<MulBackward0>)\n",
            "Step 1121 Loss:  tensor(0.0170, grad_fn=<MulBackward0>)\n",
            "Step 1122 Loss:  tensor(0.0169, grad_fn=<MulBackward0>)\n",
            "Step 1123 Loss:  tensor(0.0169, grad_fn=<MulBackward0>)\n",
            "Step 1124 Loss:  tensor(0.0168, grad_fn=<MulBackward0>)\n",
            "Step 1125 Loss:  tensor(0.0168, grad_fn=<MulBackward0>)\n",
            "Step 1126 Loss:  tensor(0.0167, grad_fn=<MulBackward0>)\n",
            "Step 1127 Loss:  tensor(0.0167, grad_fn=<MulBackward0>)\n",
            "Step 1128 Loss:  tensor(0.0166, grad_fn=<MulBackward0>)\n",
            "Step 1129 Loss:  tensor(0.0166, grad_fn=<MulBackward0>)\n",
            "Step 1130 Loss:  tensor(0.0165, grad_fn=<MulBackward0>)\n",
            "Step 1131 Loss:  tensor(0.0165, grad_fn=<MulBackward0>)\n",
            "Step 1132 Loss:  tensor(0.0165, grad_fn=<MulBackward0>)\n",
            "Step 1133 Loss:  tensor(0.0164, grad_fn=<MulBackward0>)\n",
            "Step 1134 Loss:  tensor(0.0164, grad_fn=<MulBackward0>)\n",
            "Step 1135 Loss:  tensor(0.0163, grad_fn=<MulBackward0>)\n",
            "Step 1136 Loss:  tensor(0.0163, grad_fn=<MulBackward0>)\n",
            "Step 1137 Loss:  tensor(0.0162, grad_fn=<MulBackward0>)\n",
            "Step 1138 Loss:  tensor(0.0162, grad_fn=<MulBackward0>)\n",
            "Step 1139 Loss:  tensor(0.0161, grad_fn=<MulBackward0>)\n",
            "Step 1140 Loss:  tensor(0.0161, grad_fn=<MulBackward0>)\n",
            "Step 1141 Loss:  tensor(0.0161, grad_fn=<MulBackward0>)\n",
            "Step 1142 Loss:  tensor(0.0160, grad_fn=<MulBackward0>)\n",
            "Step 1143 Loss:  tensor(0.0160, grad_fn=<MulBackward0>)\n",
            "Step 1144 Loss:  tensor(0.0159, grad_fn=<MulBackward0>)\n",
            "Step 1145 Loss:  tensor(0.0159, grad_fn=<MulBackward0>)\n",
            "Step 1146 Loss:  tensor(0.0158, grad_fn=<MulBackward0>)\n",
            "Step 1147 Loss:  tensor(0.0158, grad_fn=<MulBackward0>)\n",
            "Step 1148 Loss:  tensor(0.0158, grad_fn=<MulBackward0>)\n",
            "Step 1149 Loss:  tensor(0.0157, grad_fn=<MulBackward0>)\n",
            "Step 1150 Loss:  tensor(0.0157, grad_fn=<MulBackward0>)\n",
            "Step 1151 Loss:  tensor(0.0156, grad_fn=<MulBackward0>)\n",
            "Step 1152 Loss:  tensor(0.0156, grad_fn=<MulBackward0>)\n",
            "Step 1153 Loss:  tensor(0.0155, grad_fn=<MulBackward0>)\n",
            "Step 1154 Loss:  tensor(0.0155, grad_fn=<MulBackward0>)\n",
            "Step 1155 Loss:  tensor(0.0155, grad_fn=<MulBackward0>)\n",
            "Step 1156 Loss:  tensor(0.0154, grad_fn=<MulBackward0>)\n",
            "Step 1157 Loss:  tensor(0.0154, grad_fn=<MulBackward0>)\n",
            "Step 1158 Loss:  tensor(0.0153, grad_fn=<MulBackward0>)\n",
            "Step 1159 Loss:  tensor(0.0153, grad_fn=<MulBackward0>)\n",
            "Step 1160 Loss:  tensor(0.0152, grad_fn=<MulBackward0>)\n",
            "Step 1161 Loss:  tensor(0.0152, grad_fn=<MulBackward0>)\n",
            "Step 1162 Loss:  tensor(0.0152, grad_fn=<MulBackward0>)\n",
            "Step 1163 Loss:  tensor(0.0151, grad_fn=<MulBackward0>)\n",
            "Step 1164 Loss:  tensor(0.0151, grad_fn=<MulBackward0>)\n",
            "Step 1165 Loss:  tensor(0.0150, grad_fn=<MulBackward0>)\n",
            "Step 1166 Loss:  tensor(0.0150, grad_fn=<MulBackward0>)\n",
            "Step 1167 Loss:  tensor(0.0150, grad_fn=<MulBackward0>)\n",
            "Step 1168 Loss:  tensor(0.0149, grad_fn=<MulBackward0>)\n",
            "Step 1169 Loss:  tensor(0.0149, grad_fn=<MulBackward0>)\n",
            "Step 1170 Loss:  tensor(0.0148, grad_fn=<MulBackward0>)\n",
            "Step 1171 Loss:  tensor(0.0148, grad_fn=<MulBackward0>)\n",
            "Step 1172 Loss:  tensor(0.0148, grad_fn=<MulBackward0>)\n",
            "Step 1173 Loss:  tensor(0.0147, grad_fn=<MulBackward0>)\n",
            "Step 1174 Loss:  tensor(0.0147, grad_fn=<MulBackward0>)\n",
            "Step 1175 Loss:  tensor(0.0146, grad_fn=<MulBackward0>)\n",
            "Step 1176 Loss:  tensor(0.0146, grad_fn=<MulBackward0>)\n",
            "Step 1177 Loss:  tensor(0.0146, grad_fn=<MulBackward0>)\n",
            "Step 1178 Loss:  tensor(0.0145, grad_fn=<MulBackward0>)\n",
            "Step 1179 Loss:  tensor(0.0145, grad_fn=<MulBackward0>)\n",
            "Step 1180 Loss:  tensor(0.0144, grad_fn=<MulBackward0>)\n",
            "Step 1181 Loss:  tensor(0.0144, grad_fn=<MulBackward0>)\n",
            "Step 1182 Loss:  tensor(0.0144, grad_fn=<MulBackward0>)\n",
            "Step 1183 Loss:  tensor(0.0143, grad_fn=<MulBackward0>)\n",
            "Step 1184 Loss:  tensor(0.0143, grad_fn=<MulBackward0>)\n",
            "Step 1185 Loss:  tensor(0.0142, grad_fn=<MulBackward0>)\n",
            "Step 1186 Loss:  tensor(0.0142, grad_fn=<MulBackward0>)\n",
            "Step 1187 Loss:  tensor(0.0142, grad_fn=<MulBackward0>)\n",
            "Step 1188 Loss:  tensor(0.0141, grad_fn=<MulBackward0>)\n",
            "Step 1189 Loss:  tensor(0.0141, grad_fn=<MulBackward0>)\n",
            "Step 1190 Loss:  tensor(0.0140, grad_fn=<MulBackward0>)\n",
            "Step 1191 Loss:  tensor(0.0140, grad_fn=<MulBackward0>)\n",
            "Step 1192 Loss:  tensor(0.0140, grad_fn=<MulBackward0>)\n",
            "Step 1193 Loss:  tensor(0.0139, grad_fn=<MulBackward0>)\n",
            "Step 1194 Loss:  tensor(0.0139, grad_fn=<MulBackward0>)\n",
            "Step 1195 Loss:  tensor(0.0139, grad_fn=<MulBackward0>)\n",
            "Step 1196 Loss:  tensor(0.0138, grad_fn=<MulBackward0>)\n",
            "Step 1197 Loss:  tensor(0.0138, grad_fn=<MulBackward0>)\n",
            "Step 1198 Loss:  tensor(0.0137, grad_fn=<MulBackward0>)\n",
            "Step 1199 Loss:  tensor(0.0137, grad_fn=<MulBackward0>)\n",
            "Step 1200 Loss:  tensor(0.0137, grad_fn=<MulBackward0>)\n",
            "Step 1201 Loss:  tensor(0.0136, grad_fn=<MulBackward0>)\n",
            "Step 1202 Loss:  tensor(0.0136, grad_fn=<MulBackward0>)\n",
            "Step 1203 Loss:  tensor(0.0136, grad_fn=<MulBackward0>)\n",
            "Step 1204 Loss:  tensor(0.0135, grad_fn=<MulBackward0>)\n",
            "Step 1205 Loss:  tensor(0.0135, grad_fn=<MulBackward0>)\n",
            "Step 1206 Loss:  tensor(0.0134, grad_fn=<MulBackward0>)\n",
            "Step 1207 Loss:  tensor(0.0134, grad_fn=<MulBackward0>)\n",
            "Step 1208 Loss:  tensor(0.0134, grad_fn=<MulBackward0>)\n",
            "Step 1209 Loss:  tensor(0.0133, grad_fn=<MulBackward0>)\n",
            "Step 1210 Loss:  tensor(0.0133, grad_fn=<MulBackward0>)\n",
            "Step 1211 Loss:  tensor(0.0133, grad_fn=<MulBackward0>)\n",
            "Step 1212 Loss:  tensor(0.0132, grad_fn=<MulBackward0>)\n",
            "Step 1213 Loss:  tensor(0.0132, grad_fn=<MulBackward0>)\n",
            "Step 1214 Loss:  tensor(0.0131, grad_fn=<MulBackward0>)\n",
            "Step 1215 Loss:  tensor(0.0131, grad_fn=<MulBackward0>)\n",
            "Step 1216 Loss:  tensor(0.0131, grad_fn=<MulBackward0>)\n",
            "Step 1217 Loss:  tensor(0.0130, grad_fn=<MulBackward0>)\n",
            "Step 1218 Loss:  tensor(0.0130, grad_fn=<MulBackward0>)\n",
            "Step 1219 Loss:  tensor(0.0130, grad_fn=<MulBackward0>)\n",
            "Step 1220 Loss:  tensor(0.0129, grad_fn=<MulBackward0>)\n",
            "Step 1221 Loss:  tensor(0.0129, grad_fn=<MulBackward0>)\n",
            "Step 1222 Loss:  tensor(0.0129, grad_fn=<MulBackward0>)\n",
            "Step 1223 Loss:  tensor(0.0128, grad_fn=<MulBackward0>)\n",
            "Step 1224 Loss:  tensor(0.0128, grad_fn=<MulBackward0>)\n",
            "Step 1225 Loss:  tensor(0.0128, grad_fn=<MulBackward0>)\n",
            "Step 1226 Loss:  tensor(0.0127, grad_fn=<MulBackward0>)\n",
            "Step 1227 Loss:  tensor(0.0127, grad_fn=<MulBackward0>)\n",
            "Step 1228 Loss:  tensor(0.0126, grad_fn=<MulBackward0>)\n",
            "Step 1229 Loss:  tensor(0.0126, grad_fn=<MulBackward0>)\n",
            "Step 1230 Loss:  tensor(0.0126, grad_fn=<MulBackward0>)\n",
            "Step 1231 Loss:  tensor(0.0125, grad_fn=<MulBackward0>)\n",
            "Step 1232 Loss:  tensor(0.0125, grad_fn=<MulBackward0>)\n",
            "Step 1233 Loss:  tensor(0.0125, grad_fn=<MulBackward0>)\n",
            "Step 1234 Loss:  tensor(0.0124, grad_fn=<MulBackward0>)\n",
            "Step 1235 Loss:  tensor(0.0124, grad_fn=<MulBackward0>)\n",
            "Step 1236 Loss:  tensor(0.0124, grad_fn=<MulBackward0>)\n",
            "Step 1237 Loss:  tensor(0.0123, grad_fn=<MulBackward0>)\n",
            "Step 1238 Loss:  tensor(0.0123, grad_fn=<MulBackward0>)\n",
            "Step 1239 Loss:  tensor(0.0123, grad_fn=<MulBackward0>)\n",
            "Step 1240 Loss:  tensor(0.0122, grad_fn=<MulBackward0>)\n",
            "Step 1241 Loss:  tensor(0.0122, grad_fn=<MulBackward0>)\n",
            "Step 1242 Loss:  tensor(0.0122, grad_fn=<MulBackward0>)\n",
            "Step 1243 Loss:  tensor(0.0121, grad_fn=<MulBackward0>)\n",
            "Step 1244 Loss:  tensor(0.0121, grad_fn=<MulBackward0>)\n",
            "Step 1245 Loss:  tensor(0.0121, grad_fn=<MulBackward0>)\n",
            "Step 1246 Loss:  tensor(0.0120, grad_fn=<MulBackward0>)\n",
            "Step 1247 Loss:  tensor(0.0120, grad_fn=<MulBackward0>)\n",
            "Step 1248 Loss:  tensor(0.0120, grad_fn=<MulBackward0>)\n",
            "Step 1249 Loss:  tensor(0.0119, grad_fn=<MulBackward0>)\n",
            "Step 1250 Loss:  tensor(0.0119, grad_fn=<MulBackward0>)\n",
            "Step 1251 Loss:  tensor(0.0119, grad_fn=<MulBackward0>)\n",
            "Step 1252 Loss:  tensor(0.0118, grad_fn=<MulBackward0>)\n",
            "Step 1253 Loss:  tensor(0.0118, grad_fn=<MulBackward0>)\n",
            "Step 1254 Loss:  tensor(0.0118, grad_fn=<MulBackward0>)\n",
            "Step 1255 Loss:  tensor(0.0117, grad_fn=<MulBackward0>)\n",
            "Step 1256 Loss:  tensor(0.0117, grad_fn=<MulBackward0>)\n",
            "Step 1257 Loss:  tensor(0.0117, grad_fn=<MulBackward0>)\n",
            "Step 1258 Loss:  tensor(0.0116, grad_fn=<MulBackward0>)\n",
            "Step 1259 Loss:  tensor(0.0116, grad_fn=<MulBackward0>)\n",
            "Step 1260 Loss:  tensor(0.0116, grad_fn=<MulBackward0>)\n",
            "Step 1261 Loss:  tensor(0.0115, grad_fn=<MulBackward0>)\n",
            "Step 1262 Loss:  tensor(0.0115, grad_fn=<MulBackward0>)\n",
            "Step 1263 Loss:  tensor(0.0115, grad_fn=<MulBackward0>)\n",
            "Step 1264 Loss:  tensor(0.0115, grad_fn=<MulBackward0>)\n",
            "Step 1265 Loss:  tensor(0.0114, grad_fn=<MulBackward0>)\n",
            "Step 1266 Loss:  tensor(0.0114, grad_fn=<MulBackward0>)\n",
            "Step 1267 Loss:  tensor(0.0114, grad_fn=<MulBackward0>)\n",
            "Step 1268 Loss:  tensor(0.0113, grad_fn=<MulBackward0>)\n",
            "Step 1269 Loss:  tensor(0.0113, grad_fn=<MulBackward0>)\n",
            "Step 1270 Loss:  tensor(0.0113, grad_fn=<MulBackward0>)\n",
            "Step 1271 Loss:  tensor(0.0112, grad_fn=<MulBackward0>)\n",
            "Step 1272 Loss:  tensor(0.0112, grad_fn=<MulBackward0>)\n",
            "Step 1273 Loss:  tensor(0.0112, grad_fn=<MulBackward0>)\n",
            "Step 1274 Loss:  tensor(0.0111, grad_fn=<MulBackward0>)\n",
            "Step 1275 Loss:  tensor(0.0111, grad_fn=<MulBackward0>)\n",
            "Step 1276 Loss:  tensor(0.0111, grad_fn=<MulBackward0>)\n",
            "Step 1277 Loss:  tensor(0.0110, grad_fn=<MulBackward0>)\n",
            "Step 1278 Loss:  tensor(0.0110, grad_fn=<MulBackward0>)\n",
            "Step 1279 Loss:  tensor(0.0110, grad_fn=<MulBackward0>)\n",
            "Step 1280 Loss:  tensor(0.0110, grad_fn=<MulBackward0>)\n",
            "Step 1281 Loss:  tensor(0.0109, grad_fn=<MulBackward0>)\n",
            "Step 1282 Loss:  tensor(0.0109, grad_fn=<MulBackward0>)\n",
            "Step 1283 Loss:  tensor(0.0109, grad_fn=<MulBackward0>)\n",
            "Step 1284 Loss:  tensor(0.0108, grad_fn=<MulBackward0>)\n",
            "Step 1285 Loss:  tensor(0.0108, grad_fn=<MulBackward0>)\n",
            "Step 1286 Loss:  tensor(0.0108, grad_fn=<MulBackward0>)\n",
            "Step 1287 Loss:  tensor(0.0107, grad_fn=<MulBackward0>)\n",
            "Step 1288 Loss:  tensor(0.0107, grad_fn=<MulBackward0>)\n",
            "Step 1289 Loss:  tensor(0.0107, grad_fn=<MulBackward0>)\n",
            "Step 1290 Loss:  tensor(0.0107, grad_fn=<MulBackward0>)\n",
            "Step 1291 Loss:  tensor(0.0106, grad_fn=<MulBackward0>)\n",
            "Step 1292 Loss:  tensor(0.0106, grad_fn=<MulBackward0>)\n",
            "Step 1293 Loss:  tensor(0.0106, grad_fn=<MulBackward0>)\n",
            "Step 1294 Loss:  tensor(0.0105, grad_fn=<MulBackward0>)\n",
            "Step 1295 Loss:  tensor(0.0105, grad_fn=<MulBackward0>)\n",
            "Step 1296 Loss:  tensor(0.0105, grad_fn=<MulBackward0>)\n",
            "Step 1297 Loss:  tensor(0.0104, grad_fn=<MulBackward0>)\n",
            "Step 1298 Loss:  tensor(0.0104, grad_fn=<MulBackward0>)\n",
            "Step 1299 Loss:  tensor(0.0104, grad_fn=<MulBackward0>)\n",
            "Step 1300 Loss:  tensor(0.0104, grad_fn=<MulBackward0>)\n",
            "Step 1301 Loss:  tensor(0.0103, grad_fn=<MulBackward0>)\n",
            "Step 1302 Loss:  tensor(0.0103, grad_fn=<MulBackward0>)\n",
            "Step 1303 Loss:  tensor(0.0103, grad_fn=<MulBackward0>)\n",
            "Step 1304 Loss:  tensor(0.0102, grad_fn=<MulBackward0>)\n",
            "Step 1305 Loss:  tensor(0.0102, grad_fn=<MulBackward0>)\n",
            "Step 1306 Loss:  tensor(0.0102, grad_fn=<MulBackward0>)\n",
            "Step 1307 Loss:  tensor(0.0102, grad_fn=<MulBackward0>)\n",
            "Step 1308 Loss:  tensor(0.0101, grad_fn=<MulBackward0>)\n",
            "Step 1309 Loss:  tensor(0.0101, grad_fn=<MulBackward0>)\n",
            "Step 1310 Loss:  tensor(0.0101, grad_fn=<MulBackward0>)\n",
            "Step 1311 Loss:  tensor(0.0100, grad_fn=<MulBackward0>)\n",
            "Step 1312 Loss:  tensor(0.0100, grad_fn=<MulBackward0>)\n",
            "Step 1313 Loss:  tensor(0.0100, grad_fn=<MulBackward0>)\n",
            "Step 1314 Loss:  tensor(0.0100, grad_fn=<MulBackward0>)\n",
            "Step 1315 Loss:  tensor(0.0099, grad_fn=<MulBackward0>)\n",
            "Step 1316 Loss:  tensor(0.0099, grad_fn=<MulBackward0>)\n",
            "Step 1317 Loss:  tensor(0.0099, grad_fn=<MulBackward0>)\n",
            "Step 1318 Loss:  tensor(0.0099, grad_fn=<MulBackward0>)\n",
            "Step 1319 Loss:  tensor(0.0098, grad_fn=<MulBackward0>)\n",
            "Step 1320 Loss:  tensor(0.0098, grad_fn=<MulBackward0>)\n",
            "Step 1321 Loss:  tensor(0.0098, grad_fn=<MulBackward0>)\n",
            "Step 1322 Loss:  tensor(0.0097, grad_fn=<MulBackward0>)\n",
            "Step 1323 Loss:  tensor(0.0097, grad_fn=<MulBackward0>)\n",
            "Step 1324 Loss:  tensor(0.0097, grad_fn=<MulBackward0>)\n",
            "Step 1325 Loss:  tensor(0.0097, grad_fn=<MulBackward0>)\n",
            "Step 1326 Loss:  tensor(0.0096, grad_fn=<MulBackward0>)\n",
            "Step 1327 Loss:  tensor(0.0096, grad_fn=<MulBackward0>)\n",
            "Step 1328 Loss:  tensor(0.0096, grad_fn=<MulBackward0>)\n",
            "Step 1329 Loss:  tensor(0.0096, grad_fn=<MulBackward0>)\n",
            "Step 1330 Loss:  tensor(0.0095, grad_fn=<MulBackward0>)\n",
            "Step 1331 Loss:  tensor(0.0095, grad_fn=<MulBackward0>)\n",
            "Step 1332 Loss:  tensor(0.0095, grad_fn=<MulBackward0>)\n",
            "Step 1333 Loss:  tensor(0.0095, grad_fn=<MulBackward0>)\n",
            "Step 1334 Loss:  tensor(0.0094, grad_fn=<MulBackward0>)\n",
            "Step 1335 Loss:  tensor(0.0094, grad_fn=<MulBackward0>)\n",
            "Step 1336 Loss:  tensor(0.0094, grad_fn=<MulBackward0>)\n",
            "Step 1337 Loss:  tensor(0.0093, grad_fn=<MulBackward0>)\n",
            "Step 1338 Loss:  tensor(0.0093, grad_fn=<MulBackward0>)\n",
            "Step 1339 Loss:  tensor(0.0093, grad_fn=<MulBackward0>)\n",
            "Step 1340 Loss:  tensor(0.0093, grad_fn=<MulBackward0>)\n",
            "Step 1341 Loss:  tensor(0.0092, grad_fn=<MulBackward0>)\n",
            "Step 1342 Loss:  tensor(0.0092, grad_fn=<MulBackward0>)\n",
            "Step 1343 Loss:  tensor(0.0092, grad_fn=<MulBackward0>)\n",
            "Step 1344 Loss:  tensor(0.0092, grad_fn=<MulBackward0>)\n",
            "Step 1345 Loss:  tensor(0.0091, grad_fn=<MulBackward0>)\n",
            "Step 1346 Loss:  tensor(0.0091, grad_fn=<MulBackward0>)\n",
            "Step 1347 Loss:  tensor(0.0091, grad_fn=<MulBackward0>)\n",
            "Step 1348 Loss:  tensor(0.0091, grad_fn=<MulBackward0>)\n",
            "Step 1349 Loss:  tensor(0.0090, grad_fn=<MulBackward0>)\n",
            "Step 1350 Loss:  tensor(0.0090, grad_fn=<MulBackward0>)\n",
            "Step 1351 Loss:  tensor(0.0090, grad_fn=<MulBackward0>)\n",
            "Step 1352 Loss:  tensor(0.0090, grad_fn=<MulBackward0>)\n",
            "Step 1353 Loss:  tensor(0.0089, grad_fn=<MulBackward0>)\n",
            "Step 1354 Loss:  tensor(0.0089, grad_fn=<MulBackward0>)\n",
            "Step 1355 Loss:  tensor(0.0089, grad_fn=<MulBackward0>)\n",
            "Step 1356 Loss:  tensor(0.0089, grad_fn=<MulBackward0>)\n",
            "Step 1357 Loss:  tensor(0.0088, grad_fn=<MulBackward0>)\n",
            "Step 1358 Loss:  tensor(0.0088, grad_fn=<MulBackward0>)\n",
            "Step 1359 Loss:  tensor(0.0088, grad_fn=<MulBackward0>)\n",
            "Step 1360 Loss:  tensor(0.0088, grad_fn=<MulBackward0>)\n",
            "Step 1361 Loss:  tensor(0.0087, grad_fn=<MulBackward0>)\n",
            "Step 1362 Loss:  tensor(0.0087, grad_fn=<MulBackward0>)\n",
            "Step 1363 Loss:  tensor(0.0087, grad_fn=<MulBackward0>)\n",
            "Step 1364 Loss:  tensor(0.0087, grad_fn=<MulBackward0>)\n",
            "Step 1365 Loss:  tensor(0.0087, grad_fn=<MulBackward0>)\n",
            "Step 1366 Loss:  tensor(0.0086, grad_fn=<MulBackward0>)\n",
            "Step 1367 Loss:  tensor(0.0086, grad_fn=<MulBackward0>)\n",
            "Step 1368 Loss:  tensor(0.0086, grad_fn=<MulBackward0>)\n",
            "Step 1369 Loss:  tensor(0.0086, grad_fn=<MulBackward0>)\n",
            "Step 1370 Loss:  tensor(0.0085, grad_fn=<MulBackward0>)\n",
            "Step 1371 Loss:  tensor(0.0085, grad_fn=<MulBackward0>)\n",
            "Step 1372 Loss:  tensor(0.0085, grad_fn=<MulBackward0>)\n",
            "Step 1373 Loss:  tensor(0.0085, grad_fn=<MulBackward0>)\n",
            "Step 1374 Loss:  tensor(0.0084, grad_fn=<MulBackward0>)\n",
            "Step 1375 Loss:  tensor(0.0084, grad_fn=<MulBackward0>)\n",
            "Step 1376 Loss:  tensor(0.0084, grad_fn=<MulBackward0>)\n",
            "Step 1377 Loss:  tensor(0.0084, grad_fn=<MulBackward0>)\n",
            "Step 1378 Loss:  tensor(0.0083, grad_fn=<MulBackward0>)\n",
            "Step 1379 Loss:  tensor(0.0083, grad_fn=<MulBackward0>)\n",
            "Step 1380 Loss:  tensor(0.0083, grad_fn=<MulBackward0>)\n",
            "Step 1381 Loss:  tensor(0.0083, grad_fn=<MulBackward0>)\n",
            "Step 1382 Loss:  tensor(0.0083, grad_fn=<MulBackward0>)\n",
            "Step 1383 Loss:  tensor(0.0082, grad_fn=<MulBackward0>)\n",
            "Step 1384 Loss:  tensor(0.0082, grad_fn=<MulBackward0>)\n",
            "Step 1385 Loss:  tensor(0.0082, grad_fn=<MulBackward0>)\n",
            "Step 1386 Loss:  tensor(0.0082, grad_fn=<MulBackward0>)\n",
            "Step 1387 Loss:  tensor(0.0081, grad_fn=<MulBackward0>)\n",
            "Step 1388 Loss:  tensor(0.0081, grad_fn=<MulBackward0>)\n",
            "Step 1389 Loss:  tensor(0.0081, grad_fn=<MulBackward0>)\n",
            "Step 1390 Loss:  tensor(0.0081, grad_fn=<MulBackward0>)\n",
            "Step 1391 Loss:  tensor(0.0081, grad_fn=<MulBackward0>)\n",
            "Step 1392 Loss:  tensor(0.0080, grad_fn=<MulBackward0>)\n",
            "Step 1393 Loss:  tensor(0.0080, grad_fn=<MulBackward0>)\n",
            "Step 1394 Loss:  tensor(0.0080, grad_fn=<MulBackward0>)\n",
            "Step 1395 Loss:  tensor(0.0080, grad_fn=<MulBackward0>)\n",
            "Step 1396 Loss:  tensor(0.0079, grad_fn=<MulBackward0>)\n",
            "Step 1397 Loss:  tensor(0.0079, grad_fn=<MulBackward0>)\n",
            "Step 1398 Loss:  tensor(0.0079, grad_fn=<MulBackward0>)\n",
            "Step 1399 Loss:  tensor(0.0079, grad_fn=<MulBackward0>)\n",
            "Step 1400 Loss:  tensor(0.0079, grad_fn=<MulBackward0>)\n",
            "Step 1401 Loss:  tensor(0.0078, grad_fn=<MulBackward0>)\n",
            "Step 1402 Loss:  tensor(0.0078, grad_fn=<MulBackward0>)\n",
            "Step 1403 Loss:  tensor(0.0078, grad_fn=<MulBackward0>)\n",
            "Step 1404 Loss:  tensor(0.0078, grad_fn=<MulBackward0>)\n",
            "Step 1405 Loss:  tensor(0.0077, grad_fn=<MulBackward0>)\n",
            "Step 1406 Loss:  tensor(0.0077, grad_fn=<MulBackward0>)\n",
            "Step 1407 Loss:  tensor(0.0077, grad_fn=<MulBackward0>)\n",
            "Step 1408 Loss:  tensor(0.0077, grad_fn=<MulBackward0>)\n",
            "Step 1409 Loss:  tensor(0.0077, grad_fn=<MulBackward0>)\n",
            "Step 1410 Loss:  tensor(0.0076, grad_fn=<MulBackward0>)\n",
            "Step 1411 Loss:  tensor(0.0076, grad_fn=<MulBackward0>)\n",
            "Step 1412 Loss:  tensor(0.0076, grad_fn=<MulBackward0>)\n",
            "Step 1413 Loss:  tensor(0.0076, grad_fn=<MulBackward0>)\n",
            "Step 1414 Loss:  tensor(0.0076, grad_fn=<MulBackward0>)\n",
            "Step 1415 Loss:  tensor(0.0075, grad_fn=<MulBackward0>)\n",
            "Step 1416 Loss:  tensor(0.0075, grad_fn=<MulBackward0>)\n",
            "Step 1417 Loss:  tensor(0.0075, grad_fn=<MulBackward0>)\n",
            "Step 1418 Loss:  tensor(0.0075, grad_fn=<MulBackward0>)\n",
            "Step 1419 Loss:  tensor(0.0075, grad_fn=<MulBackward0>)\n",
            "Step 1420 Loss:  tensor(0.0074, grad_fn=<MulBackward0>)\n",
            "Step 1421 Loss:  tensor(0.0074, grad_fn=<MulBackward0>)\n",
            "Step 1422 Loss:  tensor(0.0074, grad_fn=<MulBackward0>)\n",
            "Step 1423 Loss:  tensor(0.0074, grad_fn=<MulBackward0>)\n",
            "Step 1424 Loss:  tensor(0.0074, grad_fn=<MulBackward0>)\n",
            "Step 1425 Loss:  tensor(0.0073, grad_fn=<MulBackward0>)\n",
            "Step 1426 Loss:  tensor(0.0073, grad_fn=<MulBackward0>)\n",
            "Step 1427 Loss:  tensor(0.0073, grad_fn=<MulBackward0>)\n",
            "Step 1428 Loss:  tensor(0.0073, grad_fn=<MulBackward0>)\n",
            "Step 1429 Loss:  tensor(0.0073, grad_fn=<MulBackward0>)\n",
            "Step 1430 Loss:  tensor(0.0072, grad_fn=<MulBackward0>)\n",
            "Step 1431 Loss:  tensor(0.0072, grad_fn=<MulBackward0>)\n",
            "Step 1432 Loss:  tensor(0.0072, grad_fn=<MulBackward0>)\n",
            "Step 1433 Loss:  tensor(0.0072, grad_fn=<MulBackward0>)\n",
            "Step 1434 Loss:  tensor(0.0072, grad_fn=<MulBackward0>)\n",
            "Step 1435 Loss:  tensor(0.0071, grad_fn=<MulBackward0>)\n",
            "Step 1436 Loss:  tensor(0.0071, grad_fn=<MulBackward0>)\n",
            "Step 1437 Loss:  tensor(0.0071, grad_fn=<MulBackward0>)\n",
            "Step 1438 Loss:  tensor(0.0071, grad_fn=<MulBackward0>)\n",
            "Step 1439 Loss:  tensor(0.0071, grad_fn=<MulBackward0>)\n",
            "Step 1440 Loss:  tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "Step 1441 Loss:  tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "Step 1442 Loss:  tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "Step 1443 Loss:  tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "Step 1444 Loss:  tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "Step 1445 Loss:  tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "Step 1446 Loss:  tensor(0.0069, grad_fn=<MulBackward0>)\n",
            "Step 1447 Loss:  tensor(0.0069, grad_fn=<MulBackward0>)\n",
            "Step 1448 Loss:  tensor(0.0069, grad_fn=<MulBackward0>)\n",
            "Step 1449 Loss:  tensor(0.0069, grad_fn=<MulBackward0>)\n",
            "Step 1450 Loss:  tensor(0.0069, grad_fn=<MulBackward0>)\n",
            "Step 1451 Loss:  tensor(0.0068, grad_fn=<MulBackward0>)\n",
            "Step 1452 Loss:  tensor(0.0068, grad_fn=<MulBackward0>)\n",
            "Step 1453 Loss:  tensor(0.0068, grad_fn=<MulBackward0>)\n",
            "Step 1454 Loss:  tensor(0.0068, grad_fn=<MulBackward0>)\n",
            "Step 1455 Loss:  tensor(0.0068, grad_fn=<MulBackward0>)\n",
            "Step 1456 Loss:  tensor(0.0068, grad_fn=<MulBackward0>)\n",
            "Step 1457 Loss:  tensor(0.0067, grad_fn=<MulBackward0>)\n",
            "Step 1458 Loss:  tensor(0.0067, grad_fn=<MulBackward0>)\n",
            "Step 1459 Loss:  tensor(0.0067, grad_fn=<MulBackward0>)\n",
            "Step 1460 Loss:  tensor(0.0067, grad_fn=<MulBackward0>)\n",
            "Step 1461 Loss:  tensor(0.0067, grad_fn=<MulBackward0>)\n",
            "Step 1462 Loss:  tensor(0.0066, grad_fn=<MulBackward0>)\n",
            "Step 1463 Loss:  tensor(0.0066, grad_fn=<MulBackward0>)\n",
            "Step 1464 Loss:  tensor(0.0066, grad_fn=<MulBackward0>)\n",
            "Step 1465 Loss:  tensor(0.0066, grad_fn=<MulBackward0>)\n",
            "Step 1466 Loss:  tensor(0.0066, grad_fn=<MulBackward0>)\n",
            "Step 1467 Loss:  tensor(0.0066, grad_fn=<MulBackward0>)\n",
            "Step 1468 Loss:  tensor(0.0065, grad_fn=<MulBackward0>)\n",
            "Step 1469 Loss:  tensor(0.0065, grad_fn=<MulBackward0>)\n",
            "Step 1470 Loss:  tensor(0.0065, grad_fn=<MulBackward0>)\n",
            "Step 1471 Loss:  tensor(0.0065, grad_fn=<MulBackward0>)\n",
            "Step 1472 Loss:  tensor(0.0065, grad_fn=<MulBackward0>)\n",
            "Step 1473 Loss:  tensor(0.0065, grad_fn=<MulBackward0>)\n",
            "Step 1474 Loss:  tensor(0.0064, grad_fn=<MulBackward0>)\n",
            "Step 1475 Loss:  tensor(0.0064, grad_fn=<MulBackward0>)\n",
            "Step 1476 Loss:  tensor(0.0064, grad_fn=<MulBackward0>)\n",
            "Step 1477 Loss:  tensor(0.0064, grad_fn=<MulBackward0>)\n",
            "Step 1478 Loss:  tensor(0.0064, grad_fn=<MulBackward0>)\n",
            "Step 1479 Loss:  tensor(0.0063, grad_fn=<MulBackward0>)\n",
            "Step 1480 Loss:  tensor(0.0063, grad_fn=<MulBackward0>)\n",
            "Step 1481 Loss:  tensor(0.0063, grad_fn=<MulBackward0>)\n",
            "Step 1482 Loss:  tensor(0.0063, grad_fn=<MulBackward0>)\n",
            "Step 1483 Loss:  tensor(0.0063, grad_fn=<MulBackward0>)\n",
            "Step 1484 Loss:  tensor(0.0063, grad_fn=<MulBackward0>)\n",
            "Step 1485 Loss:  tensor(0.0063, grad_fn=<MulBackward0>)\n",
            "Step 1486 Loss:  tensor(0.0062, grad_fn=<MulBackward0>)\n",
            "Step 1487 Loss:  tensor(0.0062, grad_fn=<MulBackward0>)\n",
            "Step 1488 Loss:  tensor(0.0062, grad_fn=<MulBackward0>)\n",
            "Step 1489 Loss:  tensor(0.0062, grad_fn=<MulBackward0>)\n",
            "Step 1490 Loss:  tensor(0.0062, grad_fn=<MulBackward0>)\n",
            "Step 1491 Loss:  tensor(0.0062, grad_fn=<MulBackward0>)\n",
            "Step 1492 Loss:  tensor(0.0061, grad_fn=<MulBackward0>)\n",
            "Step 1493 Loss:  tensor(0.0061, grad_fn=<MulBackward0>)\n",
            "Step 1494 Loss:  tensor(0.0061, grad_fn=<MulBackward0>)\n",
            "Step 1495 Loss:  tensor(0.0061, grad_fn=<MulBackward0>)\n",
            "Step 1496 Loss:  tensor(0.0061, grad_fn=<MulBackward0>)\n",
            "Step 1497 Loss:  tensor(0.0061, grad_fn=<MulBackward0>)\n",
            "Step 1498 Loss:  tensor(0.0060, grad_fn=<MulBackward0>)\n",
            "Step 1499 Loss:  tensor(0.0060, grad_fn=<MulBackward0>)\n",
            "Step 1500 Loss:  tensor(0.0060, grad_fn=<MulBackward0>)\n",
            "Step 1501 Loss:  tensor(0.0060, grad_fn=<MulBackward0>)\n",
            "Step 1502 Loss:  tensor(0.0060, grad_fn=<MulBackward0>)\n",
            "Step 1503 Loss:  tensor(0.0060, grad_fn=<MulBackward0>)\n",
            "Step 1504 Loss:  tensor(0.0059, grad_fn=<MulBackward0>)\n",
            "Step 1505 Loss:  tensor(0.0059, grad_fn=<MulBackward0>)\n",
            "Step 1506 Loss:  tensor(0.0059, grad_fn=<MulBackward0>)\n",
            "Step 1507 Loss:  tensor(0.0059, grad_fn=<MulBackward0>)\n",
            "Step 1508 Loss:  tensor(0.0059, grad_fn=<MulBackward0>)\n",
            "Step 1509 Loss:  tensor(0.0059, grad_fn=<MulBackward0>)\n",
            "Step 1510 Loss:  tensor(0.0059, grad_fn=<MulBackward0>)\n",
            "Step 1511 Loss:  tensor(0.0058, grad_fn=<MulBackward0>)\n",
            "Step 1512 Loss:  tensor(0.0058, grad_fn=<MulBackward0>)\n",
            "Step 1513 Loss:  tensor(0.0058, grad_fn=<MulBackward0>)\n",
            "Step 1514 Loss:  tensor(0.0058, grad_fn=<MulBackward0>)\n",
            "Step 1515 Loss:  tensor(0.0058, grad_fn=<MulBackward0>)\n",
            "Step 1516 Loss:  tensor(0.0058, grad_fn=<MulBackward0>)\n",
            "Step 1517 Loss:  tensor(0.0058, grad_fn=<MulBackward0>)\n",
            "Step 1518 Loss:  tensor(0.0057, grad_fn=<MulBackward0>)\n",
            "Step 1519 Loss:  tensor(0.0057, grad_fn=<MulBackward0>)\n",
            "Step 1520 Loss:  tensor(0.0057, grad_fn=<MulBackward0>)\n",
            "Step 1521 Loss:  tensor(0.0057, grad_fn=<MulBackward0>)\n",
            "Step 1522 Loss:  tensor(0.0057, grad_fn=<MulBackward0>)\n",
            "Step 1523 Loss:  tensor(0.0057, grad_fn=<MulBackward0>)\n",
            "Step 1524 Loss:  tensor(0.0056, grad_fn=<MulBackward0>)\n",
            "Step 1525 Loss:  tensor(0.0056, grad_fn=<MulBackward0>)\n",
            "Step 1526 Loss:  tensor(0.0056, grad_fn=<MulBackward0>)\n",
            "Step 1527 Loss:  tensor(0.0056, grad_fn=<MulBackward0>)\n",
            "Step 1528 Loss:  tensor(0.0056, grad_fn=<MulBackward0>)\n",
            "Step 1529 Loss:  tensor(0.0056, grad_fn=<MulBackward0>)\n",
            "Step 1530 Loss:  tensor(0.0056, grad_fn=<MulBackward0>)\n",
            "Step 1531 Loss:  tensor(0.0055, grad_fn=<MulBackward0>)\n",
            "Step 1532 Loss:  tensor(0.0055, grad_fn=<MulBackward0>)\n",
            "Step 1533 Loss:  tensor(0.0055, grad_fn=<MulBackward0>)\n",
            "Step 1534 Loss:  tensor(0.0055, grad_fn=<MulBackward0>)\n",
            "Step 1535 Loss:  tensor(0.0055, grad_fn=<MulBackward0>)\n",
            "Step 1536 Loss:  tensor(0.0055, grad_fn=<MulBackward0>)\n",
            "Step 1537 Loss:  tensor(0.0055, grad_fn=<MulBackward0>)\n",
            "Step 1538 Loss:  tensor(0.0055, grad_fn=<MulBackward0>)\n",
            "Step 1539 Loss:  tensor(0.0054, grad_fn=<MulBackward0>)\n",
            "Step 1540 Loss:  tensor(0.0054, grad_fn=<MulBackward0>)\n",
            "Step 1541 Loss:  tensor(0.0054, grad_fn=<MulBackward0>)\n",
            "Step 1542 Loss:  tensor(0.0054, grad_fn=<MulBackward0>)\n",
            "Step 1543 Loss:  tensor(0.0054, grad_fn=<MulBackward0>)\n",
            "Step 1544 Loss:  tensor(0.0054, grad_fn=<MulBackward0>)\n",
            "Step 1545 Loss:  tensor(0.0054, grad_fn=<MulBackward0>)\n",
            "Step 1546 Loss:  tensor(0.0053, grad_fn=<MulBackward0>)\n",
            "Step 1547 Loss:  tensor(0.0053, grad_fn=<MulBackward0>)\n",
            "Step 1548 Loss:  tensor(0.0053, grad_fn=<MulBackward0>)\n",
            "Step 1549 Loss:  tensor(0.0053, grad_fn=<MulBackward0>)\n",
            "Step 1550 Loss:  tensor(0.0053, grad_fn=<MulBackward0>)\n",
            "Step 1551 Loss:  tensor(0.0053, grad_fn=<MulBackward0>)\n",
            "Step 1552 Loss:  tensor(0.0053, grad_fn=<MulBackward0>)\n",
            "Step 1553 Loss:  tensor(0.0052, grad_fn=<MulBackward0>)\n",
            "Step 1554 Loss:  tensor(0.0052, grad_fn=<MulBackward0>)\n",
            "Step 1555 Loss:  tensor(0.0052, grad_fn=<MulBackward0>)\n",
            "Step 1556 Loss:  tensor(0.0052, grad_fn=<MulBackward0>)\n",
            "Step 1557 Loss:  tensor(0.0052, grad_fn=<MulBackward0>)\n",
            "Step 1558 Loss:  tensor(0.0052, grad_fn=<MulBackward0>)\n",
            "Step 1559 Loss:  tensor(0.0052, grad_fn=<MulBackward0>)\n",
            "Step 1560 Loss:  tensor(0.0052, grad_fn=<MulBackward0>)\n",
            "Step 1561 Loss:  tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "Step 1562 Loss:  tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "Step 1563 Loss:  tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "Step 1564 Loss:  tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "Step 1565 Loss:  tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "Step 1566 Loss:  tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "Step 1567 Loss:  tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "Step 1568 Loss:  tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "Step 1569 Loss:  tensor(0.0050, grad_fn=<MulBackward0>)\n",
            "Step 1570 Loss:  tensor(0.0050, grad_fn=<MulBackward0>)\n",
            "Step 1571 Loss:  tensor(0.0050, grad_fn=<MulBackward0>)\n",
            "Step 1572 Loss:  tensor(0.0050, grad_fn=<MulBackward0>)\n",
            "Step 1573 Loss:  tensor(0.0050, grad_fn=<MulBackward0>)\n",
            "Step 1574 Loss:  tensor(0.0050, grad_fn=<MulBackward0>)\n",
            "Step 1575 Loss:  tensor(0.0050, grad_fn=<MulBackward0>)\n",
            "Step 1576 Loss:  tensor(0.0050, grad_fn=<MulBackward0>)\n",
            "Step 1577 Loss:  tensor(0.0049, grad_fn=<MulBackward0>)\n",
            "Step 1578 Loss:  tensor(0.0049, grad_fn=<MulBackward0>)\n",
            "Step 1579 Loss:  tensor(0.0049, grad_fn=<MulBackward0>)\n",
            "Step 1580 Loss:  tensor(0.0049, grad_fn=<MulBackward0>)\n",
            "Step 1581 Loss:  tensor(0.0049, grad_fn=<MulBackward0>)\n",
            "Step 1582 Loss:  tensor(0.0049, grad_fn=<MulBackward0>)\n",
            "Step 1583 Loss:  tensor(0.0049, grad_fn=<MulBackward0>)\n",
            "Step 1584 Loss:  tensor(0.0049, grad_fn=<MulBackward0>)\n",
            "Step 1585 Loss:  tensor(0.0049, grad_fn=<MulBackward0>)\n",
            "Step 1586 Loss:  tensor(0.0048, grad_fn=<MulBackward0>)\n",
            "Step 1587 Loss:  tensor(0.0048, grad_fn=<MulBackward0>)\n",
            "Step 1588 Loss:  tensor(0.0048, grad_fn=<MulBackward0>)\n",
            "Step 1589 Loss:  tensor(0.0048, grad_fn=<MulBackward0>)\n",
            "Step 1590 Loss:  tensor(0.0048, grad_fn=<MulBackward0>)\n",
            "Step 1591 Loss:  tensor(0.0048, grad_fn=<MulBackward0>)\n",
            "Step 1592 Loss:  tensor(0.0048, grad_fn=<MulBackward0>)\n",
            "Step 1593 Loss:  tensor(0.0048, grad_fn=<MulBackward0>)\n",
            "Step 1594 Loss:  tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "Step 1595 Loss:  tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "Step 1596 Loss:  tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "Step 1597 Loss:  tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "Step 1598 Loss:  tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "Step 1599 Loss:  tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "Step 1600 Loss:  tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "Step 1601 Loss:  tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "Step 1602 Loss:  tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "Step 1603 Loss:  tensor(0.0046, grad_fn=<MulBackward0>)\n",
            "Step 1604 Loss:  tensor(0.0046, grad_fn=<MulBackward0>)\n",
            "Step 1605 Loss:  tensor(0.0046, grad_fn=<MulBackward0>)\n",
            "Step 1606 Loss:  tensor(0.0046, grad_fn=<MulBackward0>)\n",
            "Step 1607 Loss:  tensor(0.0046, grad_fn=<MulBackward0>)\n",
            "Step 1608 Loss:  tensor(0.0046, grad_fn=<MulBackward0>)\n",
            "Step 1609 Loss:  tensor(0.0046, grad_fn=<MulBackward0>)\n",
            "Step 1610 Loss:  tensor(0.0046, grad_fn=<MulBackward0>)\n",
            "Step 1611 Loss:  tensor(0.0046, grad_fn=<MulBackward0>)\n",
            "Step 1612 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1613 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1614 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1615 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1616 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1617 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1618 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1619 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1620 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1621 Loss:  tensor(0.0045, grad_fn=<MulBackward0>)\n",
            "Step 1622 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1623 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1624 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1625 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1626 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1627 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1628 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1629 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1630 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1631 Loss:  tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "Step 1632 Loss:  tensor(0.0043, grad_fn=<MulBackward0>)\n",
            "Step 1633 Loss:  tensor(0.0043, grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXnHhbVn4cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cd427ff7-5e02-46c5-fc12-6d3f8ba64082"
      },
      "source": [
        "x_1 = torch.linspace(-3, 8, 50)\n",
        "x_2 =  - w[1]/w[2] * x_1 - w[0]/w[2]\n",
        "\n",
        "plt.scatter(X_1[:, 0], X_1[:, 1])\n",
        "plt.scatter(X_2[:, 0], X_2[:, 1])\n",
        "plt.plot(x_1.detach().numpy(), x_2.detach().numpy())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjoBMUmY4nFN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}